<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="第四章：底层：训练数字分类器在第二章中看到训练各种模型的样子后，现在让我们深入了解并看看究竟发生了什么。我们将使用计算机视觉来介绍深度学习的基本工具和概念。 确切地说，我们将讨论数组和张量的作用以及广播的作用，这是一种使用它们表达性地的强大技术。我们将解释随机梯度下降（SGD），这是通过自动更新权重学习的机制。我们将讨论基本分类任务的损失函数的选择，以及小批量的作用。我们还将描述基本神经网络正在执">
<meta property="og:type" content="article">
<meta property="og:title" content="Fastai Chapter 4">
<meta property="og:url" content="http://example.com/2025/04/28/fastaichapter4/index.html">
<meta property="og:site_name" content="WangSong&#39;s blog">
<meta property="og:description" content="第四章：底层：训练数字分类器在第二章中看到训练各种模型的样子后，现在让我们深入了解并看看究竟发生了什么。我们将使用计算机视觉来介绍深度学习的基本工具和概念。 确切地说，我们将讨论数组和张量的作用以及广播的作用，这是一种使用它们表达性地的强大技术。我们将解释随机梯度下降（SGD），这是通过自动更新权重学习的机制。我们将讨论基本分类任务的损失函数的选择，以及小批量的作用。我们还将描述基本神经网络正在执">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/image/dlcf_04in01.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in02.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in03.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in04.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in05.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in06.png">
<meta property="og:image" content="http://example.com/image/dlcf_0401.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in07.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in08.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in09.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in10.png">
<meta property="og:image" content="http://example.com/image/dlcf_0402.png">
<meta property="og:image" content="http://example.com/image/dlcf_0403.png">
<meta property="og:image" content="http://example.com/image/dlcf_0404.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in11.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in12.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in12.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in13.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in14.png">
<meta property="og:image" content="http://example.com/image/dlcf_0405.png">
<meta property="og:image" content="http://example.com/image/dlcf_0406.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in15.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in16.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in17.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in15.png">
<meta property="og:image" content="http://example.com/image/dlcf_04in16.png">
<meta property="article:published_time" content="2025-04-28T01:30:20.000Z">
<meta property="article:modified_time" content="2025-05-12T08:31:16.519Z">
<meta property="article:author" content="Wang Song">
<meta property="article:tag" content="Python, C++, robot, ros , opencv, target detection">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/image/dlcf_04in01.png">

<link rel="canonical" href="http://example.com/2025/04/28/fastaichapter4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Fastai Chapter 4 | WangSong's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="WangSong's blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WangSong's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/SongSop" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/04/28/fastaichapter4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Fastai Chapter 4
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-28 09:30:20" itemprop="dateCreated datePublished" datetime="2025-04-28T09:30:20+08:00">2025-04-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-12 16:31:16" itemprop="dateModified" datetime="2025-05-12T16:31:16+08:00">2025-05-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="第四章：底层：训练数字分类器"><a href="#第四章：底层：训练数字分类器" class="headerlink" title="第四章：底层：训练数字分类器"></a>第四章：底层：训练数字分类器</h1><p>在第二章中看到训练各种模型的样子后，现在让我们深入了解并看看究竟发生了什么。我们将使用计算机视觉来介绍深度学习的基本工具和概念。</p>
<p>确切地说，我们将讨论数组和张量的作用以及广播的作用，这是一种使用它们表达性地的强大技术。我们将解释随机梯度下降（SGD），这是通过自动更新权重学习的机制。我们将讨论基本分类任务的损失函数的选择，以及小批量的作用。我们还将描述基本神经网络正在执行的数学。最后，我们将把所有这些部分组合起来。</p>
<p>在未来的章节中，我们还将深入研究其他应用，并看看这些概念和工具如何泛化。但本章是关于奠定基础的。坦率地说，这也使得这是最困难的章节之一，因为这些概念彼此相互依赖。就像一个拱门，所有的石头都需要放在正确的位置才能支撑结构。也像一个拱门，一旦发生这种情况，它就是一个强大的结构，可以支撑其他事物。但是需要一些耐心来组装。</p>
<p>让我们开始吧。第一步是考虑图像在计算机中是如何表示的。</p>
<h1 id="像素：计算机视觉的基础"><a href="#像素：计算机视觉的基础" class="headerlink" title="像素：计算机视觉的基础"></a>像素：计算机视觉的基础</h1><p>要理解计算机视觉模型中发生的事情，我们首先必须了解计算机如何处理图像。我们将使用计算机视觉中最著名的数据集之一 MNIST 进行实验。MNIST 包含由国家标准与技术研究所收集的手写数字图像，并由 Yann Lecun 及其同事整理成一个机器学习数据集。Lecun 在 1998 年使用 MNIST 在 LeNet-5 中，这是第一个演示实用手写数字序列识别的计算机系统。这是人工智能历史上最重要的突破之一。</p>
<p>对于这个初始教程，我们只是尝试创建一个模型，可以将任何图像分类为 3 或 7。所以让我们下载一个包含这些数字图像的 MNIST 样本：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path = untar_data(URLs.MNIST_SAMPLE)</span><br></pre></td></tr></table></figure>
<p>我们可以使用<code>ls</code>来查看此目录中的内容，这是 fastai 添加的一个方法。这个方法返回一个特殊的 fastai 类<code>L</code>的对象，它具有 Python 内置<code>list</code>的所有功能，还有更多功能。其中一个方便的功能是，在打印时，它会显示项目的计数，然后列出项目本身（如果项目超过 10 个，它只显示前几个）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path.ls()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#9) [Path(&#x27;cleaned.csv&#x27;),Path(&#x27;item_list.txt&#x27;),Path(&#x27;trained_model.pkl&#x27;),Path(&#x27;</span></span><br><span class="line"> &gt; models<span class="string">&#x27;),Path(&#x27;</span>valid<span class="string">&#x27;),Path(&#x27;</span>labels.csv<span class="string">&#x27;),Path(&#x27;</span>export.pkl<span class="string">&#x27;),Path(&#x27;</span>history.cs</span><br><span class="line"> &gt; v<span class="string">&#x27;),Path(&#x27;</span>train<span class="string">&#x27;)]</span></span><br></pre></td></tr></table></figure>
<p>MNIST 数据集遵循机器学习数据集的常见布局：训练集和验证（和/或测试）集分开存放。让我们看看训练集中的内容：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(path/<span class="string">&#x27;train&#x27;</span>).ls()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#2) [Path(&#x27;train/7&#x27;),Path(&#x27;train/3&#x27;)]</span></span><br></pre></td></tr></table></figure>
<p>有一个包含 3 的文件夹，和一个包含 7 的文件夹。在机器学习术语中，我们说“3”和“7”是这个数据集中的<em>标签</em>（或目标）。让我们看看其中一个文件夹中的内容（使用<code>sorted</code>确保我们都得到相同的文件顺序）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">threes = (path/<span class="string">&#x27;train&#x27;</span>/<span class="string">&#x27;3&#x27;</span>).ls().<span class="built_in">sorted</span>()</span><br><span class="line">sevens = (path/<span class="string">&#x27;train&#x27;</span>/<span class="string">&#x27;7&#x27;</span>).ls().<span class="built_in">sorted</span>()</span><br><span class="line">threes</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#6131) [Path(&#x27;train/3/10.png&#x27;),Path(&#x27;train/3/10000.png&#x27;),Path(&#x27;train/3/10011.pn</span></span><br><span class="line"> &gt; g<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10031.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10034.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10042.</span>p</span><br><span class="line"> &gt; ng<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10052.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">1007.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10074.</span>p</span><br><span class="line"> &gt; ng<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10091.</span>png<span class="string">&#x27;)...]</span></span><br></pre></td></tr></table></figure>
<p>正如我们所预期的那样，它充满了图像文件。让我们现在看一个。这是一个手写数字 3 的图像，来自著名的手写数字 MNIST 数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im3_path = threes[<span class="number">1</span>]</span><br><span class="line">im3 = Image.<span class="built_in">open</span>(im3_path)</span><br><span class="line">im3</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in01.png" alt=""></p>
<p>在这里，我们使用<em>Python Imaging Library</em>（PIL）中的<code>Image</code>类，这是最广泛使用的 Python 包，用于打开、操作和查看图像。Jupyter 知道 PIL 图像，所以它会自动为我们显示图像。</p>
<p>在计算机中，一切都以数字表示。要查看构成这幅图像的数字，我们必须将其转换为<em>NumPy 数组</em>或<em>PyTorch 张量</em>。例如，这是转换为 NumPy 数组后图像的一部分的样子：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(im3)[<span class="number">4</span>:<span class="number">10</span>,<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">       [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">29</span>],</span><br><span class="line">       [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">48</span>, <span class="number">166</span>, <span class="number">224</span>],</span><br><span class="line">       [  <span class="number">0</span>,  <span class="number">93</span>, <span class="number">244</span>, <span class="number">249</span>, <span class="number">253</span>, <span class="number">187</span>],</span><br><span class="line">       [  <span class="number">0</span>, <span class="number">107</span>, <span class="number">253</span>, <span class="number">253</span>, <span class="number">230</span>,  <span class="number">48</span>],</span><br><span class="line">       [  <span class="number">0</span>,   <span class="number">3</span>,  <span class="number">20</span>,  <span class="number">20</span>,  <span class="number">15</span>,   <span class="number">0</span>]], dtype=uint8)</span><br></pre></td></tr></table></figure>
<p><code>4:10</code>表示我们请求从索引 4（包括）到 10（不包括）的行，列也是一样。NumPy 从上到下，从左到右索引，因此此部分位于图像的左上角附近。这里是一个 PyTorch 张量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(im3)[<span class="number">4</span>:<span class="number">10</span>,<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">29</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">48</span>, <span class="number">166</span>, <span class="number">224</span>],</span><br><span class="line">        [  <span class="number">0</span>,  <span class="number">93</span>, <span class="number">244</span>, <span class="number">249</span>, <span class="number">253</span>, <span class="number">187</span>],</span><br><span class="line">        [  <span class="number">0</span>, <span class="number">107</span>, <span class="number">253</span>, <span class="number">253</span>, <span class="number">230</span>,  <span class="number">48</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">3</span>,  <span class="number">20</span>,  <span class="number">20</span>,  <span class="number">15</span>,   <span class="number">0</span>]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p><code>4:10</code>表示我们请求从索引 4（包括）到 10（不包括）的行，列也是一样。NumPy 从上到下，从左到右索引，因此此部分位于图像的左上角附近。这里是一个 PyTorch 张量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(im3)[<span class="number">4</span>:<span class="number">10</span>,<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">29</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">48</span>, <span class="number">166</span>, <span class="number">224</span>],</span><br><span class="line">        [  <span class="number">0</span>,  <span class="number">93</span>, <span class="number">244</span>, <span class="number">249</span>, <span class="number">253</span>, <span class="number">187</span>],</span><br><span class="line">        [  <span class="number">0</span>, <span class="number">107</span>, <span class="number">253</span>, <span class="number">253</span>, <span class="number">230</span>,  <span class="number">48</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">3</span>,  <span class="number">20</span>,  <span class="number">20</span>,  <span class="number">15</span>,   <span class="number">0</span>]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p>我们可以切片数组，只选择包含数字顶部部分的部分，然后使用 Pandas DataFrame 使用渐变对值进行着色，这清楚地显示了图像是如何由像素值创建的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im3_t = tensor(im3)</span><br><span class="line">df = pd.DataFrame(im3_t[<span class="number">4</span>:<span class="number">15</span>,<span class="number">4</span>:<span class="number">22</span>])</span><br><span class="line">df.style.set_properties(**&#123;<span class="string">&#x27;font-size&#x27;</span>:<span class="string">&#x27;6pt&#x27;</span>&#125;).background_gradient(<span class="string">&#x27;Greys&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in02.png" alt=""></p>
<p>你可以看到，背景白色像素存储为数字 0，黑色为数字 255，灰色在两者之间。整个图像横向包含 28 个像素，纵向包含 28 个像素，总共 768 个像素。（这比你从手机相机得到的图像要小得多，手机相机有数百万像素，但对于我们的初始学习和实验来说，这是一个方便的大小。我们将很快构建更大的全彩图像。）</p>
<p>所以，现在你已经看到了计算机对图像的看法，让我们回顾一下我们的目标：创建一个能够识别 3 和 7 的模型。你会如何让计算机做到这一点呢？</p>
<h1 id="停下来思考！"><a href="#停下来思考！" class="headerlink" title="停下来思考！"></a>停下来思考！</h1><p>在继续阅读之前，花点时间考虑一下计算机可能如何识别这两个数字。它可能能够看到什么样的特征？它可能如何识别这些特征？它如何将它们结合起来？学习最好的方式是尝试自己解决问题，而不仅仅是阅读别人的答案；所以离开这本书几分钟，拿一张纸和笔，写下一些想法。</p>
<font color = red> 我认为计算机可能会用识别到的手写数字图像的矩阵和标准的数字图像的矩阵进行运算像是点乘或是什么得出一个能给表现图片数字和标准数字相似度的一个数值进行比较取得最大的为识别数字。</font>

<h1 id="第一次尝试：像素相似度"><a href="#第一次尝试：像素相似度" class="headerlink" title="第一次尝试：像素相似度"></a>第一次尝试：像素相似度</h1><p>所以，这是一个第一个想法：我们可以找到每个 3 的像素的平均值，然后对 7 做同样的操作。这将给我们两组平均值，定义了我们可能称之为“理想”3 和 7。然后，为了将图像分类为一个数字或另一个数字，我们看看这两个理想数字中图像与哪个更相似。这肯定似乎比没有好，所以这将成为一个很好的基线。</p>
<h1 id="术语：基线"><a href="#术语：基线" class="headerlink" title="术语：基线"></a>术语：基线</h1><p>一个简单的模型，你有信心应该表现得相当不错。它应该简单实现和易于测试，这样你就可以测试每个改进的想法，并确保它们始终优于基线。如果没有以合理的基线开始，很难知道你的超级花哨的模型是否好用。创建基线的一个好方法是做我们在这里做的事情：考虑一个简单、易于实现的模型。另一个好方法是四处寻找解决类似问题的其他人，并在你的数据集上下载并运行他们的代码。最好两者都尝试一下！</p>
<p>我们简单模型的第一步是获取我们两组像素值的平均值。在这个过程中，我们将学习很多有趣的 Python 数值编程技巧！</p>
<p>让我们创建一个包含所有 3 的张量堆叠在一起。我们已经知道如何创建包含单个图像的张量。要创建一个包含目录中所有图像的张量，我们将首先使用 Python 列表推导来创建一个单个图像张量的普通列表。</p>
<p>我们将使用 Jupyter 在途中做一些小的检查——在这种情况下，确保返回的项目数量看起来合理：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seven_tensors = [tensor(Image.<span class="built_in">open</span>(o)) <span class="keyword">for</span> o <span class="keyword">in</span> sevens]</span><br><span class="line">three_tensors = [tensor(Image.<span class="built_in">open</span>(o)) <span class="keyword">for</span> o <span class="keyword">in</span> threes]</span><br><span class="line"><span class="built_in">len</span>(three_tensors),<span class="built_in">len</span>(seven_tensors)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">6131</span>, <span class="number">6265</span>)</span><br></pre></td></tr></table></figure>
<h1 id="列表推导"><a href="#列表推导" class="headerlink" title="列表推导"></a>列表推导</h1><p>列表和字典推导是 Python 的一个很棒的特性。许多 Python 程序员每天都在使用它们，包括本书的作者们——它们是“Python 的成语”。但是来自其他语言的程序员可能以前从未见过它们。许多很棒的教程只需一次网络搜索，所以我们现在不会花很长时间讨论它们。</p>
<h5 id="我来补充一下"><a href="#我来补充一下" class="headerlink" title="我来补充一下:"></a>我来补充一下:</h5><h6 id="Python-字典-Dictionary"><a href="#Python-字典-Dictionary" class="headerlink" title="Python 字典(Dictionary)"></a>Python 字典(Dictionary)</h6><font color = blue>Python 字典(Dictionary)
字典是另一种可变容器模型，且可存储任意类型对象。

字典的每个键值 key:value 对用冒号 : 分割，每个键值对之间用逗号 , 分割，整个字典包括在花括号 {} 中 ,格式如下所示：

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;key1 : value1, key2 : value2 &#125;</span><br></pre></td></tr></table></figure>
注意：dict 作为 Python 的关键字和内置函数，变量名不建议命名为 dict。
键一般是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一。
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tinydict = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tinydict[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;3&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tinydict</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
值可以取任何数据类型，但键必须是不可变的，如字符串，数字或元组。
一个简单的字典实例：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tinydict = &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="string">&#x27;2341&#x27;</span>, <span class="string">&#x27;Beth&#x27;</span>: <span class="string">&#x27;9102&#x27;</span>, <span class="string">&#x27;Cecil&#x27;</span>: <span class="string">&#x27;3258&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
也可如此创建字典：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tinydict1 = &#123; <span class="string">&#x27;abc&#x27;</span>: <span class="number">456</span> &#125;</span><br><span class="line">tinydict2 = &#123; <span class="string">&#x27;abc&#x27;</span>: <span class="number">123</span>, <span class="number">98.6</span>: <span class="number">37</span> &#125;</span><br></pre></td></tr></table></figure>
这里简单介绍一下字典在网页上还有更加详细的介绍和修改方法：[字典](https://www.runoob.com/python/python-dictionary.html)</font>

<h6 id="Python-列表-List"><a href="#Python-列表-List" class="headerlink" title="Python 列表(List)"></a>Python 列表(List)</h6><font color = orange>序列是Python中最基本的数据结构。序列中的每个元素都分配一个数字 - 它的位置，或索引，第一个索引是0，第二个索引是1，依此类推。

Python有6个序列的内置类型，但最常见的是列表和元组。

序列都可以进行的操作包括索引，切片，加，乘，检查成员。

此外，Python已经内置确定序列的长度以及确定最大和最小的元素的方法。

列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。

列表的数据项不需要具有相同的类型

创建一个列表，只要把逗号分隔的不同的数据项使用方括号括起来即可。如下所示：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;physics&#x27;</span>, <span class="string">&#x27;chemistry&#x27;</span>, <span class="number">1997</span>, <span class="number">2000</span>]</span><br><span class="line">list2 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> ]</span><br><span class="line">list3 = [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>]</span><br></pre></td></tr></table></figure>
与字符串的索引一样，列表索引从0开始。列表可以进行截取、组合等。更多对列表的操作:[列表](https://www.runoob.com/python/python-lists.html)</font>

<p>这里有一个快速的解释和示例，让您开始。列表推导看起来像这样：<code>new_list = [f(o) for o in a_list if o&gt;0]</code>。这将返回<code>a_list</code>中大于 0 的每个元素，在将其传递给函数<code>f</code>之后。这里有三个部分：您正在迭代的集合（<code>a_list</code>），一个可选的过滤器（<code>if o&gt;0</code>），以及对每个元素执行的操作（<code>f(o)</code>）。不仅写起来更短，而且比用循环创建相同列表的替代方法更快。</p>
<p>我们还将检查其中一张图像是否正常。由于我们现在有张量（Jupyter 默认会将其打印为值），而不是 PIL 图像（Jupyter 默认会显示图像），我们需要使用 fastai 的<code>show_image</code>函数来显示它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_image(three_tensors[<span class="number">1</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in03.png" alt=""></p>
<p>对于每个像素位置，我们想要计算该像素的强度在所有图像上的平均值。为了做到这一点，我们首先将此列表中的所有图像组合成一个三维张量。描述这样的张量最常见的方式是称之为<em>rank-3 张量</em>。我们经常需要将集合中的单个张量堆叠成一个张量。不出所料，PyTorch 带有一个名为<code>stack</code>的函数，我们可以用它来实现这个目的。</p>
<p>PyTorch 中的一些操作，如取平均值，需要我们将整数类型转换为浮点类型。由于我们稍后会需要这个，我们现在也将我们的堆叠张量转换为<code>float</code>。在 PyTorch 中进行转换就像写下您希望转换为的类型名称，并将其视为方法一样简单。</p>
<p>通常，当图像是浮点数时，像素值应该在 0 到 1 之间，所以我们也会在这里除以 255：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stacked_sevens = torch.stack(seven_tensors).<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">stacked_threes = torch.stack(three_tensors).<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">stacked_threes.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">6131</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure>
<p>张量最重要的属性也许是其<em>形状</em>。这告诉您每个轴的长度。在这种情况下，我们可以看到我们有 6,131 张图像，每张图像大小为 28×28 像素。关于这个张量没有特别的地方表明第一个轴是图像的数量，第二个是高度，第三个是宽度——张量的语义完全取决于我们以及我们如何构建它。就 PyTorch 而言，它只是内存中的一堆数字。</p>
<p>张量形状的<em>长度</em>是其秩：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(stacked_threes.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>对于您来说，将张量术语的这些部分记忆并加以实践非常重要：<em>秩</em>是张量中轴或维度的数量；<em>形状</em>是张量每个轴的大小。</p>
<h1 id="关于维度"><a href="#关于维度" class="headerlink" title="关于维度"></a>关于维度</h1><p>要小心，因为术语“维度”有时以两种方式使用。考虑我们生活在“三维空间”中，其中物理位置可以用长度为 3 的向量<code>v</code>描述。但根据 PyTorch，属性<code>v.ndim</code>（看起来确实像<code>v</code>的“维度数量”）等于一，而不是三！为什么？因为<code>v</code>是一个向量，它是一个秩为一的张量，这意味着它只有一个<em>轴</em>（即使该轴的长度为三）。换句话说，有时维度用于描述轴的大小（“空间是三维的”），而其他时候用于描述秩或轴的数量（“矩阵有两个维度”）。当感到困惑时，我发现将所有陈述转换为秩、轴和长度这些明确的术语是有帮助的。</p>
<p>我们也可以直接使用<code>ndim</code>来获取张量的秩：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stacked_threes.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算理想的 3 是什么样子的。我们通过沿着我们堆叠的 rank-3 张量的维度 0 取平均值来计算所有图像张量的平均值。这是索引所有图像的维度。</p>
<p>换句话说，对于每个像素位置，这将计算所有图像中该像素的平均值。结果将是每个像素位置的一个值，或者一个单独的图像。这就是它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean3 = stacked_threes.mean(<span class="number">0</span>)</span><br><span class="line">show_image(mean3);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in04.png" alt=""></p>
<p>根据这个数据集，这是理想的数字 3！（您可能不喜欢，但这就是顶级数字 3 表现的样子。）您可以看到在所有图像都认为应该是暗的地方非常暗，但在图像不一致的地方变得模糊。</p>
<p>让我们对 7 做同样的事情，但一次将所有步骤放在一起以节省时间：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean7 = stacked_sevens.mean(<span class="number">0</span>)</span><br><span class="line">show_image(mean7);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in05.png" alt=""></p>
<p>现在让我们选择一个任意的 3，并测量它与我们的“理想数字”的<em>距离</em>。</p>
<h1 id="停下来思考一下！"><a href="#停下来思考一下！" class="headerlink" title="停下来思考一下！"></a>停下来思考一下！</h1><p>您如何计算特定图像与我们的每个理想数字之间的相似程度？在继续前进之前，请记得远离这本书，记录一些想法！研究表明，通过解决问题、实验和尝试新想法，您参与学习过程时，召回和理解会显著提高。</p>
<p>这是一个示例 3：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a_3 = stacked_threes[<span class="number">1</span>]</span><br><span class="line">show_image(a_3);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in06.png" alt=""></p>
<p>我们如何确定它与我们理想的 3 之间的距离？我们不能简单地将此图像的像素之间的差异相加，并与理想数字进行比较。一些差异将是正的，而另一些将是负的，这些差异将相互抵消，导致一种情况，即在某些地方太暗而在其他地方太亮的图像可能被显示为与理想的总差异为零。那将是误导性的！</p>
<p>为了避免这种情况，数据科学家在这种情况下使用两种主要方法来测量距离：</p>
<ul>
<li><p>取差值的<em>绝对值</em>的平均值（绝对值是将负值替换为正值的函数）。这被称为<em>平均绝对差</em>或<em>L1 范数</em>。</p>
</li>
<li><p>取差值的<em>平方</em>的平均值（使所有值变为正数），然后取<em>平方根</em>（撤销平方）。这被称为<em>均方根误差</em>（RMSE）或<em>L2 范数</em>。</p>
</li>
</ul>
<h1 id="在pytorch中-取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1-范数。"><a href="#在pytorch中-取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1-范数。" class="headerlink" title="在pytorch中 取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1 范数。"></a>在pytorch中 取差值的<em>绝对值</em>的平均值（绝对值是将负值替换为正值的函数）。这被称为<em>平均绝对差</em>或<em>L1 范数</em>。</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dist_7_abs = (a_3 - mean7).<span class="built_in">abs</span>().mean()</span><br><span class="line">dist_7_sqr = ((a_3 - mean7)**<span class="number">2</span>).mean().sqrt()</span><br><span class="line">dist_7_abs,dist_7_sqr</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.1586</span>), tensor(<span class="number">0.3021</span>))</span><br></pre></td></tr></table></figure>
<p>等同于：</p>
<p>PyTorch 已经提供了这两种作为<em>损失函数</em>。您会在<code>torch.nn.functional</code>中找到这些，PyTorch 团队建议将其导入为<code>F</code>（并且默认情况下以这个名称在 fastai 中可用）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.l1_loss(a_3.<span class="built_in">float</span>(),mean7), F.mse_loss(a_3,mean7).sqrt()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.1586</span>), tensor(<span class="number">0.3021</span>))</span><br></pre></td></tr></table></figure>
<p>在这里，<code>MSE</code>代表<em>均方误差</em>，<code>l1</code>是标准数学术语<em>平均绝对值</em>的缩写（在数学中称为<em>L1 范数</em>）。</p>
<h1 id="L1-范数和-均方误差（MSE）之间的区别"><a href="#L1-范数和-均方误差（MSE）之间的区别" class="headerlink" title="L1 范数和 均方误差（MSE）之间的区别"></a>L1 范数和 均方误差（MSE）之间的区别</h1><p>直观地，L1 范数和均方误差（MSE）之间的区别在于，后者会比前者更严厉地惩罚更大的错误（并对小错误更宽容）。</p>
<h1 id="杰里米说"><a href="#杰里米说" class="headerlink" title="杰里米说"></a>杰里米说</h1><p>当我第一次遇到这个 L1 的东西时，我查了一下看它到底是什么意思。我在谷歌上发现它是使用“绝对值”作为“向量范数”，所以我查了“向量范数”并开始阅读：“给定一个实数或复数域 F 上的向量空间 V，V 上的范数是一个非负值的任意函数 p: V → [0,+∞)，具有以下属性：对于所有的 a ∈ F 和所有的 u, v ∈ V，p(u + v) ≤ p(u) + p(v)…”然后我停止阅读。“唉，我永远也理解不了数学！”我想，这已经是第一千次了。从那时起，我学到了每当实践中出现这些复杂的数学术语时，我可以用一点点代码来替换它们！比如，<em>L1 损失</em> 只等于 <code>(a-b).abs().mean()</code>，其中 <code>a</code> 和 <code>b</code> 是张量。我猜数学家们只是和我想法不同…我会确保在本书中，每当出现一些数学术语时，我会给你相应的代码片段，并用通俗的语言解释发生了什么。</p>
<p>我们刚刚在 PyTorch 张量上完成了各种数学运算。如果你之前在 PyTorch 中进行过数值编程，你可能会发现这些与 NumPy 数组相似。让我们来看看这两个重要的数据结构。</p>
<p>（请注意，fastai 在 NumPy 和 PyTorch 中添加了一些功能，使它们更加相似。如果本书中的任何代码在您的计算机上无法运行，可能是因为您忘记在笔记本的开头包含类似这样的一行代码：<code>from fastai.vision.all import *</code>。）</p>
<p>但是数组和张量是什么，为什么你应该关心呢？</p>
<p>Python 相对于许多语言来说速度较慢。在 Python、NumPy 或 PyTorch 中快速的任何东西，很可能是另一种语言（特别是 C）编写（并优化）的编译对象的包装器。事实上，<em>NumPy 数组和 PyTorch 张量可以比纯 Python 快几千倍完成计算</em>。</p>
<p>NumPy 数组是一个多维数据表，所有项都是相同类型的。由于可以是任何类型，它们甚至可以是数组的数组，内部数组可能是不同大小的 - 这被称为 <em>不规则数组</em>。通过“多维数据表”，我们指的是，例如，一个列表（一维）、一个表或矩阵（二维）、一个表的表或立方体（三维），等等。如果所有项都是简单类型，如整数或浮点数，NumPy 将它们存储为紧凑的 C 数据结构在内存中。这就是 NumPy 的优势所在。NumPy 有各种运算符和方法，可以在这些紧凑结构上以优化的 C 速度运行计算，因为它们是用优化的 C 编写的。</p>
<p>PyTorch 张量几乎与 NumPy 数组相同，但有一个额外的限制，可以解锁额外的功能。它与 NumPy 数组相同，也是一个多维数据表，所有项都是相同类型的。然而，限制是张量不能使用任何旧类型 - 它必须对所有组件使用单一基本数值类型。因此，张量不像真正的数组数组那样灵活。例如，PyTorch 张量不能是不规则的。它始终是一个形状规则的多维矩形结构。</p>
<p>NumPy 在这些结构上支持的绝大多数方法和运算符在 PyTorch 上也支持，但 PyTorch 张量具有额外的功能。一个主要功能是这些结构可以存在于 GPU 上，这样它们的计算将被优化为 GPU，并且可以运行得更快（给定大量值进行处理）。此外，PyTorch 可以自动计算这些操作的导数，包括操作的组合。正如你将看到的，没有这种能力，实际上是不可能进行深度学习的。</p>
<h1 id="如何有效地使用数组-张量-API-是最重要的新编码技能。"><a href="#如何有效地使用数组-张量-API-是最重要的新编码技能。" class="headerlink" title="如何有效地使用数组/张量 API 是最重要的新编码技能。"></a>如何有效地使用数组/张量 API 是最重要的新编码技能。</h1><p>要创建一个数组或张量，将列表（或列表的列表，或列表的列表的列表等）传递给<code>array</code>或<code>tensor</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">arr = array (data)</span><br><span class="line">tns = tensor(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr  <span class="comment"># numpy</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns  <span class="comment"># pytorch</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<p>以下所有操作都是在张量上展示的，但 NumPy 数组的语法和结果是相同的。</p>
<p>你可以选择一行（请注意，与 Python 中的列表一样，张量是从 0 开始索引的，所以 1 指的是第二行/列）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>或者通过使用<code>:</code>来指示<em>所有第一个轴</em>（我们有时将张量/数组的维度称为<em>轴</em>）选择一列。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>你可以结合 Python 切片语法（<code>[*start*:*end*]</code>，其中<em><code>end</code></em>被排除）来选择一行或一列的一部分：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns[<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>你可以使用标准运算符，如<code>+</code>、<code>-</code>、<code>*</code>和<code>/</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns+<span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p>张量有一个类型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns.<span class="built_in">type</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;torch.LongTensor&#x27;</span></span><br></pre></td></tr></table></figure>
<p>并且会根据需要自动更改该类型；例如，从<code>int</code>到<code>float</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns*<span class="number">1.5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.5000</span>, <span class="number">3.0000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">6.0000</span>, <span class="number">7.5000</span>, <span class="number">9.0000</span>]])</span><br></pre></td></tr></table></figure>
<p>那么，我们的基准模型好吗？为了量化这一点，我们必须定义一个度量。</p>
<h1 id="使用广播计算度量"><a href="#使用广播计算度量" class="headerlink" title="使用广播计算度量"></a>使用广播计算度量</h1><p>回想一下<em>度量</em>是基于我们模型的预测和数据集中正确标签计算出来的一个数字，以告诉我们我们的模型有多好。例如，我们可以使用我们在上一节中看到的两个函数之一，均方误差或平均绝对误差，并计算整个数据集上它们的平均值。然而，这两个数字对大多数人来说并不是很容易理解；实际上，我们通常使用<em>准确度</em>作为分类模型的度量。</p>
<p>正如我们讨论过的，我们想要在<em>验证集</em>上计算我们的度量。这样我们就不会无意中过拟合——也就是说，训练一个模型只在我们的训练数据上表现良好。这对于我们在这里作为第一次尝试使用的像素相似度模型来说并不是真正的风险，因为它没有经过训练的组件，但我们仍然会使用一个验证集来遵循正常的实践，并为我们稍后的第二次尝试做好准备。</p>
<p>为了获得一个验证集，我们需要完全从训练数据中删除一些数据，这样模型根本就看不到它。事实证明，MNIST 数据集的创建者已经为我们做了这个。你还记得<em>valid</em>这个整个独立的目录吗？这个目录就是为此而设立的！</p>
<p>所以，让我们从那个目录中为我们的 3 和 7 创建张量。这些是我们将用来计算度量的张量，用来衡量我们第一次尝试模型的质量，这个度量衡量了与理想图像的距离：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">valid_3_tens = torch.stack([tensor(Image.<span class="built_in">open</span>(o))</span><br><span class="line">                            <span class="keyword">for</span> o <span class="keyword">in</span> (path/<span class="string">&#x27;valid&#x27;</span>/<span class="string">&#x27;3&#x27;</span>).ls()])</span><br><span class="line">valid_3_tens = valid_3_tens.<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">valid_7_tens = torch.stack([tensor(Image.<span class="built_in">open</span>(o))</span><br><span class="line">                            <span class="keyword">for</span> o <span class="keyword">in</span> (path/<span class="string">&#x27;valid&#x27;</span>/<span class="string">&#x27;7&#x27;</span>).ls()])</span><br><span class="line">valid_7_tens = valid_7_tens.<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">valid_3_tens.shape,valid_7_tens.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">1010</span>, <span class="number">28</span>, <span class="number">28</span>]), torch.Size([<span class="number">1028</span>, <span class="number">28</span>, <span class="number">28</span>]))</span><br></pre></td></tr></table></figure>
<p>在进行操作时检查形状是一个好习惯。在这里我们看到两个张量，一个代表了 1,010 张大小为 28×28 的 3 的验证集，另一个代表了 1,028 张大小为 28×28 的 7 的验证集。</p>
<p>我们最终想要编写一个函数<code>is_3</code>，它将决定任意图像是 3 还是 7。它将通过确定任意图像更接近我们的两个“理想数字”中的哪一个来实现这一点。为此，我们需要定义<em>距离</em>的概念——即，计算两个图像之间距离的函数。</p>
<p>我们可以编写一个简单的函数，使用与我们在上一节中编写的表达式非常相似的表达式来计算平均绝对误差：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_distance</span>(<span class="params">a,b</span>): <span class="keyword">return</span> (a-b).<span class="built_in">abs</span>().mean((-<span class="number">1</span>,-<span class="number">2</span>))</span><br><span class="line">mnist_distance(a_3, mean3)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.1114</span>)</span><br></pre></td></tr></table></figure>
<p>这是我们先前为这两个图像之间的距离计算的相同值，理想数字 3 <code>mean_3</code>和任意样本 3 <code>a_3</code>，它们都是形状为<code>[28,28]</code>的单个图像张量。</p>
<p>但是要计算整体准确度的指标，我们需要计算验证集中<em>每张</em>图像到理想数字 3 的距离。我们如何进行这种计算？我们可以编写一个循环，遍历验证集张量<code>valid_3_tens</code>中堆叠的所有单图像张量，其形状为<code>[1010,28,28]</code>，表示 1,010 张图像。但是有一种更好的方法。</p>
<p>当我们使用相同的距离函数，设计用于比较两个单个图像，但将表示 3 的验证集张量<code>valid_3_tens</code>作为参数传入时，会发生一些有趣的事情：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">valid_3_dist = mnist_distance(valid_3_tens, mean3)</span><br><span class="line">valid_3_dist, valid_3_dist.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([<span class="number">0.1050</span>, <span class="number">0.1526</span>, <span class="number">0.1186</span>,  ..., <span class="number">0.1122</span>, <span class="number">0.1170</span>, <span class="number">0.1086</span>]),</span><br><span class="line"> torch.Size([<span class="number">1010</span>]))</span><br></pre></td></tr></table></figure>
<p>它没有抱怨形状不匹配，而是为每个单个图像返回了一个距离（即，长度为 1,010 的秩-1 张量）。这是如何发生的？</p>
<p>再看看我们的函数<code>mnist_distance</code>，您会看到我们在那里有减法<code>(a-b)</code>。魔术技巧在于 PyTorch 在尝试在不同秩的两个张量之间执行简单的减法操作时，将使用<em>广播</em>：它将自动扩展秩较小的张量，使其大小与秩较大的张量相同。广播是一种重要的功能，使张量代码更容易编写。</p>
<p>在广播后，使两个参数张量具有相同的秩后，PyTorch 对于秩相同的两个张量应用其通常的逻辑：它对两个张量的每个对应元素执行操作，并返回张量结果。例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) + tensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>因此，在这种情况下，PyTorch 将<code>mean3</code>视为一个表示单个图像的秩-2 张量，就好像它是 1,010 个相同图像的副本，然后从我们的验证集中的每个 3 中减去每个副本。您期望这个张量的形状是什么？在查看这里的答案之前，请尝试自己想出来：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(valid_3_tens-mean3).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">1010</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure>
<p>我们正在计算我们的理想数字 3 与验证集中的每个 1,010 个 3 之间的差异，对于每个 28×28 图像，结果形状为<code>[1010,28,28]</code>。</p>
<p>有关广播实现的一些重要要点，使其不仅对于表达性有价值，而且对于性能也有价值：</p>
<ul>
<li><p>PyTorch 实际上并没有将<code>mean3</code>复制 1,010 次。它<em>假装</em>它是一个具有该形状的张量，但不分配任何额外内存。</p>
</li>
<li><p>它在 C 中完成整个计算（或者，如果您使用 GPU，则在 CUDA 中，相当于 GPU 上的 C），比纯 Python 快数万倍（在 GPU 上甚至快数百万倍！）。</p>
</li>
</ul>
<p>这适用于 PyTorch 中所有广播和逐元素操作和函数。<em>这是您要了解的最重要的技术，以创建高效的 PyTorch 代码。</em></p>
<p>接下来在<code>mnist_distance</code>中我们看到<code>abs</code>。现在您可能能猜到将其应用于张量时会发生什么。它将方法应用于张量中的每个单独元素，并返回结果的张量（即，它逐元素应用方法）。因此，在这种情况下，我们将得到 1,010 个绝对值。</p>
<p>最后，我们的函数调用<code>mean((-1,-2))</code>。元组<code>(-1,-2)</code>表示一系列轴。在 Python 中，<code>-1</code>指的是最后一个元素，<code>-2</code>指的是倒数第二个元素。因此，在这种情况下，这告诉 PyTorch 我们要对张量的最后两个轴的值进行平均。最后两个轴是图像的水平和垂直维度。在对最后两个轴进行平均后，我们只剩下第一个张量轴，它索引我们的图像，这就是为什么我们的最终大小是<code>(1010)</code>。换句话说，对于每个图像，我们对该图像中所有像素的强度进行了平均。</p>
<p>在本书中，我们将学习更多关于广播的知识，特别是在第十七章中，并且也会经常进行实践。</p>
<p>我们可以使用<code>mnist_distance</code>来确定一幅图像是否为 3，方法是使用以下逻辑：如果问题中的数字与理想的 3 之间的距离小于到理想的 7 的距离，则它是一个 3。这个函数将自动进行广播，并逐个应用，就像所有 PyTorch 函数和运算符一样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_3</span>(<span class="params">x</span>): <span class="keyword">return</span> mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)</span><br></pre></td></tr></table></figure>
<p>让我们在我们的示例案例上测试一下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">is_3(a_3), is_3(a_3).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="literal">True</span>), tensor(<span class="number">1.</span>))</span><br></pre></td></tr></table></figure>
<p>请注意，当我们将布尔响应转换为浮点数时，<code>True</code>会得到<code>1.0</code>，<code>False</code>会得到<code>0.0</code>。</p>
<p>由于广播，我们还可以在所有 3 的完整验证集上进行测试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">is_3(valid_3_tens)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>,  ..., <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>
<p>现在我们可以计算每个 3 和 7 的准确率，方法是对所有 3 的函数取平均值，对所有 7 的函数取其倒数的平均值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">accuracy_3s =      is_3(valid_3_tens).<span class="built_in">float</span>() .mean()</span><br><span class="line">accuracy_7s = (<span class="number">1</span> - is_3(valid_7_tens).<span class="built_in">float</span>()).mean()</span><br><span class="line"></span><br><span class="line">accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/<span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.9168</span>), tensor(<span class="number">0.9854</span>), tensor(<span class="number">0.9511</span>))</span><br></pre></td></tr></table></figure>
<p>这看起来是一个相当不错的开始！我们在 3 和 7 上都获得了超过 90%的准确率，我们已经看到了如何使用广播方便地定义度量。但让我们诚实一点：3 和 7 是非常不同的数字。到目前为止，我们只对 10 个可能的数字中的 2 个进行分类。所以我们需要做得更好！</p>
<p>为了做得更好，也许现在是时候尝试一个真正学习的系统了，一个可以自动修改自身以提高性能的系统。换句话说，现在是时候谈论训练过程和 SGD 了。</p>
<h1 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h1><p>你还记得 Arthur Samuel 在第一章中描述机器学习的方式吗？</p>
<blockquote>
<p>假设我们安排一些自动手段来测试任何当前权重分配的有效性，以实际性能为基础，并提供一种机制来改变权重分配以最大化性能。我们不需要详细了解这种程序的细节，就可以看到它可以完全自动化，并且可以看到一个这样编程的机器会从中学习。</p>
</blockquote>
<p>正如我们讨论过的，这是让我们拥有一个可以变得越来越好的模型的关键，可以学习。但我们的像素相似性方法实际上并没有做到这一点。我们没有任何权重分配，也没有任何根据测试权重分配的有效性来改进的方法。换句话说，我们无法通过修改一组参数来改进我们的像素相似性方法。为了充分利用深度学习的力量，我们首先必须按照 Samuel 描述的方式来表示我们的任务。</p>
<p>与其尝试找到图像与“理想图像”之间的相似性，我们可以查看每个单独的像素，并为每个像素提出一组权重，使得最高的权重与最有可能为特定类别的黑色像素相关联。例如，向右下方的像素不太可能被激活为 7，因此它们对于 7 的权重应该很低，但它们很可能被激活为 8，因此它们对于 8 的权重应该很高。这可以表示为一个函数和每个可能类别的一组权重值，例如，成为数字 8 的概率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pr_eight</span>(<span class="params">x,w</span>) = (x*w).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>在这里，我们假设<code>X</code>是图像，表示为一个向量—换句话说，所有行都堆叠在一起形成一个长长的单行。我们假设权重是一个向量<code>W</code>。如果我们有了这个函数，我们只需要一种方法来更新权重，使它们变得更好一点。通过这种方法，我们可以重复这个步骤多次，使权重变得越来越好，直到我们能够使它们尽可能好。</p>
<p>我们希望找到导致我们的函数对于那些是 8 的图像结果高，对于那些不是的图像结果低的向量<code>W</code>的特定值。搜索最佳向量<code>W</code>是搜索最佳函数以识别 8 的一种方式。（因为我们还没有使用深度神经网络，我们受到我们的函数能力的限制，我们将在本章后面解决这个约束。）</p>
<p>更具体地说，<font color=orange>以下是将这个函数转化为机器学习分类器所需的步骤：</font></p>
<ol>
<li><p><em>初始化</em>权重。</p>
</li>
<li><p>对于每个图像，使用这些权重来<em>预测</em>它是 3 还是 7。</p>
</li>
<li><p>基于这些预测，计算模型有多好（它的<em>损失</em>）。</p>
</li>
<li><p>计算<em>梯度</em>，它衡量了每个权重的变化如何改变损失。</p>
</li>
<li><p>根据这个计算，<em>改变</em>（即，改变）所有权重。</p>
</li>
<li><p>回到步骤 2 并<em>重复</em>这个过程。</p>
</li>
<li><p>迭代直到你决定<em>停止</em>训练过程（例如，因为模型已经足够好或者你不想再等待了）。</p>
<!-- htmlmin:ignore -->
<p><pre class="mermaid">graph TD<br>A[初始化权重] —&gt; B[预测图像是3或7]<br>B —&gt; C[计算模型损失]<br>C —&gt; D[计算梯度]<br>D —&gt; E[更新权重]<br>E —&gt; F{停止条件满足？}<br>F — 否 —&gt; B<br>F — 是 —&gt; G[结束训练]</p>
<p>style A fill:#f9f,stroke:#333<br>style B fill:#bbf,stroke:#333<br>style C fill:#bfb,stroke:#333<br>style D fill:#ffb,stroke:#333<br>style E fill:#fbb,stroke:#333<br>style F fill:#fbf,stroke:#333,shape:hexagon<br>style G fill:#9f9,stroke:#333&lt;/pre&gt;</p>
<!-- htmlmin:ignore -->
<p>这七个步骤，如图 4-1 所示，是所有深度学习模型训练的关键。深度学习完全依赖于这些步骤，这是非常令人惊讶和反直觉的。令人惊奇的是，这个过程可以解决如此复杂的问题。但是，正如你将看到的，它确实可以！</p>
</li>
</ol>
<p><img src="/image/dlcf_0401.png" alt="显示梯度下降步骤的图表"></p>
<h6 id="图-4-1-梯度下降过程"><a href="#图-4-1-梯度下降过程" class="headerlink" title="图 4-1. 梯度下降过程"></a>图 4-1. 梯度下降过程</h6><p>每个步骤都有许多方法，我们将在本书的其余部分学习它们。这些细节对于深度学习从业者来说非常重要，但事实证明，对于每个步骤的一般方法都遵循一些基本原则。以下是一些建议：</p>
<font color = cred>初始化</font>

<p>我们将参数初始化为随机值。这可能听起来令人惊讶。我们当然可以做其他选择，比如将它们初始化为该类别激活该像素的百分比—但由于我们已经知道我们有一种方法来改进这些权重，结果证明只是从随机权重开始就可以完全正常运行。</p>
<font color = cred>损失</font>

<p>这就是 Samuel 所说的<em>根据实际表现测试任何当前权重分配的有效性</em>。我们需要一个函数，如果模型的表现好，它将返回一个小的数字（标准方法是将小的损失视为好的，大的损失视为坏的，尽管这只是一种约定）。</p>
<font color = cred>步骤</font>

<p>一个简单的方法来判断一个权重是否应该增加一点或减少一点就是尝试一下：增加一点权重，看看损失是增加还是减少。一旦找到正确的方向，你可以再多改变一点或少改变一点，直到找到一个效果好的量。然而，这很慢！正如我们将看到的，微积分的魔力使我们能够直接找出每个权重应该朝哪个方向改变，大概改变多少，而不必尝试所有这些小的改变。这样做的方法是通过计算<em>梯度</em>。这只是一种性能优化；我们也可以通过使用更慢的手动过程得到完全相同的结果。</p>
<font color = cred>停止</font>

<p>一旦我们决定要为模型训练多少个周期（之前的列表中给出了一些建议），我们就会应用这个决定。对于我们的数字分类器，我们会继续训练，直到模型的准确率开始变差，或者我们用完时间为止。</p>
<p>在将这些步骤应用于我们的图像分类问题之前，让我们在一个更简单的情况下看看它们是什么样子。首先我们将定义一个非常简单的函数，二次函数—假设这是我们的损失函数，<code>x</code>是函数的权重参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="keyword">return</span> x**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>这是该函数的图表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(f, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x**2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in07.png" alt=""></p>
<p>我们之前描述的步骤序列从选择参数的随机值开始，并计算损失的值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_function(f, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x**2&#x27;</span>)</span><br><span class="line">plt.scatter(-<span class="number">1.5</span>, f(-<span class="number">1.5</span>), color=<span class="string">&#x27;red&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in08.png" alt=""></p>
<p>现在我们来看看如果我们稍微增加或减少参数会发生什么—<em>调整</em>。这只是特定点的斜率：</p>
<p><img src="/image/dlcf_04in09.png" alt="显示在某一点的斜率的平方函数的图表"></p>
<p>我们可以稍微改变我们的权重朝着斜坡的方向，计算我们的损失和调整，然后再重复几次。最终，我们将到达曲线上的最低点：</p>
<p><img src="/image/dlcf_04in10.png" alt="梯度下降的示意图"></p>
<p>这个基本思想最早可以追溯到艾萨克·牛顿，他指出我们可以以这种方式优化任意函数。无论我们的函数变得多么复杂，梯度下降的这种基本方法不会有太大变化。我们在本书后面看到的唯一微小变化是一些方便的方法，可以让我们更快地找到更好的步骤。</p>
<h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>唯一的魔法步骤是计算梯度的部分。正如我们提到的，我们使用微积分作为性能优化；它让我们更快地计算当我们调整参数时我们的损失会上升还是下降。换句话说，梯度将告诉我们我们需要改变每个权重多少才能使我们的模型更好。</p>
<p>您可能还记得高中微积分课上的<em>导数</em>告诉您函数参数的变化会如何改变其结果。如果不记得，不用担心；我们很多人高中毕业后就忘了微积分！但在继续之前，您需要对导数有一些直观的理解，所以如果您对此一头雾水，可以前往 Khan Academy 完成<a target="_blank" rel="noopener" href="https://oreil.ly/nyd0R">基本导数课程</a>。您不必自己计算导数；您只需要知道导数是什么。</p>
<p>导数的关键点在于：对于任何函数，比如我们在前一节中看到的二次函数，我们可以计算它的导数。导数是另一个函数。它计算的是变化，而不是值。例如，在值为 3 时，二次函数的导数告诉我们函数在值为 3 时的变化速度。更具体地说，您可能还记得梯度被定义为<em>上升/水平移动</em>；也就是说，函数值的变化除以参数值的变化。当我们知道我们的函数将如何变化时，我们就知道我们需要做什么来使它变小。这是机器学习的关键：有一种方法来改变函数的参数使其变小。微积分为我们提供了一个计算的捷径，即导数，它让我们直接计算我们函数的梯度。</p>
<p>一个重要的事情要注意的是我们的函数有很多需要调整的权重，所以当我们计算导数时，我们不会得到一个数字，而是很多个—每个权重都有一个梯度。但在这里没有数学上的技巧；您可以计算相对于一个权重的导数，将其他所有权重视为常数，然后对每个其他权重重复这个过程。这就是计算所有梯度的方法，对于每个权重。</p>
<p>刚才我们提到您不必自己计算任何梯度。这怎么可能？令人惊讶的是，PyTorch 能够自动计算几乎任何函数的导数！而且，它计算得非常快。大多数情况下，它至少与您手动创建的任何导数函数一样快。让我们看一个例子。</p>
<p>首先，让我们选择一个张量数值，我们想要梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xt = tensor(<span class="number">3.</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<p>注意特殊方法<code>requires_grad_</code>？这是我们告诉 PyTorch 我们想要计算梯度的神奇咒语。这实质上是给变量打上标记，这样 PyTorch 就会记住如何计算您要求的其他直接计算的梯度。</p>
<font color=orange> 好像仅仅是求导 </font>

<h1 id="Alexis-说"><a href="#Alexis-说" class="headerlink" title="Alexis 说"></a>Alexis 说</h1><p>如果您来自数学或物理学，这个 API 可能会让您困惑。在这些背景下，函数的“梯度”只是另一个函数（即，它的导数），因此您可能期望与梯度相关的 API 提供给您一个新函数。但在深度学习中，“梯度”通常意味着函数的导数在特定参数值处的<em>值</em>。PyTorch API 也将重点放在参数上，而不是您实际计算梯度的函数。起初可能感觉有些反常，但这只是一个不同的视角。</p>
<p>现在我们用这个值计算我们的函数。注意 PyTorch 打印的不仅是计算的值，还有一个提示，它有一个梯度函数将在需要时用来计算我们的梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yt = f(xt)</span><br><span class="line">yt</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">9.</span>, grad_fn=&lt;PowBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>最后，我们告诉 PyTorch 为我们计算梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yt.backward()</span><br></pre></td></tr></table></figure>
<p>这里的<code>backward</code>指的是<em>反向传播</em>，这是计算每一层导数的过程的名称。我们将在第十七章中看到这是如何精确完成的，当我们从头开始计算深度神经网络的梯度时。这被称为网络的<em>反向传播</em>，与<em>前向传播</em>相对，前者是计算激活的地方。如果<code>backward</code>只是被称为<code>calculate_grad</code>，生活可能会更容易，但深度学习的人确实喜欢在任何地方添加行话！</p>
<p>我们现在可以通过检查我们张量的<code>grad</code>属性来查看梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xt.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure>
<p>如果您记得高中微积分规则，<code>x**2</code>的导数是<code>2*x</code>，我们有<code>x=3</code>，所以梯度应该是<code>2*3=6</code>，这就是 PyTorch 为我们计算的结果！</p>
<p>现在我们将重复前面的步骤，但使用一个向量参数来计算我们的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xt = tensor([<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">10.</span>]).requires_grad_()</span><br><span class="line">xt</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">3.</span>,  <span class="number">4.</span>, <span class="number">10.</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>并且我们将<code>sum</code>添加到我们的函数中，以便它可以接受一个向量（即，一个秩为 1 的张量）并返回一个标量（即，一个秩为 0 的张量）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="keyword">return</span> (x**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">yt = f(xt)</span><br><span class="line">yt</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">125.</span>, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>我们的梯度是<code>2*xt</code>，正如我们所期望的！</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yt.backward()</span><br><span class="line">xt.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">20.</span>])</span><br></pre></td></tr></table></figure>
<p>梯度告诉我们函数的斜率；它们并不告诉我们要调整参数多远。但它们确实给了我们一些想法：如果斜率非常大，那可能意味着我们需要更多的调整，而如果斜率非常小，那可能意味着我们接近最优值。</p>
<h2 id="使用学习率进行步进"><a href="#使用学习率进行步进" class="headerlink" title="使用学习率进行步进"></a>使用学习率进行步进</h2><p>根据梯度值来决定如何改变我们的参数是深度学习过程中的一个重要部分。几乎所有方法都从一个基本思想开始，即将梯度乘以一些小数字，称为<em>学习率</em>（LR）。学习率通常是 0.001 到 0.1 之间的数字，尽管它可以是任何值。通常人们通过尝试几个学习率来选择一个，并找出哪个在训练后产生最佳模型的结果（我们将在本书后面展示一个更好的方法，称为<em>学习率查找器</em>）。一旦选择了学习率，您可以使用这个简单函数调整参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w -= w.grad * lr</span><br></pre></td></tr></table></figure>
<p>这被称为<em>调整</em>您的参数，使用<em>优化步骤</em>。</p>
<p>如果您选择的学习率太低，可能意味着需要执行很多步骤。图 4-2 说明了这一点。</p>
<p><img src="/image/dlcf_0402.png" alt="梯度下降示例，学习率过低"></p>
<h6 id="图-4-2。学习率过低的梯度下降"><a href="#图-4-2。学习率过低的梯度下降" class="headerlink" title="图 4-2。学习率过低的梯度下降"></a>图 4-2。学习率过低的梯度下降</h6><p>但选择一个学习率太高的学习率更糟糕——它可能导致损失变得<em>更糟</em>，正如我们在图 4-3 中看到的！</p>
<p><img src="/image/dlcf_0403.png" alt="学习率过高的梯度下降示例"></p>
<h6 id="图-4-3-学习率过高的梯度下降"><a href="#图-4-3-学习率过高的梯度下降" class="headerlink" title="图 4-3. 学习率过高的梯度下降"></a>图 4-3. 学习率过高的梯度下降</h6><p>如果学习率太高，它也可能会“弹跳”而不是发散；图 4-4 显示了这样做需要许多步骤才能成功训练。</p>
<p><img src="/image/dlcf_0404.png" alt="带有弹跳学习率的梯度下降示例"></p>
<h6 id="图-4-4-带有弹跳学习率的梯度下降"><a href="#图-4-4-带有弹跳学习率的梯度下降" class="headerlink" title="图 4-4. 带有弹跳学习率的梯度下降"></a>图 4-4. 带有弹跳学习率的梯度下降</h6><p>现在让我们在一个端到端的示例中应用所有这些。</p>
<h2 id="一个端到端的-SGD-示例"><a href="#一个端到端的-SGD-示例" class="headerlink" title="一个端到端的 SGD 示例"></a>一个端到端的 SGD 示例</h2><p>我们已经看到如何使用梯度来最小化我们的损失。现在是时候看一个 SGD 示例，并看看如何找到最小值来训练模型以更好地拟合数据。</p>
<p>让我们从一个简单的合成示例模型开始。想象一下，您正在测量过山车通过顶峰时的速度。它会开始快速，然后随着上坡而变慢；在顶部最慢，然后在下坡时再次加速。您想建立一个关于速度随时间变化的模型。如果您每秒手动测量速度 20 秒，它可能看起来像这样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time = torch.arange(<span class="number">0</span>,<span class="number">20</span>).<span class="built_in">float</span>(); time</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>,</span><br><span class="line"> &gt; <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">speed = torch.randn(<span class="number">20</span>)*<span class="number">3</span> + <span class="number">0.75</span>*(time-<span class="number">9.5</span>)**<span class="number">2</span> + <span class="number">1</span></span><br><span class="line">plt.scatter(time,speed);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in11.png" alt=""></p>
<p>我们添加了一些随机噪声，因为手动测量不够精确。这意味着很难回答问题：过山车的速度是多少？使用 SGD，我们可以尝试找到一个与我们的观察相匹配的函数。我们无法考虑每种可能的函数，所以让我们猜测它将是二次的；即，一个形式为<code>a*(time**2)+(b*time)+c</code>的函数。</p>
<p>我们希望清楚地区分函数的输入（我们测量过山车速度的时间）和其参数（定义<em>我们正在尝试的</em>二次函数的值）。因此，让我们将参数收集在一个参数中，从而在函数的签名中分离输入<code>t</code>和参数<code>params</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">t, params</span>):</span><br><span class="line">    a,b,c = params</span><br><span class="line">    <span class="keyword">return</span> a*(t**<span class="number">2</span>) + (b*t) + c</span><br></pre></td></tr></table></figure>
<p>换句话说，我们已经将找到最佳拟合数据的最佳函数的问题限制为找到最佳<em>二次</em>函数。这极大地简化了问题，因为每个二次函数都由三个参数<code>a</code>、<code>b</code>和<code>c</code>完全定义。因此，要找到最佳二次函数，我们只需要找到最佳的<code>a</code>、<code>b</code>和<code>c</code>的值。</p>
<p>如果我们可以解决二次函数的三个参数的问题，我们就能够对其他具有更多参数的更复杂函数应用相同的方法——比如神经网络。让我们先找到<code>f</code>的参数，然后我们将回来对 MNIST 数据集使用神经网络做同样的事情。</p>
<p>首先，我们需要定义“最佳”是什么意思。我们通过选择一个<em>损失函数</em>来精确定义这一点，该函数将根据预测和目标返回一个值，其中函数的较低值对应于“更好”的预测。对于连续数据，通常使用<em>均方误差</em>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mse</span>(<span class="params">preds, targets</span>): <span class="keyword">return</span> ((preds-targets)**<span class="number">2</span>).mean()</span><br></pre></td></tr></table></figure>
<p>现在，让我们按照我们的七步流程进行工作。</p>
<h3 id="第一步：初始化参数"><a href="#第一步：初始化参数" class="headerlink" title="第一步：初始化参数"></a>第一步：初始化参数</h3><p>首先，我们将参数初始化为随机值，并告诉 PyTorch 我们要使用<code>requires_grad_</code>跟踪它们的梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params = torch.randn(<span class="number">3</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<h3 id="第二步：计算预测"><a href="#第二步：计算预测" class="headerlink" title="第二步：计算预测"></a>第二步：计算预测</h3><p>接下来，我们计算预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preds = f(time, params)</span><br></pre></td></tr></table></figure>
<p>让我们创建一个小函数来查看我们的预测与目标的接近程度，并看一看：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_preds</span>(<span class="params">preds, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>: ax=plt.subplots()[<span class="number">1</span>]</span><br><span class="line">    ax.scatter(time, speed)</span><br><span class="line">    ax.scatter(time, to_np(preds), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    ax.set_ylim(-<span class="number">300</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_preds(preds)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in12.png" alt=""></p>
<p>这看起来并不接近——我们的随机参数表明过山车最终会倒退，因为我们有负速度！</p>
<h3 id="第一步：初始化参数-1"><a href="#第一步：初始化参数-1" class="headerlink" title="第一步：初始化参数"></a>第一步：初始化参数</h3><p>首先，我们将参数初始化为随机值，并告诉 PyTorch 我们要使用<code>requires_grad_</code>跟踪它们的梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params = torch.randn(<span class="number">3</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<h3 id="第二步：计算预测-1"><a href="#第二步：计算预测-1" class="headerlink" title="第二步：计算预测"></a>第二步：计算预测</h3><p>接下来，我们计算预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preds = f(time, params)</span><br></pre></td></tr></table></figure>
<p>让我们创建一个小函数来查看我们的预测与目标的接近程度，并看一看：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_preds</span>(<span class="params">preds, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>: ax=plt.subplots()[<span class="number">1</span>]</span><br><span class="line">    ax.scatter(time, speed)</span><br><span class="line">    ax.scatter(time, to_np(preds), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    ax.set_ylim(-<span class="number">300</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_preds(preds)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in12.png" alt=""></p>
<p>这看起来并不接近——我们的随机参数表明过山车最终会倒退，因为我们有负速度！</p>
<h3 id="第三步：计算损失"><a href="#第三步：计算损失" class="headerlink" title="第三步：计算损失"></a>第三步：计算损失</h3><p>我们计算损失如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = mse(preds, speed)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">25823.8086</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>我们的目标现在是改进这一点。为了做到这一点，我们需要知道梯度。</p>
<h3 id="第四步：计算梯度"><a href="#第四步：计算梯度" class="headerlink" title="第四步：计算梯度"></a>第四步：计算梯度</h3><p>下一步是计算梯度，或者近似参数需要如何改变：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">params.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">53195.8594</span>,  -<span class="number">3419.7146</span>,   -<span class="number">253.8908</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params.grad * <span class="number">1e-5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.5320</span>, -<span class="number">0.0342</span>, -<span class="number">0.0025</span>])</span><br></pre></td></tr></table></figure>
<p>我们可以利用这些梯度来改进我们的参数。我们需要选择一个学习率（我们将在下一章中讨论如何在实践中做到这一点；现在，我们将使用 1e-5 或 0.00001）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.7658</span>, -<span class="number">0.7506</span>,  <span class="number">1.3525</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="第-5-步：调整权重"><a href="#第-5-步：调整权重" class="headerlink" title="第 5 步：调整权重"></a>第 5 步：调整权重</h3><p>现在我们需要根据刚刚计算的梯度更新参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-5</span></span><br><span class="line">params.data -= lr * params.grad.data</span><br><span class="line">params.grad = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h1 id="Alexis-说-1"><a href="#Alexis-说-1" class="headerlink" title="Alexis 说"></a>Alexis 说</h1><p>理解这一点取决于记住最近的历史。为了计算梯度，我们在<code>loss</code>上调用<code>backward</code>。但是这个<code>loss</code>本身是通过<code>mse</code>计算的，而<code>mse</code>又以<code>preds</code>作为输入，<code>preds</code>是使用<code>f</code>计算的，<code>f</code>以<code>params</code>作为输入，<code>params</code>是我们最初调用<code>required_grads_</code>的对象，这是最初的调用，现在允许我们在<code>loss</code>上调用<code>backward</code>。这一系列函数调用代表了函数的数学组合，使得 PyTorch 能够在幕后使用微积分的链式法则来计算这些梯度。</p>
<p>让我们看看损失是否有所改善：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = f(time,params)</span><br><span class="line">mse(preds, speed)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">5435.5366</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>再看一下图表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_preds(preds)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in13.png" alt=""></p>
<p>我们需要重复这个过程几次，所以我们将创建一个应用一步的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply_step</span>(<span class="params">params, prn=<span class="literal">True</span></span>):</span><br><span class="line">    preds = f(time, params)</span><br><span class="line">    loss = mse(preds, speed)</span><br><span class="line">    loss.backward()</span><br><span class="line">    params.data -= lr * params.grad.data</span><br><span class="line">    params.grad = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> prn: <span class="built_in">print</span>(loss.item())</span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure>
<h3 id="第-6-步：重复这个过程"><a href="#第-6-步：重复这个过程" class="headerlink" title="第 6 步：重复这个过程"></a>第 6 步：重复这个过程</h3><p>现在我们进行迭代。通过循环和进行许多改进，我们希望达到一个好的结果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): apply_step(params)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5435.53662109375</span></span><br><span class="line"><span class="number">1577.4495849609375</span></span><br><span class="line"><span class="number">847.3780517578125</span></span><br><span class="line"><span class="number">709.22265625</span></span><br><span class="line"><span class="number">683.0757446289062</span></span><br><span class="line"><span class="number">678.12451171875</span></span><br><span class="line"><span class="number">677.1839599609375</span></span><br><span class="line"><span class="number">677.0025024414062</span></span><br><span class="line"><span class="number">676.96435546875</span></span><br><span class="line"><span class="number">676.9537353515625</span></span><br></pre></td></tr></table></figure>
<p>损失正在下降，正如我们所希望的！但仅仅看这些损失数字掩盖了一个事实，即每次迭代代表尝试一个完全不同的二次函数，以找到最佳可能的二次函数。如果我们不打印出损失函数，而是在每一步绘制函数，我们可以看到形状是如何接近我们的数据的最佳可能的二次函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">_,axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>,figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axs: show_preds(apply_step(params, <span class="literal">False</span>), ax)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in14.png" alt=""></p>
<h3 id="第-7-步：停止"><a href="#第-7-步：停止" class="headerlink" title="第 7 步：停止"></a>第 7 步：停止</h3><p>我们刚刚决定在任意选择的 10 个 epochs 后停止。在实践中，我们会观察训练和验证损失以及我们的指标，以决定何时停止，正如我们所讨论的那样。</p>
<h2 id="总结梯度下降"><a href="#总结梯度下降" class="headerlink" title="总结梯度下降"></a>总结梯度下降</h2><p>现在您已经看到每个步骤中发生的事情，让我们再次看一下我们的梯度下降过程的图形表示（图 4-5）并进行一个快速回顾。</p>
<p><img src="/image/dlcf_0405.png" alt="显示梯度下降步骤的图表"></p>
<h6 id="图-4-5-梯度下降过程"><a href="#图-4-5-梯度下降过程" class="headerlink" title="图 4-5. 梯度下降过程"></a>图 4-5. 梯度下降过程</h6><p>在开始时，我们模型的权重可以是随机的（从头开始训练）或来自预训练模型（迁移学习）。在第一种情况下，我们从输入得到的输出与我们想要的完全无关，即使在第二种情况下，预训练模型也可能不太擅长我们所针对的特定任务。因此，模型需要学习更好的权重。</p>
<p>我们首先将模型给出的输出与我们的目标进行比较（我们有标记数据，所以我们知道模型应该给出什么结果），使用一个<em>损失函数</em>，它返回一个数字，我们希望通过改进我们的权重使其尽可能低。为了做到这一点，我们从训练集中取出一些数据项（如图像）并将它们馈送给我们的模型。我们使用我们的损失函数比较相应的目标，我们得到的分数告诉我们我们的预测有多么错误。然后我们稍微改变权重使其稍微更好。</p>
<p>为了找出如何改变权重使损失稍微变好，我们使用微积分来计算<em>梯度</em>。（实际上，我们让 PyTorch 为我们做这个！）让我们考虑一个类比。想象一下你在山上迷路了，你的车停在最低点。为了找到回去的路，你可能会朝着随机方向走，但那可能不会有太大帮助。由于你知道你的车在最低点，你最好是往下走。通过始终朝着最陡峭的下坡方向迈出一步，你最终应该到达目的地。我们使用梯度的大小（即坡度的陡峭程度）来告诉我们应该迈多大一步；具体来说，我们将梯度乘以我们选择的一个称为<em>学习率</em>的数字来决定步长。然后我们<em>迭代</em>直到达到最低点，那将是我们的停车场；然后我们可以<em>停止</em>。</p>
<p>我们刚刚看到的所有内容都可以直接转换到 MNIST 数据集，除了损失函数。现在让我们看看如何定义一个好的训练目标。</p>
<h1 id="MNIST-损失函数"><a href="#MNIST-损失函数" class="headerlink" title="MNIST 损失函数"></a>MNIST 损失函数</h1><p>我们已经有了我们的<code>x</code>—也就是我们的自变量，图像本身。我们将它们全部连接成一个单一的张量，并且还将它们从矩阵列表（一个秩为 3 的张量）转换为向量列表（一个秩为 2 的张量）。我们可以使用<code>view</code>来做到这一点，<code>view</code>是一个 PyTorch 方法，可以改变张量的形状而不改变其内容。<code>-1</code>是<code>view</code>的一个特殊参数，意思是“使这个轴尽可能大以适应所有数据”：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_x = torch.cat([stacked_threes, stacked_sevens]).view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br></pre></td></tr></table></figure>
<p>我们需要为每张图片标记。我们将使用<code>1</code>表示 3，<code>0</code>表示 7：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_y = tensor([<span class="number">1</span>]*<span class="built_in">len</span>(threes) + [<span class="number">0</span>]*<span class="built_in">len</span>(sevens)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">train_x.shape,train_y.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">12396</span>, <span class="number">784</span>]), torch.Size([<span class="number">12396</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>在 PyTorch 中，当索引时，<code>Dataset</code>需要返回一个<code>(x,y)</code>元组。Python 提供了一个<code>zip</code>函数，当与<code>list</code>结合使用时，可以简单地实现这个功能：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dset = <span class="built_in">list</span>(<span class="built_in">zip</span>(train_x,train_y))</span><br><span class="line">x,y = dset[<span class="number">0</span>]</span><br><span class="line">x.shape,y</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">784</span>]), tensor([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">valid_y = tensor([<span class="number">1</span>]*<span class="built_in">len</span>(valid_3_tens) + [<span class="number">0</span>]*<span class="built_in">len</span>(valid_7_tens)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">valid_dset = <span class="built_in">list</span>(<span class="built_in">zip</span>(valid_x,valid_y))</span><br></pre></td></tr></table></figure>
<p>现在我们需要为每个像素（最初是随机的）分配一个权重（这是我们七步过程中的<em>初始化</em>步骤）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">size, std=<span class="number">1.0</span></span>): <span class="keyword">return</span> (torch.randn(size)*std).requires_grad_()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = init_params((<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>函数<code>weights*pixels</code>不够灵活—当像素等于 0 时，它总是等于 0（即其<em>截距</em>为 0）。你可能还记得高中数学中线的公式是<code>y=w*x+b</code>；我们仍然需要<code>b</code>。我们也会将其初始化为一个随机数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bias = init_params(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>在神经网络中，方程<code>y=w*x+b</code>中的<code>w</code>被称为<em>权重</em>，<code>b</code>被称为<em>偏置</em>。权重和偏置一起构成<em>参数</em>。</p>
<h1 id="术语：参数"><a href="#术语：参数" class="headerlink" title="术语：参数"></a>术语：参数</h1><p>模型的<em>权重</em>和<em>偏置</em>。权重是方程<code>w*x+b</code>中的<code>w</code>，偏置是该方程中的<code>b</code>。</p>
<p>现在我们可以为一张图片计算一个预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_x[<span class="number">0</span>]*weights.T).<span class="built_in">sum</span>() + bias</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">20.2336</span>], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>虽然我们可以使用 Python 的<code>for</code>循环来计算每张图片的预测，但那将非常慢。因为 Python 循环不在 GPU 上运行，而且因为 Python 在一般情况下循环速度较慢，我们需要尽可能多地使用高级函数来表示模型中的计算。</p>
<p>在这种情况下，有一个非常方便的数学运算可以为矩阵的每一行计算<code>w*x</code>—它被称为<em>矩阵乘法</em>。图 4-6 展示了矩阵乘法的样子。</p>
<p><img src="/image/dlcf_0406.png" alt="矩阵乘法"></p>
<h6 id="图-4-6-矩阵乘法"><a href="#图-4-6-矩阵乘法" class="headerlink" title="图 4-6. 矩阵乘法"></a>图 4-6. 矩阵乘法</h6><p>这幅图展示了两个矩阵<code>A</code>和<code>B</code>相乘。结果的每个项目，我们称之为<code>AB</code>，包含了<code>A</code>的对应行的每个项目与<code>B</code>的对应列的每个项目相乘后相加。例如，第 1 行第 2 列（带有红色边框的黄色点）计算为<math alttext="a 下标 1，1 乘以 b 下标 1，2 加上 a 下标 1，2 乘以 b 下标 2，2">。如果您需要复习矩阵乘法，我们建议您查看 Khan Academy 的“矩阵乘法简介”，因为这是深度学习中最重要的数学运算。</p>
<p>在 Python 中，矩阵乘法用<code>@</code>运算符表示。让我们试一试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear1</span>(<span class="params">xb</span>): <span class="keyword">return</span> xb@weights + bias</span><br><span class="line">preds = linear1(train_x)</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">20.2336</span>],</span><br><span class="line">        [<span class="number">17.0644</span>],</span><br><span class="line">        [<span class="number">15.2384</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">18.3804</span>],</span><br><span class="line">        [<span class="number">23.8567</span>],</span><br><span class="line">        [<span class="number">28.6816</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>第一个元素与我们之前计算的相同，正如我们所期望的。这个方程<code>batch @ weights + bias</code>是任何神经网络的两个基本方程之一（另一个是<em>激活函数</em>，我们马上会看到）。</p>
<p>让我们检查我们的准确性。为了确定输出代表 3 还是 7，我们只需检查它是否大于 0，因此我们可以计算每个项目的准确性（使用广播，因此没有循环！）如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">corrects = (preds&gt;<span class="number">0.0</span>).<span class="built_in">float</span>() == train_y</span><br><span class="line">corrects</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corrects.<span class="built_in">float</span>().mean().item()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4912068545818329</span></span><br></pre></td></tr></table></figure>
<p>现在让我们看看一个权重的微小变化对准确性的影响是什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights[<span class="number">0</span>] *= <span class="number">1.0001</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = linear1(train_x)</span><br><span class="line">((preds&gt;<span class="number">0.0</span>).<span class="built_in">float</span>() == train_y).<span class="built_in">float</span>().mean().item()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4912068545818329</span></span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，我们需要梯度来通过 SGD 改进我们的模型，为了计算梯度，我们需要一个<em>损失函数</em>，它代表了我们的模型有多好。这是因为梯度是损失函数如何随着对权重的微小调整而变化的度量。</p>
<p>因此，我们需要选择一个损失函数。显而易见的方法是使用准确性作为我们的度量标准，也作为我们的损失函数。在这种情况下，我们将为每个图像计算我们的预测，收集这些值以计算总体准确性，然后计算每个权重相对于总体准确性的梯度。</p>
<p>不幸的是，我们在这里有一个重要的技术问题。函数的梯度是其<em>斜率</em>，或者是其陡峭程度，可以定义为<em>上升与下降</em>——也就是说，函数值上升或下降的幅度，除以我们改变输入的幅度。我们可以用数学方式写成：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(y_new – y_old) / (x_new – x_old)</span><br></pre></td></tr></table></figure>
<p>当<code>x_new</code>非常类似于<code>x_old</code>时，这给出了梯度的良好近似，这意味着它们的差异非常小。但是，只有当预测从 3 变为 7，或者反之时，准确性才会发生变化。问题在于，从<code>x_old</code>到<code>x_new</code>的权重的微小变化不太可能导致任何预测发生变化，因此<code>(y_new - y_old)</code>几乎总是为 0。换句话说，梯度几乎在任何地方都为 0。</p>
<p>权重值的微小变化通常不会改变准确性。这意味着使用准确性作为损失函数是没有用的——如果我们这样做，大多数时候我们的梯度将为 0，模型将无法从该数字中学习。</p>
<h1 id="Sylvain-说"><a href="#Sylvain-说" class="headerlink" title="Sylvain 说"></a>Sylvain 说</h1><p>在数学术语中，准确性是一个几乎在任何地方都是常数的函数（除了阈值 0.5），因此它的导数几乎在任何地方都是零（在阈值处为无穷大）。这将导致梯度为 0 或无穷大，这对于更新模型是没有用的。</p>
<p>相反，我们需要一个损失函数，当我们的权重导致稍微更好的预测时，给出稍微更好的损失。那么，“稍微更好的预测”具体是什么样呢？在这种情况下，这意味着如果正确答案是 3，则分数稍高，或者如果正确答案是 7，则分数稍低。</p>
<p>现在让我们编写这样一个函数。它是什么形式？</p>
<p>损失函数接收的不是图像本身，而是模型的预测。因此，让我们做一个参数<code>prds</code>，值在 0 和 1 之间，其中每个值是图像是 3 的预测。它是一个矢量（即，一个秩-1 张量），索引在图像上。</p>
<p>损失函数的目的是衡量预测值与真实值之间的差异，即目标（又称标签）。因此，让我们再做一个参数<code>trgts</code>，其值为 0 或 1，告诉图像实际上是 3 还是不是 3。它也是一个矢量（即，另一个秩-1 张量），索引在图像上。</p>
<p>例如，假设我们有三幅图像，我们知道其中一幅是 3，一幅是 7，一幅是 3。假设我们的模型以高置信度（<code>0.9</code>）预测第一幅是 3，以轻微置信度（<code>0.4</code>）预测第二幅是 7，以公平置信度（<code>0.2</code>），但是错误地预测最后一幅是 7。这意味着我们的损失函数将接收这些值作为其输入：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trgts  = tensor([<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">prds   = tensor([<span class="number">0.9</span>, <span class="number">0.4</span>, <span class="number">0.2</span>])</span><br></pre></td></tr></table></figure>
<p>这是一个测量<code>predictions</code>和<code>targets</code>之间距离的损失函数的第一次尝试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.where(targets==<span class="number">1</span>, <span class="number">1</span>-predictions, predictions).mean()</span><br></pre></td></tr></table></figure>
<p>我们正在使用一个新函数，<code>torch.where(a,b,c)</code>。这与运行列表推导<code>[b[i] if a[i] else c[i] for i in range(len(a))]</code>相同，只是它在张量上运行，以 C/CUDA 速度运行。简单来说，这个函数将衡量每个预测离 1 有多远，如果应该是 1 的话，以及它离 0 有多远，如果应该是 0 的话，然后它将取所有这些距离的平均值。</p>
<h1 id="阅读文档"><a href="#阅读文档" class="headerlink" title="阅读文档"></a>阅读文档</h1><p>学习 PyTorch 这样的函数很重要，因为在 Python 中循环张量的速度是 Python 速度，而不是 C/CUDA 速度！现在尝试运行<code>help(torch.where)</code>来阅读此函数的文档，或者更好的是，在 PyTorch 文档站点上查找。</p>
<p>让我们在我们的<code>prds</code>和<code>trgts</code>上尝试一下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.where(trgts==<span class="number">1</span>, <span class="number">1</span>-prds, prds)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.1000</span>, <span class="number">0.4000</span>, <span class="number">0.8000</span>])</span><br></pre></td></tr></table></figure>
<p>您可以看到，当预测更准确时，当准确预测更自信时（绝对值更高），以及当不准确预测更不自信时，此函数返回较低的数字。在 PyTorch 中，我们始终假设损失函数的较低值更好。由于我们需要一个标量作为最终损失，<code>mnist_loss</code>取前一个张量的平均值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist_loss(prds,trgts)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.4333</span>)</span><br></pre></td></tr></table></figure>
<p>例如，如果我们将对一个“错误”目标的预测从<code>0.2</code>更改为<code>0.8</code>，损失将减少，表明这是一个更好的预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist_loss(tensor([<span class="number">0.9</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]),trgts)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.2333</span>)</span><br></pre></td></tr></table></figure>
<p><code>mnist_loss</code>当前定义的一个问题是它假设预测总是在 0 和 1 之间。因此，我们需要确保这实际上是这种情况！恰好有一个函数可以做到这一点，让我们来看看。</p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><code>sigmoid</code>函数总是输出一个介于 0 和 1 之间的数字。它的定义如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>): <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+torch.exp(-x))</span><br></pre></td></tr></table></figure>
<p>PyTorch 为我们定义了一个加速版本，所以我们不需要自己的。这是深度学习中一个重要的函数，因为我们经常希望确保数值在 0 和 1 之间。它看起来是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(torch.sigmoid, title=<span class="string">&#x27;Sigmoid&#x27;</span>, <span class="built_in">min</span>=-<span class="number">4</span>, <span class="built_in">max</span>=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in15.png" alt=""></p>
<p>正如您所看到的，它接受任何输入值，正数或负数，并将其压缩为 0 和 1 之间的输出值。它还是一个只上升的平滑曲线，这使得 SGD 更容易找到有意义的梯度。</p>
<p>让我们更新<code>mnist_loss</code>，首先对输入应用<code>sigmoid</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    predictions = predictions.sigmoid()</span><br><span class="line">    <span class="keyword">return</span> torch.where(targets==<span class="number">1</span>, <span class="number">1</span>-predictions, predictions).mean()</span><br></pre></td></tr></table></figure>
<p>现在我们可以确信我们的损失函数将起作用，即使预测不在 0 和 1 之间。唯一需要的是更高的预测对应更高的置信度。</p>
<p>定义了一个损失函数，现在是一个好时机回顾为什么这样做。毕竟，我们已经有了一个度量标准，即整体准确率。那么为什么我们定义了一个损失？</p>
<p>关键区别在于指标用于驱动人类理解，而损失用于驱动自动学习。为了驱动自动学习，损失必须是一个具有有意义导数的函数。它不能有大的平坦部分和大的跳跃，而必须是相当平滑的。这就是为什么我们设计了一个损失函数，可以对置信水平的小变化做出响应。这个要求意味着有时它实际上并不完全反映我们试图实现的目标，而是我们真正目标和一个可以使用其梯度进行优化的函数之间的妥协。损失函数是针对数据集中的每个项目计算的，然后在时代结束时，所有损失值都被平均，整体均值被报告为时代。</p>
<p>另一方面，指标是我们关心的数字。这些是在每个时代结束时打印的值，告诉我们我们的模型表现如何。重要的是，我们学会关注这些指标，而不是损失，来评估模型的性能。</p>
<h2 id="SGD-和小批次"><a href="#SGD-和小批次" class="headerlink" title="SGD 和小批次"></a>SGD 和小批次</h2><p>现在我们有了一个适合驱动 SGD 的损失函数，我们可以考虑学习过程的下一阶段涉及的一些细节，即根据梯度改变或更新权重。这被称为<em>优化步骤</em>。</p>
<p>要进行优化步骤，我们需要计算一个或多个数据项的损失。我们应该使用多少？我们可以为整个数据集计算并取平均值，或者可以为单个数据项计算。但这两种方法都不理想。为整个数据集计算将需要很长时间。为单个数据项计算将不会使用太多信息，因此会导致不精确和不稳定的梯度。您将费力更新权重，但只考虑这将如何改善模型在该单个数据项上的性能。</p>
<p>因此，我们做出妥协：我们一次计算几个数据项的平均损失。这被称为<em>小批次</em>。小批次中的数据项数量称为<em>批次大小</em>。较大的批次大小意味着您将从损失函数中获得更准确和稳定的数据集梯度估计，但这将需要更长时间，并且您将在每个时代处理较少的小批次。选择一个好的批次大小是您作为深度学习从业者需要做出的决定之一，以便快速准确地训练您的模型。我们将在本书中讨论如何做出这个选择。</p>
<p>使用小批次而不是在单个数据项上计算梯度的另一个很好的理由是，实际上，我们几乎总是在加速器上进行训练，例如 GPU。这些加速器只有在一次有很多工作要做时才能表现良好，因此如果我们可以给它们很多数据项来处理，这将是有帮助的。使用小批次是实现这一目标的最佳方法之一。但是，如果您一次给它们太多数据来处理，它们会耗尽内存——让 GPU 保持愉快也是棘手的！</p>
<p>正如您在第二章中关于数据增强的讨论中所看到的，如果我们在训练过程中可以改变一些东西，我们会获得更好的泛化能力。我们可以改变的一个简单而有效的事情是将哪些数据项放入每个小批次。我们通常不是简单地按顺序枚举我们的数据集，而是在每个时代之前随机洗牌，然后创建小批次。PyTorch 和 fastai 提供了一个类，可以为您执行洗牌和小批次整理，称为<code>DataLoader</code>。</p>
<p><code>DataLoader</code>可以将任何 Python 集合转换为一个迭代器，用于生成多个批次，就像这样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">coll = <span class="built_in">range</span>(<span class="number">15</span>)</span><br><span class="line">dl = DataLoader(coll, batch_size=<span class="number">5</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(dl)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[tensor([ <span class="number">3</span>, <span class="number">12</span>,  <span class="number">8</span>, <span class="number">10</span>,  <span class="number">2</span>]),</span><br><span class="line"> tensor([ <span class="number">9</span>,  <span class="number">4</span>,  <span class="number">7</span>, <span class="number">14</span>,  <span class="number">5</span>]),</span><br><span class="line"> tensor([ <span class="number">1</span>, <span class="number">13</span>,  <span class="number">0</span>,  <span class="number">6</span>, <span class="number">11</span>])]</span><br></pre></td></tr></table></figure>
<p>对于训练模型，我们不只是想要任何 Python 集合，而是一个包含独立和相关变量（模型的输入和目标）的集合。包含独立和相关变量元组的集合在 PyTorch 中被称为<code>Dataset</code>。这是一个极其简单的<code>Dataset</code>的示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ds = L(<span class="built_in">enumerate</span>(string.ascii_lowercase))</span><br><span class="line">ds</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#26) [(0, &#x27;a&#x27;),(1, &#x27;b&#x27;),(2, &#x27;c&#x27;),(3, &#x27;d&#x27;),(4, &#x27;e&#x27;),(5, &#x27;f&#x27;),(6, &#x27;g&#x27;),(7,</span></span><br><span class="line"> &gt; <span class="string">&#x27;h&#x27;</span>),(<span class="number">8</span>, <span class="string">&#x27;i&#x27;</span>),(<span class="number">9</span>, <span class="string">&#x27;j&#x27;</span>)...]</span><br></pre></td></tr></table></figure>
<p>当我们将<code>Dataset</code>传递给<code>DataLoader</code>时，我们将得到许多批次，它们本身是表示独立和相关变量批次的张量元组：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dl = DataLoader(ds, batch_size=<span class="number">6</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(dl)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[(tensor([<span class="number">17</span>, <span class="number">18</span>, <span class="number">10</span>, <span class="number">22</span>,  <span class="number">8</span>, <span class="number">14</span>]), (<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;o&#x27;</span>)),</span><br><span class="line"> (tensor([<span class="number">20</span>, <span class="number">15</span>,  <span class="number">9</span>, <span class="number">13</span>, <span class="number">21</span>, <span class="number">12</span>]), (<span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;j&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;m&#x27;</span>)),</span><br><span class="line"> (tensor([ <span class="number">7</span>, <span class="number">25</span>,  <span class="number">6</span>,  <span class="number">5</span>, <span class="number">11</span>, <span class="number">23</span>]), (<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;x&#x27;</span>)),</span><br><span class="line"> (tensor([ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">0</span>, <span class="number">24</span>, <span class="number">19</span>, <span class="number">16</span>]), (<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;q&#x27;</span>)),</span><br><span class="line"> (tensor([<span class="number">2</span>, <span class="number">4</span>]), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;e&#x27;</span>))]</span><br></pre></td></tr></table></figure>
<p>我们现在准备为使用 SGD 的模型编写我们的第一个训练循环！</p>
<h1 id="把所有东西放在一起"><a href="#把所有东西放在一起" class="headerlink" title="把所有东西放在一起"></a>把所有东西放在一起</h1><p>是时候实现我们在图 4-1 中看到的过程了。在代码中，我们的过程将为每个时期实现类似于这样的东西：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> dl:</span><br><span class="line">    pred = model(x)</span><br><span class="line">    loss = loss_func(pred, y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    parameters -= parameters.grad * lr</span><br></pre></td></tr></table></figure>
<p>首先，让我们重新初始化我们的参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = init_params((<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>))</span><br><span class="line">bias = init_params(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><code>DataLoader</code>可以从<code>Dataset</code>创建：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dl = DataLoader(dset, batch_size=<span class="number">256</span>)</span><br><span class="line">xb,yb = first(dl)</span><br><span class="line">xb.shape,yb.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">256</span>, <span class="number">784</span>]), torch.Size([<span class="number">256</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>我们将对验证集执行相同的操作：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">valid_dl = DataLoader(valid_dset, batch_size=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<p>让我们创建一个大小为 4 的小批量进行测试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = train_x[:<span class="number">4</span>]</span><br><span class="line">batch.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = linear1(batch)</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">11.1002</span>],</span><br><span class="line">        [  <span class="number">5.9263</span>],</span><br><span class="line">        [  <span class="number">9.9627</span>],</span><br><span class="line">        [ -<span class="number">8.1484</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = mnist_loss(preds, train_y[:<span class="number">4</span>])</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.5006</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>现在我们可以计算梯度了：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">weights.grad.shape,weights.grad.mean(),bias.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">784</span>, <span class="number">1</span>]), tensor(-<span class="number">0.0001</span>), tensor([-<span class="number">0.0008</span>]))</span><br></pre></td></tr></table></figure>
<p>让我们把所有这些放在一个函数中：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calc_grad</span>(<span class="params">xb, yb, model</span>):</span><br><span class="line">    preds = model(xb)</span><br><span class="line">    loss = mnist_loss(preds, yb)</span><br><span class="line">    loss.backward()</span><br></pre></td></tr></table></figure>
<p>并测试它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_grad(batch, train_y[:<span class="number">4</span>], linear1)</span><br><span class="line">weights.grad.mean(),bias.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(-<span class="number">0.0002</span>), tensor([-<span class="number">0.0015</span>]))</span><br></pre></td></tr></table></figure>
<p>但是看看如果我们调用两次会发生什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_grad(batch, train_y[:<span class="number">4</span>], linear1)</span><br><span class="line">weights.grad.mean(),bias.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(-<span class="number">0.0003</span>), tensor([-<span class="number">0.0023</span>]))</span><br></pre></td></tr></table></figure>
<p>梯度已经改变了！这是因为<code>loss.backward</code> <em>添加</em>了<code>loss</code>的梯度到当前存储的任何梯度中。因此，我们首先必须将当前梯度设置为 0：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights.grad.zero_()</span><br><span class="line">bias.grad.zero_();</span><br></pre></td></tr></table></figure>
<h1 id="原地操作"><a href="#原地操作" class="headerlink" title="原地操作"></a>原地操作</h1><p>PyTorch 中以下划线结尾的方法会<em>原地</em>修改它们的对象。例如，<code>bias.zero_</code>会将张量<code>bias</code>的所有元素设置为 0。</p>
<p>我们唯一剩下的步骤是根据梯度和学习率更新权重和偏差。当我们这样做时，我们必须告诉 PyTorch 不要对这一步骤进行梯度计算，否则当我们尝试在下一个批次计算导数时会变得混乱！如果我们将张量的<code>data</code>属性赋值，PyTorch 将不会对该步骤进行梯度计算。这是我们用于一个时期的基本训练循环：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">model, lr, params</span>):</span><br><span class="line">    <span class="keyword">for</span> xb,yb <span class="keyword">in</span> dl:</span><br><span class="line">        calc_grad(xb, yb, model)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">            p.data -= p.grad*lr</span><br><span class="line">            p.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>我们还想通过查看验证集的准确性来检查我们的表现。要决定输出是否代表 3 或 7，我们只需检查它是否大于 0。因此，我们可以计算每个项目的准确性（使用广播，所以没有循环！）如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(preds&gt;<span class="number">0.0</span>).<span class="built_in">float</span>() == train_y[:<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<p>这给了我们计算验证准确性的这个函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_accuracy</span>(<span class="params">xb, yb</span>):</span><br><span class="line">    preds = xb.sigmoid()</span><br><span class="line">    correct = (preds&gt;<span class="number">0.5</span>) == yb</span><br><span class="line">    <span class="keyword">return</span> correct.<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>我们可以检查它是否有效：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_accuracy(linear1(batch), train_y[:<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.5000</span>)</span><br></pre></td></tr></table></figure>
<p>然后把批次放在一起：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">validate_epoch</span>(<span class="params">model</span>):</span><br><span class="line">    accs = [batch_accuracy(model(xb), yb) <span class="keyword">for</span> xb,yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(torch.stack(accs).mean().item(), <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">validate_epoch(linear1)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.5219</span></span><br></pre></td></tr></table></figure>
<p>这是我们的起点。让我们训练一个时期，看看准确性是否提高：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1.</span></span><br><span class="line">params = weights,bias</span><br><span class="line">train_epoch(linear1, lr, params)</span><br><span class="line">validate_epoch(linear1)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.6883</span></span><br></pre></td></tr></table></figure>
<p>然后再做几次：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    train_epoch(linear1, lr, params)</span><br><span class="line">    <span class="built_in">print</span>(validate_epoch(linear1), end=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.8314</span> <span class="number">0.9017</span> <span class="number">0.9227</span> <span class="number">0.9349</span> <span class="number">0.9438</span> <span class="number">0.9501</span> <span class="number">0.9535</span> <span class="number">0.9564</span> <span class="number">0.9594</span> <span class="number">0.9618</span> <span class="number">0.9613</span></span><br><span class="line"> &gt; <span class="number">0.9638</span> <span class="number">0.9643</span> <span class="number">0.9652</span> <span class="number">0.9662</span> <span class="number">0.9677</span> <span class="number">0.9687</span> <span class="number">0.9691</span> <span class="number">0.9691</span> <span class="number">0.9696</span></span><br></pre></td></tr></table></figure>
<p>看起来不错！我们的准确性已经接近“像素相似性”方法的准确性，我们已经创建了一个通用的基础可以构建。我们的下一步将是创建一个将处理 SGD 步骤的对象。在 PyTorch 中，它被称为<em>优化器</em>。</p>
<h2 id="创建一个优化器"><a href="#创建一个优化器" class="headerlink" title="创建一个优化器"></a>创建一个优化器</h2><p>因为这是一个如此通用的基础，PyTorch 提供了一些有用的类来使实现更容易。我们可以做的第一件事是用 PyTorch 的<code>nn.Linear</code>模块替换我们的<code>linear</code>函数。<em>模块</em>是从 PyTorch <code>nn.Module</code>类继承的类的对象。这个类的对象的行为与标准 Python 函数完全相同，您可以使用括号调用它们，它们将返回模型的激活。</p>
<p><code>nn.Linear</code>做的事情与我们的<code>init_params</code>和<code>linear</code>一样。它包含了<em>权重</em>和<em>偏差</em>在一个单独的类中。这是我们如何复制上一节中的模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear_model = nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>每个 PyTorch 模块都知道它有哪些可以训练的参数；它们可以通过<code>parameters</code>方法获得：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w,b = linear_model.parameters()</span><br><span class="line">w.shape,b.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">1</span>, <span class="number">784</span>]), torch.Size([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>我们可以使用这些信息创建一个优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicOptim</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,params,lr</span>): <span class="variable language_">self</span>.params,<span class="variable language_">self</span>.lr = <span class="built_in">list</span>(params),lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.params: p.data -= p.grad.data * <span class="variable language_">self</span>.lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_grad</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.params: p.grad = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过传入模型的参数来创建优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opt = BasicOptim(linear_model.parameters(), lr)</span><br></pre></td></tr></table></figure>
<p>我们的训练循环现在可以简化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">for</span> xb,yb <span class="keyword">in</span> dl:</span><br><span class="line">        calc_grad(xb, yb, model)</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br></pre></td></tr></table></figure>
<p>我们的验证函数不需要任何更改：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">validate_epoch(linear_model)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4157</span></span><br></pre></td></tr></table></figure>
<p>让我们把我们的小训练循环放在一个函数中，让事情变得更简单：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        train_epoch(model)</span><br><span class="line">        <span class="built_in">print</span>(validate_epoch(model), end=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果与上一节相同：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_model(linear_model, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4932</span> <span class="number">0.8618</span> <span class="number">0.8203</span> <span class="number">0.9102</span> <span class="number">0.9331</span> <span class="number">0.9468</span> <span class="number">0.9555</span> <span class="number">0.9629</span> <span class="number">0.9658</span> <span class="number">0.9673</span> <span class="number">0.9687</span></span><br><span class="line"> &gt; <span class="number">0.9707</span> <span class="number">0.9726</span> <span class="number">0.9751</span> <span class="number">0.9761</span> <span class="number">0.9761</span> <span class="number">0.9775</span> <span class="number">0.978</span> <span class="number">0.9785</span> <span class="number">0.9785</span></span><br></pre></td></tr></table></figure>
<p>fastai 提供了<code>SGD</code>类，默认情况下与我们的<code>BasicOptim</code>做相同的事情：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear_model = nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line">opt = SGD(linear_model.parameters(), lr)</span><br><span class="line">train_model(linear_model, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4932</span> <span class="number">0.852</span> <span class="number">0.8335</span> <span class="number">0.9116</span> <span class="number">0.9326</span> <span class="number">0.9473</span> <span class="number">0.9555</span> <span class="number">0.9624</span> <span class="number">0.9648</span> <span class="number">0.9668</span> <span class="number">0.9692</span></span><br><span class="line"> &gt; <span class="number">0.9712</span> <span class="number">0.9731</span> <span class="number">0.9746</span> <span class="number">0.9761</span> <span class="number">0.9765</span> <span class="number">0.9775</span> <span class="number">0.978</span> <span class="number">0.9785</span> <span class="number">0.9785</span></span><br></pre></td></tr></table></figure>
<p>fastai 还提供了<code>Learner.fit</code>，我们可以使用它来代替<code>train_model</code>。要创建一个<code>Learner</code>，我们首先需要创建一个<code>DataLoaders</code>，通过传入我们的训练和验证<code>DataLoader</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dls = DataLoaders(dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>要创建一个<code>Learner</code>而不使用应用程序（如<code>cnn_learner</code>），我们需要传入本章中创建的所有元素：<code>DataLoaders</code>，模型，优化函数（将传递参数），损失函数，以及可选的任何要打印的指标：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = Learner(dls, nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>), opt_func=SGD,</span><br><span class="line">                loss_func=mnist_loss, metrics=batch_accuracy)</span><br></pre></td></tr></table></figure>
<p>现在我们可以调用<code>fit</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit(<span class="number">10</span>, lr=lr)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>batch_accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.636857</td>
<td>0.503549</td>
<td>0.495584</td>
<td>00:00</td>
</tr>
<tr>
<td>1</td>
<td>0.545725</td>
<td>0.170281</td>
<td>0.866045</td>
<td>00:00</td>
</tr>
<tr>
<td>2</td>
<td>0.199223</td>
<td>0.184893</td>
<td>0.831207</td>
<td>00:00</td>
</tr>
<tr>
<td>3</td>
<td>0.086580</td>
<td>0.107836</td>
<td>0.911187</td>
<td>00:00</td>
</tr>
<tr>
<td>4</td>
<td>0.045185</td>
<td>0.078481</td>
<td>0.932777</td>
<td>00:00</td>
</tr>
<tr>
<td>5</td>
<td>0.029108</td>
<td>0.062792</td>
<td>0.946516</td>
<td>00:00</td>
</tr>
<tr>
<td>6</td>
<td>0.022560</td>
<td>0.053017</td>
<td>0.955348</td>
<td>00:00</td>
</tr>
<tr>
<td>7</td>
<td>0.019687</td>
<td>0.046500</td>
<td>0.962218</td>
<td>00:00</td>
</tr>
<tr>
<td>8</td>
<td>0.018252</td>
<td>0.041929</td>
<td>0.965162</td>
<td>00:00</td>
</tr>
<tr>
<td>9</td>
<td>0.017402</td>
<td>0.038573</td>
<td>0.967615</td>
<td>00:00</td>
</tr>
</tbody>
</table>
</div>
<p>正如您所看到的，PyTorch 和 fastai 类并没有什么神奇之处。它们只是方便的预打包部件，使您的生活变得更轻松！（它们还提供了许多我们将在未来章节中使用的额外功能。）</p>
<p>有了这些类，我们现在可以用神经网络替换我们的线性模型。</p>
<h1 id="添加非线性"><a href="#添加非线性" class="headerlink" title="添加非线性"></a>添加非线性</h1><p>到目前为止，我们已经有了一个优化函数的一般过程，并且我们已经在一个无聊的函数上尝试了它：一个简单的线性分类器。线性分类器在能做什么方面受到限制。为了使其更复杂一些（并且能够处理更多任务），我们需要在两个线性分类器之间添加一些非线性（即与 ax+b 不同的东西）——这就是给我们神经网络的东西。</p>
<p>这是一个基本神经网络的完整定义：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">simple_net</span>(<span class="params">xb</span>):</span><br><span class="line">    res = xb@w1 + b1</span><br><span class="line">    res = res.<span class="built_in">max</span>(tensor(<span class="number">0.0</span>))</span><br><span class="line">    res = res@w2 + b2</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>就是这样！在<code>simple_net</code>中，我们只有两个线性分类器，它们之间有一个<code>max</code>函数。</p>
<p>在这里，<code>w1</code>和<code>w2</code>是权重张量，<code>b1</code>和<code>b2</code>是偏置张量；也就是说，这些参数最初是随机初始化的，就像我们在上一节中所做的一样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w1 = init_params((<span class="number">28</span>*<span class="number">28</span>,<span class="number">30</span>))</span><br><span class="line">b1 = init_params(<span class="number">30</span>)</span><br><span class="line">w2 = init_params((<span class="number">30</span>,<span class="number">1</span>))</span><br><span class="line">b2 = init_params(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>关键点是<code>w1</code>有 30 个输出激活（这意味着<code>w2</code>必须有 30 个输入激活，以便匹配）。这意味着第一层可以构建 30 个不同的特征，每个特征代表不同的像素混合。您可以将<code>30</code>更改为任何您喜欢的数字，以使模型更复杂或更简单。</p>
<p>那个小函数<code>res.max(tensor(0.0))</code>被称为<em>修正线性单元</em>，也被称为<em>ReLU</em>。我们认为我们都可以同意<em>修正线性单元</em>听起来相当花哨和复杂…但实际上，它不过是<code>res.max(tensor(0.0))</code>——换句话说，用零替换每个负数。这个微小的函数在 PyTorch 中也可以作为<code>F.relu</code>使用：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(F.relu)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in16.png" alt=""></p>
<h1 id="Jeremy-说"><a href="#Jeremy-说" class="headerlink" title="Jeremy 说"></a>Jeremy 说</h1><p>深度学习中有大量行话，包括<em>修正线性单元</em>等术语。绝大多数这些行话并不比我们在这个例子中看到的一行代码更复杂。事实是，学术界为了发表论文，他们需要让论文听起来尽可能令人印象深刻和复杂。他们通过引入行话来实现这一点。不幸的是，这导致该领域变得比应该更加令人生畏和难以进入。您确实需要学习这些行话，因为否则论文和教程对您来说将毫无意义。但这并不意味着您必须觉得这些行话令人生畏。只需记住，当您遇到以前未见过的单词或短语时，它几乎肯定是指一个非常简单的概念。</p>
<p>基本思想是通过使用更多的线性层，我们的模型可以进行更多的计算，从而模拟更复杂的函数。但是，直接将一个线性布局放在另一个线性布局之后是没有意义的，因为当我们将事物相乘然后多次相加时，可以用不同的事物相乘然后只相加一次来替代！也就是说，一系列任意数量的线性层可以被替换为具有不同参数集的单个线性层。</p>
<p>但是，如果我们在它们之间放置一个非线性函数，比如<code>max</code>，这就不再成立了。现在每个线性层都有点解耦，可以做自己有用的工作。<code>max</code>函数特别有趣，因为它作为一个简单的<code>if</code>语句运行。</p>
<h1 id="Sylvain-说-1"><a href="#Sylvain-说-1" class="headerlink" title="Sylvain 说"></a>Sylvain 说</h1><p>数学上，我们说两个线性函数的组合是另一个线性函数。因此，我们可以堆叠任意多个线性分类器在一起，而它们之间没有非线性函数，这将与一个线性分类器相同。</p>
<p>令人惊讶的是，可以数学证明这个小函数可以解决任何可计算问题，只要你能找到<code>w1</code>和<code>w2</code>的正确参数，并且使这些矩阵足够大。对于任何任意波动的函数，我们可以将其近似为一堆连接在一起的线条；为了使其更接近波动函数，我们只需使用更短的线条。这被称为<em>通用逼近定理</em>。我们这里的三行代码被称为<em>层</em>。第一和第三行被称为<em>线性层</em>，第二行代码被称为<em>非线性</em>或<em>激活函数</em>。</p>
<p>就像在前一节中一样，我们可以利用 PyTorch 简化这段代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">simple_net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">30</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>nn.Sequential</code>创建一个模块，依次调用列出的每个层或函数。</p>
<p><code>nn.ReLU</code>是一个 PyTorch 模块，与<code>F.relu</code>函数完全相同。大多数可以出现在模型中的函数也有相同的模块形式。通常，只需将<code>F</code>替换为<code>nn</code>并更改大小写。在使用<code>nn.Sequential</code>时，PyTorch 要求我们使用模块版本。由于模块是类，我们必须实例化它们，这就是为什么在这个例子中看到<code>nn.ReLU</code>。</p>
<p>因为<code>nn.Sequential</code>是一个模块，我们可以获取它的参数，它将返回它包含的所有模块的所有参数的列表。让我们试一试！由于这是一个更深层的模型，我们将使用更低的学习率和更多的周期：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = Learner(dls, simple_net, opt_func=SGD,</span><br><span class="line">                loss_func=mnist_loss, metrics=batch_accuracy)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit(<span class="number">40</span>, <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>我们这里不展示 40 行输出，以节省空间；训练过程记录在<code>learn.recorder</code>中，输出表存储在<code>values</code>属性中，因此我们可以绘制训练过程中的准确性：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(L(learn.recorder.values).itemgot(<span class="number">2</span>));</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in17.png" alt=""></p>
<p>我们可以查看最终的准确性：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.recorder.values[-<span class="number">1</span>][<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.982826292514801</span></span><br></pre></td></tr></table></figure>
<p>在这一点上，我们有一些非常神奇的东西：</p>
<ul>
<li><p>给定正确的参数集，可以解决任何问题到任何精度的函数（神经网络）</p>
</li>
<li><p>找到任何函数的最佳参数集的方法（随机梯度下降）</p>
</li>
</ul>
<p>这就是为什么深度学习可以做出如此奇妙的事情。相信这些简单技术的组合确实可以解决任何问题是我们发现许多学生必须迈出的最大步骤之一。这似乎太好了，以至于难以置信——事情肯定应该比这更困难和复杂吧？我们的建议是：试一试！我们刚刚在 MNIST 数据集上尝试了一下，你已经看到了结果。由于我们自己从头开始做所有事情（除了计算梯度），所以你知道背后没有隐藏任何特殊的魔法。</p>
<h2 id="更深入地探讨"><a href="#更深入地探讨" class="headerlink" title="更深入地探讨"></a>更深入地探讨</h2><p>我们不必止步于只有两个线性层。我们可以添加任意数量的线性层，只要在每对线性层之间添加一个非线性。然而，正如您将了解的那样，模型变得越深，实际中优化参数就越困难。在本书的后面，您将学习一些简单但非常有效的训练更深层模型的技巧。</p>
<p>我们已经知道，一个带有两个线性层的单个非线性足以逼近任何函数。那么为什么要使用更深的模型呢？原因是性能。通过更深的模型（具有更多层），我们不需要使用太多参数；事实证明，我们可以使用更小的矩阵，更多的层，获得比使用更大的矩阵和少量层获得更好的结果。</p>
<p>这意味着我们可以更快地训练模型，并且它将占用更少的内存。在 1990 年代，研究人员如此专注于通用逼近定理，以至于很少有人尝试超过一个非线性。这种理论但不实际的基础阻碍了该领域多年。然而，一些研究人员确实尝试了深度模型，并最终能够证明这些模型在实践中表现得更好。最终，出现了理论结果，解释了为什么会发生这种情况。今天，几乎不可能找到任何人只使用一个非线性的神经网络。</p>
<p>当我们使用与我们在第一章中看到的相同方法训练一个 18 层模型时会发生什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dls = ImageDataLoaders.from_folder(path)</span><br><span class="line">learn = cnn_learner(dls, resnet18, pretrained=<span class="literal">False</span>,</span><br><span class="line">                    loss_func=F.cross_entropy, metrics=accuracy)</span><br><span class="line">learn.fit_one_cycle(<span class="number">1</span>, <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>时代</th>
<th>训练损失</th>
<th>验证损失</th>
<th>准确性</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.082089</td>
<td>0.009578</td>
<td>0.997056</td>
<td>00:11</td>
</tr>
</tbody>
</table>
</div>
<p>近乎 100%的准确性！这与我们简单的神经网络相比有很大的差异。但是在本书的剩余部分中，您将学习到一些小技巧，可以让您自己从头开始获得如此出色的结果。您已经了解了关键的基础知识。 （当然，即使您知道所有技巧，您几乎总是希望使用 PyTorch 和 fastai 提供的预构建类，因为它们可以帮助您省去自己考虑所有细节的麻烦。）</p>
<h1 id="术语回顾"><a href="#术语回顾" class="headerlink" title="术语回顾"></a>术语回顾</h1><p>恭喜：您现在知道如何从头开始创建和训练深度神经网络了！我们经历了很多步骤才达到这一点，但您可能会惊讶于它实际上是多么简单。</p>
<p>既然我们已经到了这一点，现在是一个很好的机会来定义和回顾一些术语和关键概念。</p>
<p>神经网络包含很多数字，但它们只有两种类型：计算的数字和这些数字计算出的参数。这给我们学习最重要的两个术语：</p>
<p>激活</p>
<p>计算的数字（线性和非线性层）</p>
<p>参数</p>
<p>随机初始化并优化的数字（即定义模型的数字）</p>
<p>在本书中，我们经常谈论激活和参数。请记住它们具有特定的含义。它们是数字。它们不是抽象概念，而是实际存在于您的模型中的具体数字。成为一名优秀的深度学习从业者的一部分是习惯于查看您的激活和参数，并绘制它们以及测试它们是否正确运行的想法。</p>
<p>我们的激活和参数都包含在 <em>张量</em> 中。这些只是正规形状的数组—例如，一个矩阵。矩阵有行和列；我们称这些为 <em>轴</em> 或 <em>维度</em>。张量的维度数是它的 <em>等级</em>。有一些特殊的张量：</p>
<ul>
<li><p>等级-0：标量</p>
</li>
<li><p>等级-1：向量</p>
</li>
<li><p>等级-2：矩阵</p>
</li>
</ul>
<p>神经网络包含多个层。每一层都是<em>线性</em>或<em>非线性</em>的。我们通常在神经网络中交替使用这两种类型的层。有时人们将线性层及其后续的非线性一起称为一个单独的层。是的，这很令人困惑。有时非线性被称为<em>激活函数</em>。</p>
<p>表 4-1 总结了与 SGD 相关的关键概念。</p>
<p>表 4-1. 深度学习词汇表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>术语</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>对负数返回 0 且不改变正数的函数。</td>
</tr>
<tr>
<td>小批量</td>
<td>一小组输入和标签，聚集在两个数组中。在这个批次上更新梯度下降步骤（而不是整个 epoch）。</td>
</tr>
<tr>
<td>前向传播</td>
<td>将模型应用于某些输入并计算预测。</td>
</tr>
<tr>
<td>损失</td>
<td>代表我们的模型表现如何（好或坏）的值。</td>
</tr>
<tr>
<td>梯度</td>
<td>损失相对于模型某个参数的导数。</td>
</tr>
<tr>
<td>反向传播</td>
<td>计算损失相对于所有模型参数的梯度。</td>
</tr>
<tr>
<td>梯度下降</td>
<td>沿着梯度相反方向迈出一步，使模型参数稍微变得更好。</td>
</tr>
<tr>
<td>学习率</td>
<td>当应用 SGD 更新模型参数时我们所采取的步骤的大小。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="选择你的冒险-提醒"><a href="#选择你的冒险-提醒" class="headerlink" title="选择你的冒险 提醒"></a><em>选择你的冒险</em> 提醒</h1><p>在你兴奋地想要窥探内部机制时，你选择跳过第 2 和第三章节了吗？好吧，这里提醒你现在回到第二章，因为你很快就会需要了解那些内容！</p>
<h1 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h1><ol>
<li><p>灰度图像在计算机上是如何表示的？彩色图像呢？</p>
<ul>
<li>灰度图像通过单通道的二维矩阵（每个像素为0-255的亮度值）表示，而彩色图像通过三通道（RGB）的三维数组（每个像素包含红、绿、蓝三个0-255的强度值）表示。</li>
</ul>
</li>
<li><p><code>MNIST_SAMPLE</code>数据集中的文件和文件夹是如何结构化的？为什么？</p>
<ul>
<li>MNIST_SAMPLE数据集通常按类别分层组织（如train/3、train/7和valid/3、valid/7），这种结构便于机器学习框架（如PyTorch的ImageFolder）自动识别标签并划分训练集/验证集，简化数据加载流程。</li>
</ul>
</li>
<li><p>解释“像素相似性”方法如何工作以对数字进行分类。</p>
<ul>
<li>与其尝试找到图像与“理想图像”之间的相似性，我们可以查看每个单独的像素，并为每个像素提出一组权重，使得最高的权重与最有可能为特定类别的黑色像素相关联。例如，向右下方的像素不太可能被激活为 7，因此它们对于 7 的权重应该很低，但它们很可能被激活为 8，因此它们对于 8 的权重应该很高。这可以表示为一个函数和每个可能类别的一组权重值，例如，成为数字 8 的概率</li>
</ul>
</li>
<li><p>什么是列表推导？现在创建一个从列表中选择奇数并将其加倍的列表推导。</p>
<ul>
<li>new_list = [x*2 for x in a_list if x%2 != 0]</li>
</ul>
</li>
<li><p>什么是秩-3 张量？</p>
<ul>
<li>秩-3 张量是具有三个独立维度的多维数组（形状如 (a, b, c)），可表示多矩阵堆叠或复杂三维数据（如视频帧序列、批量文本的词向量等）。</li>
</ul>
</li>
<li><p>张量秩和形状之间有什么区别？如何从形状中获取秩？</p>
<ul>
<li>张量秩指维度数量（如秩3是三维数组），形状描述各维度长度（如(2,3,4)）；秩等于形状元组的长度（len(tensor.shape)）。</li>
</ul>
</li>
<li><p>RMSE 和 L1 范数是什么？</p>
<ul>
<li>RMSE（均方根误差）是预测值与真实值误差平方均值的平方根，用于衡量回归模型精度；<script type="math/tex; mode=display">RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}</script></li>
</ul>
<ul>
<li>真实值：$y_i \in \mathbb{R}$  </li>
<li>预测值：$\hat{y}_i \in \mathbb{R}$  </li>
<li>样本数：$n \in \mathbb{N}^*$  </li>
</ul>
<ul>
<li>L1范数（如MAE）是误差绝对值的总和，常用于鲁棒性损失函数或稀疏正则化。<script type="math/tex; mode=display">L1 = \sum_{i=1}^{n} |y_i - \hat{y}_i|</script></li>
</ul>
<ul>
<li><strong>MAE（平均绝对误差）</strong>：<script type="math/tex; mode=display">MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|</script></li>
</ul>
</li>
</ol>
<ol>
<li><p>如何才能比 Python 循环快几千倍地一次性对数千个数字进行计算？</p>
<ul>
<li>将数据转换为 NumPy 数组，利用其底层C语言实现的向量化操作：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原生Python循环（慢）</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, ..., <span class="number">10000</span>]</span><br><span class="line">result = [x * <span class="number">2</span> + <span class="number">5</span> <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy向量化（快几千倍）</span></span><br><span class="line">data_np = np.array(data)</span><br><span class="line">result_np = data_np * <span class="number">2</span> + <span class="number">5</span>  <span class="comment"># 无显式循环，逐元素操作</span></span><br></pre></td></tr></table></figure>
<ul>
<li>超大规模数据时，通过 GPU 并行计算（需NVIDIA显卡）：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cupy <span class="keyword">as</span> cp</span><br><span class="line"></span><br><span class="line">data_gpu = cp.array(data)  <span class="comment"># 数据传至GPU显存</span></span><br><span class="line">result_gpu = cp.exp(data_gpu) * <span class="number">10</span>  <span class="comment"># GPU并行计算指数和乘法</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>对复杂逻辑，通过 JIT编译 生成机器码加速</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> jit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="meta">@jit(<span class="params">nopython=<span class="literal">True</span></span>)  </span><span class="comment"># 强制编译为原生机器码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_array</span>(<span class="params">arr</span>):</span><br><span class="line">    result = np.empty_like(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        result[i] = arr[i] ** <span class="number">2</span> + np.sin(arr[i])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">data = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">result = process_array(data)  <span class="comment"># 首次运行编译，后续调用极快</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个包含从 1 到 9 的数字的 3×3 张量或数组。将其加倍。选择右下角的四个数字。</p>
<ul>
<li>PyTorch<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 3x3 张量（数值1-9）</span></span><br><span class="line">tensor = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有元素加倍</span></span><br><span class="line">doubled_tensor = tensor * <span class="number">2</span>  <span class="comment"># tensor([[ 2,  4,  6], [ 8, 10, 12], [14, 16, 18]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择右下角四个数字（最后两行 &amp; 最后两列）</span></span><br><span class="line">selected = doubled_tensor[<span class="number">1</span>:, <span class="number">1</span>:]  <span class="comment"># tensor([[10, 12], [16, 18]])</span></span><br></pre></td></tr></table></figure></li>
<li>numpy<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 3x3 数组（数值1-9）</span></span><br><span class="line">array = np.arange(<span class="number">1</span>, <span class="number">10</span>).reshape(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有元素加倍</span></span><br><span class="line">doubled_array = array * <span class="number">2</span>  <span class="comment"># array([[ 2,  4,  6], [ 8, 10, 12], [14, 16, 18]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择右下角四个数字</span></span><br><span class="line">selected = doubled_array[<span class="number">1</span>:, <span class="number">1</span>:]  <span class="comment"># array([[10, 12], [16, 18]])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>广播是什么？</p>
<ul>
<li>广播是一种重要的功能，使张量代码更容易编写。在广播后，使两个参数张量具有相同的秩后，PyTorch 对于秩相同的两个张量应用其通常的逻辑：它对两个张量的每个对应元素执行操作，并返回张量结果。</li>
</ul>
</li>
<li><p>度量通常是使用训练集还是验证集计算的？为什么？</p>
<ul>
<li>验证集，排除过拟合状态。</li>
</ul>
</li>
<li><p>SGD 是什么？</p>
<ul>
<li>SGD（随机梯度下降，Stochastic Gradient Descent） 是一种用于优化模型参数的迭代算法，广泛应用于机器学习和深度学习。其核心思想是通过随机选取小批量数据（mini-batch） 计算梯度并更新参数，而非使用全部数据，从而显著提升训练效率。</li>
</ul>
</li>
<li><p>为什么 SGD 使用小批量？</p>
<ul>
<li><p>利用硬件并行加速，提升计算效率；</p>
</li>
<li><p>平衡梯度噪声与稳定性，避免剧烈震荡；</p>
</li>
<li><p>适应内存限制，灵活处理大规模数据。</p>
</li>
</ul>
</li>
<li><p>SGD 在机器学习中有哪七个步骤？</p>
<ul>
<li><p><em>初始化</em>权重。</p>
</li>
<li><p>对于每个图像，使用这些权重来<em>预测</em>它是 3 还是 7。</p>
</li>
<li><p>基于这些预测，计算模型有多好（它的<em>损失</em>）。</p>
</li>
<li><p>计算<em>梯度</em>，它衡量了每个权重的变化如何改变损失。</p>
</li>
<li><p>根据这个计算，<em>改变</em>（即，改变）所有权重。</p>
</li>
<li><p>回到步骤 2 并<em>重复</em>这个过程。</p>
</li>
<li><p>迭代直到你决定<em>停止</em>训练过程（例如，因为模型已经足够好或者你不想再等待了）。</p>
</li>
</ul>
</li>
<li><p>我们如何初始化模型中的权重？</p>
<ul>
<li>我们将参数初始化为随机值。这可能听起来令人惊讶。我们当然可以做其他选择，比如将它们初始化为该类别激活该像素的百分比—但由于我们已经知道我们有一种方法来改进这些权重，结果证明只是从随机权重开始就可以完全正常运行。</li>
</ul>
</li>
<li><p>什么是损失？</p>
<ul>
<li>这就是 Samuel 所说的<em>根据实际表现测试任何当前权重分配的有效性</em>。我们需要一个函数，如果模型的表现好，它将返回一个小的数字（标准方法是将小的损失视为好的，大的损失视为坏的，尽管这只是一种约定）。</li>
</ul>
</li>
<li><p>为什么我们不能总是使用高学习率？</p>
<ul>
<li>如果学习率太高，它也可能会“弹跳”而不是发散；图 4-4 显示了这样做需要许多步骤才能成功训练。</li>
</ul>
</li>
</ol>
<ol>
<li><p>什么是梯度？</p>
<ul>
<li>SGD中的梯度是损失函数对参数的敏感度，指导参数向损失降低的方向调整。小批量计算兼顾了效率与稳定性，是深度学习的核心驱动力。</li>
</ul>
</li>
<li><p>你需要知道如何自己计算梯度吗？</p>
<ul>
<li>不需要</li>
</ul>
</li>
<li><p>为什么我们不能将准确率作为损失函数使用？</p>
<ul>
<li>关键区别在于指标用于驱动人类理解，而损失用于驱动自动学习。为了驱动自动学习，损失必须是一个具有有意义导数的函数。它不能有大的平坦部分和大的跳跃，而必须是相当平滑的。这就是为什么我们设计了一个损失函数，可以对置信水平的小变化做出响应。这个要求意味着有时它实际上并不完全反映我们试图实现的目标，而是我们真正目标和一个可以使用其梯度进行优化的函数之间的妥协。损失函数是针对数据集中的每个项目计算的，然后在时代结束时，所有损失值都被平均，整体均值被报告为时代。</li>
</ul>
</li>
<li><p>绘制 Sigmoid 函数。它的形状有什么特别之处？</p>
<ul>
<li>正如您所看到的，它接受任何输入值，正数或负数，并将其压缩为 0 和 1 之间的输出值。它还是一个只上升的平滑曲线，这使得 SGD 更容易找到有意义的梯度。</li>
</ul>
<p><img src="/image/dlcf_04in15.png" alt=""></p>
</li>
</ol>
<ol>
<li><p>损失函数和度量之间有什么区别？</p>
<ul>
<li>损失函数：是模型训练的“指南针”，需可导且适合优化。<br>度量：是模型性能的“成绩单”，反映实际任务需求。<br>核心原则：损失函数服务于训练过程，度量服务于业务目标，两者需根据任务特性协同设计。</li>
</ul>
</li>
<li><p>使用学习率计算新权重的函数是什么？</p>
<p>参数-=学习率*梯度</p>
</li>
<li><p><code>DataLoader</code>类是做什么的？</p>
<ul>
<li>如果我们在训练过程中可以改变一些东西，我们会获得更好的泛化能力。我们可以改变的一个简单而有效的事情是将哪些数据项放入每个小批次。我们通常不是简单地按顺序枚举我们的数据集，而是在每个时代之前随机洗牌，然后创建小批次。PyTorch 和 fastai 提供了一个类，可以为您执行洗牌和小批次整理，称为<code>DataLoader</code>。</li>
</ul>
</li>
<li><p>编写伪代码，显示每个 epoch 中 SGD 所采取的基本步骤。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数 θ（例如权重矩阵、偏置向量）</span></span><br><span class="line">Initialize θ randomly</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="number">1</span> to num_epochs:</span><br><span class="line">    <span class="comment"># 将训练数据随机打乱（确保样本独立性）</span></span><br><span class="line">    Shuffle training data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将数据划分为多个小批量（mini-batch）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span> to (num_samples / batch_size - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 提取当前小批量数据</span></span><br><span class="line">        batch_X = X_train[i*batch_size : (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        batch_Y = Y_train[i*batch_size : (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向传播：计算当前参数下的预测值</span></span><br><span class="line">        predictions = forward_pass(θ, batch_X)  <span class="comment"># 例如 y_pred = θ^T X + b</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失函数值（如均方误差、交叉熵）</span></span><br><span class="line">        loss = compute_loss(predictions, batch_Y)  <span class="comment"># 例如 L = 1/m Σ(y_pred - y_true)^2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：计算损失对参数θ的梯度 ∇θ</span></span><br><span class="line">        gradients = compute_gradients(θ, batch_X, batch_Y)  <span class="comment"># ∇θ = dL/dθ</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数：沿梯度反方向调整θ</span></span><br><span class="line">        θ = θ - learning_rate * gradients</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># （可选）在每轮结束后计算验证集损失/度量</span></span><br><span class="line">    val_predictions = forward_pass(θ, X_val)</span><br><span class="line">    val_loss = compute_loss(val_predictions, Y_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>: Train Loss = <span class="subst">&#123;loss&#125;</span>, Val Loss = <span class="subst">&#123;val_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个函数，如果传递两个参数<code>[1,2,3,4]</code>和<code>&#39;abcd&#39;</code>，则返回<code>[(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]</code>。该输出数据结构有什么特别之处？</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pair_elements</span>(<span class="params">nums, chars</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(nums, chars))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="built_in">print</span>(pair_elements([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], <span class="string">&#x27;abcd&#x27;</span>))  <span class="comment"># 输出 [(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (3, &#x27;c&#x27;), (4, &#x27;d&#x27;)]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>PyTorch 中的<code>view</code>是做什么的？</p>
<ul>
<li>我们已经有了我们的<code>x</code>—也就是我们的自变量，图像本身。我们将它们全部连接成一个单一的张量，并且还将它们从矩阵列表（一个秩为 3 的张量）转换为向量列表（一个秩为 2 的张量）。我们可以使用<code>view</code>来做到这一点，<code>view</code>是一个 PyTorch 方法，可以改变张量的形状而不改变其内容。<code>-1</code>是<code>view</code>的一个特殊参数，意思是“使这个轴尽可能大以适应所有数据”</li>
</ul>
</li>
<li><p>神经网络中的偏差参数是什么？我们为什么需要它们？</p>
<ul>
<li><p>神经网络中的偏差参数（Bias）是每个神经元中的一个可学习参数，用于在加权和计算后添加一个常数偏移。</p>
</li>
<li><p>平移激活函数的输入：允许调整加权和的基线位置，使激活函数能适应不同数据分布。增强模型表达能力：没有偏差时，模型只能学习经过原点的超平面；加入偏差后，模型可以表示任意位置的超平面。</p>
</li>
</ul>
</li>
<li><p>Python 中的<code>@</code>运算符是做什么的？</p>
<ul>
<li>矩阵乘法用<code>@</code>运算符表示</li>
</ul>
</li>
<li><p><code>backward</code>方法是做什么的？</p>
<ul>
<li>这里的<code>backward</code>指的是<em>反向传播</em>，这是计算每一层导数的过程的名称。我们将在第十七章中看到这是如何精确完成的，当我们从头开始计算深度神经网络的梯度时。这被称为网络的<em>反向传播</em>，与<em>前向传播</em>相对，前者是计算激活的地方。如果<code>backward</code>只是被称为<code>calculate_grad</code>，生活可能会更容易，但深度学习的人确实喜欢在任何地方添加行话！</li>
</ul>
</li>
<li><p>为什么我们必须将梯度清零？</p>
<ul>
<li>必须将梯度清零是为了防止不同批次（batch）的梯度在反向传播时累积，导致参数更新方向错误，确保每个批次的梯度独立计算并正确更新模型参数。</li>
</ul>
</li>
<li><p>我们需要向<code>Learner</code>传递什么信息？</p>
<ul>
<li>我们需要传入本章中创建的所有元素：<code>DataLoaders</code>，模型，优化函数（将传递参数），损失函数，以及可选的任何要打印的指标：</li>
</ul>
</li>
<li><p>展示训练循环的基本步骤的 Python 或伪代码。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 初始化模型、损失函数、优化器</span></span><br><span class="line">model = 初始化神经网络()</span><br><span class="line">loss_function = 选择损失函数()  <span class="comment"># 如交叉熵、均方误差</span></span><br><span class="line">optimizer = 选择优化器(model.parameters(), 学习率)  <span class="comment"># 如SGD、Adam</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据预处理</span></span><br><span class="line">dataset = 加载数据集()</span><br><span class="line">dataloader = 分批次(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练循环（按epoch迭代）</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="number">1</span>到最大训练轮次:</span><br><span class="line">    model.训练模式()  <span class="comment"># 启用Dropout/BatchNorm等训练特定层</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历所有小批量（mini-batch）</span></span><br><span class="line">    <span class="keyword">for</span> 每个batch的输入数据x, 标签y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="comment"># 3.1 梯度清零（关键！防止梯度累积）</span></span><br><span class="line">        optimizer.清空梯度()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.2 前向传播：计算预测值</span></span><br><span class="line">        预测值 = model.前向计算(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.3 计算损失（预测值与真实值差距）</span></span><br><span class="line">        loss = loss_function(预测值, y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.4 反向传播：计算梯度</span></span><br><span class="line">        loss.反向传播()  <span class="comment"># 自动计算各参数梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.5 参数更新：沿梯度反方向调整参数</span></span><br><span class="line">        optimizer.更新参数()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># （可选）4. 验证/测试循环</span></span><br><span class="line">    model.评估模式()  <span class="comment"># 禁用Dropout/BatchNorm等训练特定层</span></span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> 每个验证batch的输入x_val, 标签y_val <span class="keyword">in</span> 验证集:</span><br><span class="line">        预测_val = model.前向计算(x_val)</span><br><span class="line">        total_loss += loss_function(预测_val, y_val)</span><br><span class="line">    平均验证损失 = total_loss / 验证batch数量</span><br><span class="line">    打印(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, 验证损失: <span class="subst">&#123;平均验证损失&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>ReLU 是什么？为值从<code>-2</code>到<code>+2</code>绘制一个图。</p>
<ul>
<li>那个小函数<code>res.max(tensor(0.0))</code>被称为<em>修正线性单元</em>，也被称为<em>ReLU</em>。我们认为我们都可以同意<em>修正线性单元</em>听起来相当花哨和复杂…但实际上，它不过是<code>res.max(tensor(0.0))</code>——换句话说，用零替换每个负数。这个微小的函数在 PyTorch 中也可以作为<code>F.relu</code>使用：</li>
</ul>
</li>
</ol>
<p><img src="/image/dlcf_04in16.png" alt=""></p>
<ol>
<li><p>什么是激活函数？</p>
<ul>
<li>神经网络包含多个层。每一层都是<em>线性</em>或<em>非线性</em>的。我们通常在神经网络中交替使用这两种类型的层。有时人们将线性层及其后续的非线性一起称为一个单独的层。是的，这很令人困惑。有时非线性被称为<em>激活函数</em>。</li>
</ul>
</li>
<li><p><code>F.relu</code>和<code>nn.ReLU</code>之间有什么区别？</p>
<ul>
<li>nn.ReLU：是 模块化的层，适合定义静态模型结构，参数在初始化时固定。<br>F.relu：是 函数式接口，适合动态或条件性激活场景，参数在调用时指定。<br>当需要将激活函数作为模型的一部分（如保存/加载模型）时，优先用 nn.ReLU。<br>当需要灵活控制激活逻辑时，用 F.relu。</li>
</ul>
</li>
<li><p>通用逼近定理表明，任何函数都可以使用一个非线性逼近得到所需的精度。那么为什么我们通常使用更多的非线性函数？</p>
<ul>
<li>在 1990 年代，研究人员如此专注于通用逼近定理，以至于很少有人尝试超过一个非线性。这种理论但不实际的基础阻碍了该领域多年。然而，一些研究人员确实尝试了深度模型，并最终能够证明这些模型在实践中表现得更好。最终，出现了理论结果，解释了为什么会发生这种情况。今天，几乎不可能找到任何人只使用一个非线性的神经网络。</li>
</ul>
</li>
</ol>
<h2 id="进一步研究"><a href="#进一步研究" class="headerlink" title="进一步研究"></a>进一步研究</h2><ol>
<li><p>从头开始创建自己的<code>Learner</code>实现，基于本章展示的训练循环。</p>
</li>
<li><p>使用完整的 MNIST 数据集完成本章的所有步骤（不仅仅是 3 和 7）。这是一个重要的项目，需要花费相当多的时间来完成！您需要进行一些研究，以找出如何克服在途中遇到的障碍。</p>
</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/04/28/fastaichapter3/" rel="prev" title="Fastai Chapter 3">
      <i class="fa fa-chevron-left"></i> Fastai Chapter 3
    </a></div>
      <div class="post-nav-item">
    <a href="/2025/05/12/AboutConda/" rel="next" title="AboutConda">
      AboutConda <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0%EF%BC%9A%E5%BA%95%E5%B1%82%EF%BC%9A%E8%AE%AD%E7%BB%83%E6%95%B0%E5%AD%97%E5%88%86%E7%B1%BB%E5%99%A8"><span class="nav-number">1.</span> <span class="nav-text">第四章：底层：训练数字分类器</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%83%8F%E7%B4%A0%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89%E7%9A%84%E5%9F%BA%E7%A1%80"><span class="nav-number">2.</span> <span class="nav-text">像素：计算机视觉的基础</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%9C%E4%B8%8B%E6%9D%A5%E6%80%9D%E8%80%83%EF%BC%81"><span class="nav-number">3.</span> <span class="nav-text">停下来思考！</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%B0%9D%E8%AF%95%EF%BC%9A%E5%83%8F%E7%B4%A0%E7%9B%B8%E4%BC%BC%E5%BA%A6"><span class="nav-number">4.</span> <span class="nav-text">第一次尝试：像素相似度</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AF%E8%AF%AD%EF%BC%9A%E5%9F%BA%E7%BA%BF"><span class="nav-number">5.</span> <span class="nav-text">术语：基线</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%97%E8%A1%A8%E6%8E%A8%E5%AF%BC"><span class="nav-number">6.</span> <span class="nav-text">列表推导</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%88%91%E6%9D%A5%E8%A1%A5%E5%85%85%E4%B8%80%E4%B8%8B"><span class="nav-number">6.0.0.0.1.</span> <span class="nav-text">我来补充一下:</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#Python-%E5%AD%97%E5%85%B8-Dictionary"><span class="nav-number">6.0.0.0.1.1.</span> <span class="nav-text">Python 字典(Dictionary)</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#Python-%E5%88%97%E8%A1%A8-List"><span class="nav-number">6.0.0.0.1.2.</span> <span class="nav-text">Python 列表(List)</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E%E7%BB%B4%E5%BA%A6"><span class="nav-number">7.</span> <span class="nav-text">关于维度</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%81%9C%E4%B8%8B%E6%9D%A5%E6%80%9D%E8%80%83%E4%B8%80%E4%B8%8B%EF%BC%81"><span class="nav-number">8.</span> <span class="nav-text">停下来思考一下！</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9C%A8pytorch%E4%B8%AD-%E5%8F%96%E5%B7%AE%E5%80%BC%E7%9A%84%E7%BB%9D%E5%AF%B9%E5%80%BC%E7%9A%84%E5%B9%B3%E5%9D%87%E5%80%BC%EF%BC%88%E7%BB%9D%E5%AF%B9%E5%80%BC%E6%98%AF%E5%B0%86%E8%B4%9F%E5%80%BC%E6%9B%BF%E6%8D%A2%E4%B8%BA%E6%AD%A3%E5%80%BC%E7%9A%84%E5%87%BD%E6%95%B0%EF%BC%89%E3%80%82%E8%BF%99%E8%A2%AB%E7%A7%B0%E4%B8%BA%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E5%B7%AE%E6%88%96L1-%E8%8C%83%E6%95%B0%E3%80%82"><span class="nav-number">9.</span> <span class="nav-text">在pytorch中 取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1 范数。</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#L1-%E8%8C%83%E6%95%B0%E5%92%8C-%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%EF%BC%88MSE%EF%BC%89%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-number">10.</span> <span class="nav-text">L1 范数和 均方误差（MSE）之间的区别</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9D%B0%E9%87%8C%E7%B1%B3%E8%AF%B4"><span class="nav-number">11.</span> <span class="nav-text">杰里米说</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E6%9C%89%E6%95%88%E5%9C%B0%E4%BD%BF%E7%94%A8%E6%95%B0%E7%BB%84-%E5%BC%A0%E9%87%8F-API-%E6%98%AF%E6%9C%80%E9%87%8D%E8%A6%81%E7%9A%84%E6%96%B0%E7%BC%96%E7%A0%81%E6%8A%80%E8%83%BD%E3%80%82"><span class="nav-number">12.</span> <span class="nav-text">如何有效地使用数组&#x2F;张量 API 是最重要的新编码技能。</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%B9%BF%E6%92%AD%E8%AE%A1%E7%AE%97%E5%BA%A6%E9%87%8F"><span class="nav-number">13.</span> <span class="nav-text">使用广播计算度量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">14.</span> <span class="nav-text">随机梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9B%BE-4-1-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B"><span class="nav-number">14.0.0.0.0.1.</span> <span class="nav-text">图 4-1. 梯度下降过程</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="nav-number">14.1.</span> <span class="nav-text">计算梯度</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Alexis-%E8%AF%B4"><span class="nav-number">15.</span> <span class="nav-text">Alexis 说</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%9B%E8%A1%8C%E6%AD%A5%E8%BF%9B"><span class="nav-number">15.1.</span> <span class="nav-text">使用学习率进行步进</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9B%BE-4-2%E3%80%82%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%87%E4%BD%8E%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">15.1.0.0.0.1.</span> <span class="nav-text">图 4-2。学习率过低的梯度下降</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9B%BE-4-3-%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%BF%87%E9%AB%98%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">15.1.0.0.0.2.</span> <span class="nav-text">图 4-3. 学习率过高的梯度下降</span></a></li><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9B%BE-4-4-%E5%B8%A6%E6%9C%89%E5%BC%B9%E8%B7%B3%E5%AD%A6%E4%B9%A0%E7%8E%87%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">15.1.0.0.0.3.</span> <span class="nav-text">图 4-4. 带有弹跳学习率的梯度下降</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E4%B8%AA%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84-SGD-%E7%A4%BA%E4%BE%8B"><span class="nav-number">15.2.</span> <span class="nav-text">一个端到端的 SGD 示例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0"><span class="nav-number">15.2.1.</span> <span class="nav-text">第一步：初始化参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B"><span class="nav-number">15.2.2.</span> <span class="nav-text">第二步：计算预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E6%AD%A5%EF%BC%9A%E5%88%9D%E5%A7%8B%E5%8C%96%E5%8F%82%E6%95%B0-1"><span class="nav-number">15.2.3.</span> <span class="nav-text">第一步：初始化参数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B-1"><span class="nav-number">15.2.4.</span> <span class="nav-text">第二步：计算预测</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%8D%9F%E5%A4%B1"><span class="nav-number">15.2.5.</span> <span class="nav-text">第三步：计算损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E6%AD%A5%EF%BC%9A%E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6"><span class="nav-number">15.2.6.</span> <span class="nav-text">第四步：计算梯度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC-5-%E6%AD%A5%EF%BC%9A%E8%B0%83%E6%95%B4%E6%9D%83%E9%87%8D"><span class="nav-number">15.2.7.</span> <span class="nav-text">第 5 步：调整权重</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Alexis-%E8%AF%B4-1"><span class="nav-number">16.</span> <span class="nav-text">Alexis 说</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC-6-%E6%AD%A5%EF%BC%9A%E9%87%8D%E5%A4%8D%E8%BF%99%E4%B8%AA%E8%BF%87%E7%A8%8B"><span class="nav-number">16.0.1.</span> <span class="nav-text">第 6 步：重复这个过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AC%AC-7-%E6%AD%A5%EF%BC%9A%E5%81%9C%E6%AD%A2"><span class="nav-number">16.0.2.</span> <span class="nav-text">第 7 步：停止</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">16.1.</span> <span class="nav-text">总结梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9B%BE-4-5-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B"><span class="nav-number">16.1.0.0.0.1.</span> <span class="nav-text">图 4-5. 梯度下降过程</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MNIST-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">17.</span> <span class="nav-text">MNIST 损失函数</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AF%E8%AF%AD%EF%BC%9A%E5%8F%82%E6%95%B0"><span class="nav-number">18.</span> <span class="nav-text">术语：参数</span></a><ol class="nav-child"><li class="nav-item nav-level-6"><a class="nav-link" href="#%E5%9B%BE-4-6-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95"><span class="nav-number">18.0.0.0.0.1.</span> <span class="nav-text">图 4-6. 矩阵乘法</span></a></li></ol></li></ol></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Sylvain-%E8%AF%B4"><span class="nav-number">19.</span> <span class="nav-text">Sylvain 说</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E6%96%87%E6%A1%A3"><span class="nav-number">20.</span> <span class="nav-text">阅读文档</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Sigmoid"><span class="nav-number">20.1.</span> <span class="nav-text">Sigmoid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SGD-%E5%92%8C%E5%B0%8F%E6%89%B9%E6%AC%A1"><span class="nav-number">20.2.</span> <span class="nav-text">SGD 和小批次</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%8A%8A%E6%89%80%E6%9C%89%E4%B8%9C%E8%A5%BF%E6%94%BE%E5%9C%A8%E4%B8%80%E8%B5%B7"><span class="nav-number">21.</span> <span class="nav-text">把所有东西放在一起</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8E%9F%E5%9C%B0%E6%93%8D%E4%BD%9C"><span class="nav-number">22.</span> <span class="nav-text">原地操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E4%BC%98%E5%8C%96%E5%99%A8"><span class="nav-number">22.1.</span> <span class="nav-text">创建一个优化器</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0%E9%9D%9E%E7%BA%BF%E6%80%A7"><span class="nav-number">23.</span> <span class="nav-text">添加非线性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Jeremy-%E8%AF%B4"><span class="nav-number">24.</span> <span class="nav-text">Jeremy 说</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Sylvain-%E8%AF%B4-1"><span class="nav-number">25.</span> <span class="nav-text">Sylvain 说</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9B%B4%E6%B7%B1%E5%85%A5%E5%9C%B0%E6%8E%A2%E8%AE%A8"><span class="nav-number">25.1.</span> <span class="nav-text">更深入地探讨</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%9C%AF%E8%AF%AD%E5%9B%9E%E9%A1%BE"><span class="nav-number">26.</span> <span class="nav-text">术语回顾</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%80%89%E6%8B%A9%E4%BD%A0%E7%9A%84%E5%86%92%E9%99%A9-%E6%8F%90%E9%86%92"><span class="nav-number">27.</span> <span class="nav-text">选择你的冒险 提醒</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BE%E5%90%8E%E4%B9%A0%E9%A2%98"><span class="nav-number">28.</span> <span class="nav-text">课后习题</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BF%9B%E4%B8%80%E6%AD%A5%E7%A0%94%E7%A9%B6"><span class="nav-number">28.1.</span> <span class="nav-text">进一步研究</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Wang Song"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Wang Song</p>
  <div class="site-description" itemprop="description">a graduate student working at Huzhou institute of Zhejiang University</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/SongSop" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SongSop" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3042903197@qq.com" title="E-Mail → mailto:3042903197@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Song</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
