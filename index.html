<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.json"};
  </script>

  <meta name="description" content="a graduate student working at Huzhou institute of Zhejiang University">
<meta property="og:type" content="website">
<meta property="og:title" content="WangSong&#39;s blog">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="WangSong&#39;s blog">
<meta property="og:description" content="a graduate student working at Huzhou institute of Zhejiang University">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Wang Song">
<meta property="article:tag" content="Python, C++, robot, ros , opencv, target detection">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>WangSong's blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="WangSong's blog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">WangSong's blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/SongSop" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/30/%E4%BA%86%E8%A7%A3%E4%BD%A0%E7%9A%84vscode/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/30/%E4%BA%86%E8%A7%A3%E4%BD%A0%E7%9A%84vscode/" class="post-title-link" itemprop="url">了解你的vscode</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-05-30 14:50:04 / 修改时间：16:32:12" itemprop="dateCreated datePublished" datetime="2025-05-30T14:50:04+08:00">2025-05-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>对于利用vscode进行代码开发和调试我需要补充一些新的知识</p>
<p>了解 <code>&#123;&#125; launch.json</code></p>
<h3 id="launch-json是什么？"><a href="#launch-json是什么？" class="headerlink" title="launch.json是什么？"></a>launch.json是什么？</h3><p>在 Visual Studio Code (VS Code) 中，launch.json 是一个用于配置调试会话的重要文件。它定义了如何启动和配置调试器，以及代码在调试过程中的行为。无论你是在本地开发环境还是远程服务器上进行调试，launch.json 都是一个关键的配置文件，能够帮助你更加高效地进行代码调试。</p>
<h3 id="我没有这个文件应该如何获得？"><a href="#我没有这个文件应该如何获得？" class="headerlink" title="我没有这个文件应该如何获得？"></a>我没有这个文件应该如何获得？</h3><p>创建launch.json文件后可以打开文件的编辑界面在右下角找到<code>添加配置</code>选择你想要添加的配置你就会获得一个基础的launch.json(其他两个文件同理)</p>
<h3 id="其中的内容有什么含义？"><a href="#其中的内容有什么含义？" class="headerlink" title="其中的内容有什么含义？"></a>其中的内容有什么含义？</h3><p>其中生成的launch.json如下，我们挑几个经常用的先讲一下，其他的以后遇到再补充</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="comment">// 使用 IntelliSense 了解相关属性。 </span></span><br><span class="line">    <span class="comment">// 悬停以查看现有属性的描述。</span></span><br><span class="line">    <span class="comment">// 欲了解更多信息，请访问: https://go.microsoft.com/fwlink/?linkid=830387</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.2.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;(gdb) 启动&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppdbg&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;request&quot;</span><span class="punctuation">:</span> <span class="string">&quot;launch&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;stopAtEntry&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;fileDirname&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;externalConsole&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;MIMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;setupCommands&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;description&quot;</span><span class="punctuation">:</span> <span class="string">&quot;为 gdb 启用整齐打印&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;-enable-pretty-printing&quot;</span><span class="punctuation">,</span></span><br><span class="line">                    <span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;preLaunchTask&quot;</span><span class="punctuation">:</span> <span class="string">&quot;C/C++: g++ 生成活动文件&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;miDebuggerPath&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/usr/bin/gdb&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<p>其中每个key的功能如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>键名 (Key)</th>
<th>类型</th>
<th>功能描述</th>
<th>示例值/选项</th>
<th>重要性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>name</code></strong></td>
<td>string</td>
<td>配置项在调试器下拉菜单中的显示名称</td>
<td><code>&quot;(gdb) 启动&quot;</code></td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>type</code></strong></td>
<td>string</td>
<td>指定调试器类型</td>
<td><code>&quot;cppdbg&quot;</code> (C++调试)</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>request</code></strong></td>
<td>string</td>
<td>调试启动类型</td>
<td><code>&quot;launch&quot;</code> (启动程序) 或 <code>&quot;attach&quot;</code> (附加进程)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>program</code></strong></td>
<td>string</td>
<td><strong>被调试程序路径</strong></td>
<td><code>$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;</code></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>args</code></strong></td>
<td>array</td>
<td>传递给程序的命令行参数</td>
<td><code>[&quot;--param=value&quot;, &quot;input.txt&quot;]</code></td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>stopAtEntry</code></strong></td>
<td>boolean</td>
<td><strong>是否在 main 函数入口暂停</strong></td>
<td><code>true</code> (暂停)/<code>false</code> (不暂停)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>cwd</code></strong></td>
<td>string</td>
<td><strong>程序工作目录</strong></td>
<td><code>$&#123;fileDirname&#125;</code> (当前文件目录)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>environment</code></strong></td>
<td>array</td>
<td>设置环境变量</td>
<td><code>[&#123;&quot;name&quot;: &quot;PATH&quot;, &quot;value&quot;: &quot;/custom/bin&quot;&#125;]</code></td>
<td>⭐⭐</td>
</tr>
<tr>
<td><strong><code>externalConsole</code></strong></td>
<td>boolean</td>
<td><strong>是否使用外部终端</strong></td>
<td><code>true</code> (外部)/<code>false</code> (VSC内置终端)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>MIMode</code></strong></td>
<td>string</td>
<td><strong>指定调试引擎</strong></td>
<td><code>&quot;gdb&quot;</code> (GNU调试器)</td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>setupCommands</code></strong></td>
<td>array</td>
<td><strong>gdb 初始化命令</strong></td>
<td><code>-enable-pretty-printing</code> (结构化输出)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>preLaunchTask</code></strong></td>
<td>string</td>
<td><strong>调试前执行的任务</strong></td>
<td><code>&quot;C/C++: g++ 生成活动文件&quot;</code> (编译任务)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>miDebuggerPath</code></strong></td>
<td>string</td>
<td><strong>gdb 调试器路径</strong></td>
<td><code>&quot;/usr/bin/gdb&quot;</code></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
</tbody>
</table>
</div>
<h3 id="关键路径变量说明："><a href="#关键路径变量说明：" class="headerlink" title="关键路径变量说明："></a>关键路径变量说明：</h3><ul>
<li><code>$&#123;fileDirname&#125;</code>：当前打开文件所在目录</li>
<li><code>$&#123;fileBasenameNoExtension&#125;</code>：当前文件名（不含扩展名）</li>
<li><code>$&#123;workspaceFolder&#125;</code>：VSCode 打开的工作区根目录</li>
</ul>
<h3 id="使用技巧："><a href="#使用技巧：" class="headerlink" title="使用技巧："></a>使用技巧：</h3><ol>
<li><p><strong>单文件调试</strong>：<br><code>program</code> 使用 <code>$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;</code> 可直接调试当前打开的文件</p>
</li>
<li><p><strong>ROS节点调试</strong>：  </p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;program&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;workspaceFolder&#125;/devel/lib/pkg_name/node_name&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;_param:=value&quot;</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure></li>
<li><strong>添加条件断点</strong> <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;text&quot;</span><span class="punctuation">:</span> <span class="string">&quot;break main if argc &gt; 1&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;ignoreFailures&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">false</span></span></span><br></pre></td></tr></table></figure></li>
<li><strong>添加ros环境</strong> <figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    <span class="attr">&quot;environment&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span></span><br><span class="line"><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;ROS_MASTER_URI&quot;</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">&quot;value&quot;</span><span class="punctuation">:</span> <span class="string">&quot;http://localhost:11311&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<h3 id="tasks-json是什么"><a href="#tasks-json是什么" class="headerlink" title="tasks.json是什么?"></a>tasks.json是什么?</h3></li>
</ol>
<p>使用不同的编程语言可能有不同的开发流程，比如 C/C++ 就需要编译（广义编译，包括了链接）、运行、测试、打包等等流程，而 Python 只需要运行即可，为了把各种语言的不同开发流程抽象成同一套流程，于是有了编码（Code）— 构建（build）— 运行/调试（run/debug）— 测试 （test） — 打包（package） 等等，其中每个环节都可以认为是一个 task，所以可以利用 tasks.json来手动完成那些使用 IDE 时被隐藏的开发流程细节</p>
<h3 id="其中的内容有什么含义？-1"><a href="#其中的内容有什么含义？-1" class="headerlink" title="其中的内容有什么含义？"></a>其中的内容有什么含义？</h3><p>其中代码如下：<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;tasks&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cppbuild&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;label&quot;</span><span class="punctuation">:</span> <span class="string">&quot;C/C++: g++ 生成活动文件&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;command&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/usr/bin/g++&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;-fdiagnostics-color=always&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;-g&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;$&#123;file&#125;&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;-o&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="string">&quot;$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;options&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;cwd&quot;</span><span class="punctuation">:</span> <span class="string">&quot;$&#123;fileDirname&#125;&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;problemMatcher&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;$gcc&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;group&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;kind&quot;</span><span class="punctuation">:</span> <span class="string">&quot;build&quot;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">&quot;isDefault&quot;</span><span class="punctuation">:</span> <span class="literal"><span class="keyword">true</span></span></span><br><span class="line">            <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;detail&quot;</span><span class="punctuation">:</span> <span class="string">&quot;调试器生成的任务。&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="string">&quot;2.0.0&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>键名 (Key)</th>
<th>类型</th>
<th>功能描述</th>
<th>示例值/选项</th>
<th>重要性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>type</code></strong></td>
<td>string</td>
<td>任务类型</td>
<td><code>&quot;cppbuild&quot;</code> (C++编译任务)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>label</code></strong></td>
<td>string</td>
<td><strong>任务标识符</strong></td>
<td><code>&quot;C/C++: g++ 生成活动文件&quot;</code></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>command</code></strong></td>
<td>string</td>
<td><strong>编译器路径</strong></td>
<td><code>&quot;/usr/bin/g++&quot;</code></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>args</code></strong></td>
<td>array</td>
<td><strong>编译参数</strong></td>
<td><code>[&quot;-fdiagnostics-color=always&quot;, &quot;-g&quot;, ...]</code></td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>options.cwd</code></strong></td>
<td>string</td>
<td>编译时的工作目录</td>
<td><code>&quot;$&#123;fileDirname&#125;&quot;</code> (当前文件目录)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>problemMatcher</code></strong></td>
<td>array</td>
<td>错误匹配器</td>
<td><code>[&quot;$gcc&quot;]</code> (GCC错误格式)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>group.kind</code></strong></td>
<td>string</td>
<td>任务分组类型</td>
<td><code>&quot;build&quot;</code> (构建任务)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>group.isDefault</code></strong></td>
<td>boolean</td>
<td><strong>是否为默认构建任务</strong></td>
<td><code>true</code> (是默认任务)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>detail</code></strong></td>
<td>string</td>
<td>任务描述信息</td>
<td><code>&quot;调试器生成的任务。&quot;</code></td>
<td>⭐</td>
</tr>
</tbody>
</table>
</div>
<h4 id="编译参数-args-详解："><a href="#编译参数-args-详解：" class="headerlink" title="编译参数 (args) 详解："></a>编译参数 (<code>args</code>) 详解：</h4><div class="table-container">
<table>
<thead>
<tr>
<th>参数</th>
<th>功能说明</th>
<th>重要性</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-fdiagnostics-color=always</code></td>
<td>启用彩色错误输出</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><code>-g</code></td>
<td><strong>生成调试信息</strong></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><code>$&#123;file&#125;</code></td>
<td><strong>当前活动文件</strong></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><code>-o</code></td>
<td>指定输出文件名</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><code>$&#123;fileDirname&#125;/$&#123;fileBasenameNoExtension&#125;</code></td>
<td><strong>输出文件路径</strong></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
</tbody>
</table>
</div>
<h4 id="常用扩展参数："><a href="#常用扩展参数：" class="headerlink" title="常用扩展参数："></a>常用扩展参数：</h4><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;args&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">  <span class="string">&quot;-Wall&quot;</span><span class="punctuation">,</span>               <span class="comment">// 开启所有警告</span></span><br><span class="line">  <span class="string">&quot;-Wextra&quot;</span><span class="punctuation">,</span>             <span class="comment">// 启用额外警告</span></span><br><span class="line">  <span class="string">&quot;-O0&quot;</span><span class="punctuation">,</span>                 <span class="comment">// 禁用优化（调试时推荐）</span></span><br><span class="line">  <span class="string">&quot;-I$&#123;workspaceFolder&#125;/include&quot;</span><span class="punctuation">,</span> <span class="comment">// 添加头文件路径</span></span><br><span class="line">  <span class="string">&quot;-L$&#123;workspaceFolder&#125;/lib&quot;</span><span class="punctuation">,</span>    <span class="comment">// 添加库文件路径</span></span><br><span class="line">  <span class="string">&quot;-lmy_library&quot;</span>         <span class="comment">// 链接指定库</span></span><br><span class="line"><span class="punctuation">]</span></span><br></pre></td></tr></table></figure>
<h3 id="c-cpp-properties-json的功能"><a href="#c-cpp-properties-json的功能" class="headerlink" title="c_cpp_properties.json的功能"></a>c_cpp_properties.json的功能</h3><p>代码如下：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;configurations&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Linux&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;includePath&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                <span class="string">&quot;$&#123;workspaceFolder&#125;/**&quot;</span></span><br><span class="line">            <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;defines&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;compilerPath&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/usr/bin/gcc&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cStandard&quot;</span><span class="punctuation">:</span> <span class="string">&quot;c17&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;cppStandard&quot;</span><span class="punctuation">:</span> <span class="string">&quot;gnu++14&quot;</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;intelliSenseMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;linux-gcc-x64&quot;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;version&quot;</span><span class="punctuation">:</span> <span class="number">4</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>键名 (Key)</th>
<th>类型</th>
<th>功能描述</th>
<th>示例值/选项</th>
<th>重要性</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong><code>name</code></strong></td>
<td>string</td>
<td>配置名称</td>
<td><code>&quot;Linux&quot;</code> (平台标识)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>includePath</code></strong></td>
<td>array</td>
<td><strong>头文件搜索路径</strong></td>
<td><code>[&quot;$&#123;workspaceFolder&#125;/**&quot;]</code></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>defines</code></strong></td>
<td>array</td>
<td>预处理器宏定义</td>
<td><code>[&quot;DEBUG&quot;, &quot;VERSION=1.0&quot;]</code></td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>compilerPath</code></strong></td>
<td>string</td>
<td><strong>编译器路径</strong></td>
<td><code>&quot;/usr/bin/gcc&quot;</code></td>
<td>⭐⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>cStandard</code></strong></td>
<td>string</td>
<td>C语言标准版本</td>
<td><code>&quot;c17&quot;</code> (C17标准)</td>
<td>⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>cppStandard</code></strong></td>
<td>string</td>
<td><strong>C++语言标准版本</strong></td>
<td><code>&quot;gnu++14&quot;</code> (GNU C++14标准)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><strong><code>intelliSenseMode</code></strong></td>
<td>string</td>
<td><strong>IntelliSense 引擎模式</strong></td>
<td><code>&quot;linux-gcc-x64&quot;</code> (Linux+GCC+x64)</td>
<td>⭐⭐⭐⭐</td>
</tr>
<tr>
<td><code>version</code></td>
<td>number</td>
<td>配置文件版本</td>
<td><code>4</code></td>
<td>⭐</td>
</tr>
</tbody>
</table>
</div>
<h4 id="关键配置说明："><a href="#关键配置说明：" class="headerlink" title="关键配置说明："></a>关键配置说明：</h4><ol>
<li><p><strong>includePath</strong>：</p>
<ul>
<li><code>$&#123;workspaceFolder&#125;/**</code>：包含工作区所有子目录</li>
<li>添加ROS路径：<code>&quot;/opt/ros/noetic/include/**&quot;</code></li>
</ul>
</li>
<li><p><strong>cppStandard</strong> 选项：</p>
<ul>
<li><code>c++11</code>, <code>c++14</code>, <code>c++17</code>, <code>c++20</code></li>
<li>GNU扩展版本：<code>gnu++14</code></li>
</ul>
</li>
<li><p><strong>intelliSenseMode</strong> 常用值：</p>
<ul>
<li><code>linux-gcc-x64</code></li>
<li><code>windows-msvc-x64</code></li>
<li><code>macos-clang-x64</code></li>
</ul>
</li>
</ol>
<h1 id="开始基础调试"><a href="#开始基础调试" class="headerlink" title="开始基础调试"></a>开始基础调试</h1><h3 id="执行编译"><a href="#执行编译" class="headerlink" title="执行编译"></a>执行编译</h3><ul>
<li>group中的isDefault: 值为true表示支持通过快捷键ctrl+shift+B来执行该编译任务。如果值改为false，也可以从菜单中选择运行：Terminal(终端)&gt;Run Build Task（进行生成任务）。</li>
</ul>
<p>生成结果如下，表示g++编译成功：<br><img src="/image/build.png" alt="build"></p>
<p>按+号创建一个新的在当前目录下的终端</p>
<p><img src="/image/newshell.png" alt="newshell"></p>
<p>同时可以输入ls查看发现多了一个相关的可执行文件,输入./filename你就可以运行这个文件了</p>
<p><img src="/image/runfile.png" alt="runfile"></p>
<h3 id="执行调试"><a href="#执行调试" class="headerlink" title="执行调试"></a>执行调试</h3><h4 id="设置一下stopAtEntry"><a href="#设置一下stopAtEntry" class="headerlink" title="设置一下stopAtEntry"></a>设置一下stopAtEntry</h4><p>stopAtEntry: 默认情况下，C++拓展不会向源代码添加任何断点，stopAtEntry 值设置为 false。 将stopAtEntry值更改为 true 将使调试器在开始调试时停止在 main 方法上。</p>
<h4 id="开始一个调试会话"><a href="#开始一个调试会话" class="headerlink" title="开始一个调试会话"></a>开始一个调试会话</h4><p>在调试的程序页面按f5或者Run(运行) &gt; Start Debugging（启动调试）,用户界面的几个变化： 集成终端出现在源代码编辑器的底部，在“Debug Output”选项卡中，会看到指示调试器已启动并正在运行的输出。编辑器突出显示 main 方法中的第一条语句。这是 C++拓展自动设置的断点。</p>
<p><img src="/image/debug.png" alt="debug"></p>
<p>使用箭头指出的工具进行调试：</p>
<ul>
<li><p>第一个是从断点开始继续进行程序</p>
</li>
<li><p>第二个是逐步进行程序</p>
</li>
<li><p>第三个是更细致的逐步进行</p>
</li>
<li><p>第四个是重新启动</p>
</li>
<li><p>第五个是停止</p>
</li>
</ul>
<p>逐行执行代码时在debug下可以看到变量的值，同时可以在监管里点击+添加你要监控的变量，将鼠标直接放在变量上也会显示他的值：</p>
<p><img src="/image/debugwatch.png" alt="debugwatch"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/30/c-%E5%8C%85%E7%9A%84%E7%AE%A1%E7%90%86%E5%92%8C%E5%AE%89%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/30/c-%E5%8C%85%E7%9A%84%E7%AE%A1%E7%90%86%E5%92%8C%E5%AE%89%E8%A3%85/" class="post-title-link" itemprop="url">c++包的管理和安装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-05-30 10:22:43 / 修改时间：10:55:39" itemprop="dateCreated datePublished" datetime="2025-05-30T10:22:43+08:00">2025-05-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>为了从python转到cpp编程必须学习一下cpp的包管理和安装，其并没有python的那麽方便。</p>
<h1 id="命令方式"><a href="#命令方式" class="headerlink" title="命令方式"></a>命令方式</h1><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install libxxx-dev</span><br><span class="line">sudo apt install xxx</span><br></pre></td></tr></table></figure>
<blockquote>
<p>安装位置会在/usr/lib/usr/lib</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install ros-yourrosversion-xxxx</span><br></pre></td></tr></table></figure>
<h1 id="源码方式"><a href="#源码方式" class="headerlink" title="源码方式"></a>源码方式</h1><blockquote>
<p>源码获取方式：github,git</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">cmake构建安装 源码最外层目录中有cmakelsit.txt</span></span><br><span class="line">1.git clone yourcodewebsite</span><br><span class="line">2.code 打开源码</span><br><span class="line">3.源代码管理-&gt;tag选择版本</span><br><span class="line">4.build mkdir</span><br><span class="line">5.cd build</span><br><span class="line">6.cmake ..</span><br><span class="line">7.make 或者 sudo make</span><br><span class="line">8.sudo make install</span><br><span class="line">·············································</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">一定需要在build文件夹中</span></span><br><span class="line">如果需要卸载就使用: sudo make uninstall </span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">configure构建安装</span></span><br><span class="line">1.git clone yourcodewebsite</span><br><span class="line">2.进入目录找到 configure文件 </span><br><span class="line">3.mkdir build</span><br><span class="line">4.cd build</span><br><span class="line">5.../configure --prefix=/usr/local #生成makefile,并指定库的安装位置</span><br><span class="line">6.make 或 sudo make</span><br><span class="line">7.sudo make install</span><br><span class="line">·············································</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">一定需要在build文件夹中</span></span><br><span class="line">如果需要卸载就使用: sudo make uninstall </span><br></pre></td></tr></table></figure>
<blockquote>
<p>pip源码安装用的很少</p>
</blockquote>
<h1 id="直接拷贝"><a href="#直接拷贝" class="headerlink" title="直接拷贝"></a>直接拷贝</h1><blockquote>
<p>.so .a 文件直接拷贝到/usr/local/lib下<br>对应的头文件放在/usr/local/include,方便直接引用</p>
</blockquote>
<h1 id="deb安装"><a href="#deb安装" class="headerlink" title="deb安装"></a>deb安装</h1><p>最有效的安装方式</p>
<blockquote>
<p>阅读第三方库的readme.md查看是否支持</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/30/%E5%85%B3%E4%BA%8Ejson%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/30/%E5%85%B3%E4%BA%8Ejson%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%80%E4%BA%9B%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">关于json文件的一些知识</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-05-30 08:54:37 / 修改时间：10:17:39" itemprop="dateCreated datePublished" datetime="2025-05-30T08:54:37+08:00">2025-05-30</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><h2 id="什么是-JSON-？"><a href="#什么是-JSON-？" class="headerlink" title="什么是 JSON ？"></a>什么是 JSON ？</h2><ol>
<li>JSON 指的是 JavaScript 对象表示法（JavaScript Object Notation）</li>
<li>JSON 是轻量级的文本数据交换格式</li>
<li>JSON 独立于语言：JSON 使用 Javascript语法来描述数据对象，但是 JSON 仍然独立于语言和平台。JSON 解析器和 JSON 库支持许多不同的编程语言。 目前非常多的动态（PHP，JSP，.NET）编程语言都支持 JSON</li>
<li>JSON 具有自我描述性，更易理解</li>
</ol>
<h2 id="这里有一些json的工具以后可能会用到："><a href="#这里有一些json的工具以后可能会用到：" class="headerlink" title="这里有一些json的工具以后可能会用到："></a>这里有一些json的工具以后可能会用到：</h2><p><a target="_blank" rel="noopener" href="https://www.jyshare.com/front-end/53/">JSON 格式化工具</a><br><a target="_blank" rel="noopener" href="https://www.jyshare.com/front-end/7683/">JSON 转义/去除转义</a><br><a target="_blank" rel="noopener" href="https://www.jyshare.com/front-end/7438/">JSON 在线解析工具</a><br><a target="_blank" rel="noopener" href="https://www.jyshare.com/front-end/9557/">JSON 差异对比工具</a></p>
<h1 id="来开始看看json的结构"><a href="#来开始看看json的结构" class="headerlink" title="来开始看看json的结构"></a>来开始看看json的结构</h1><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;wangsong&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;age&quot;</span><span class="punctuation">:</span><span class="string">&quot;23&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;hobbies&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;Swimmming&quot;</span><span class="punctuation">,</span><span class="string">&quot;Running&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;address&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;city&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huzhou&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;Inc&quot;</span><span class="punctuation">:</span><span class="string">&quot;Huzhou institute of Zhejiang University&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;coordinates&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;latitude&quot;</span><span class="punctuation">:</span><span class="number">40.7128</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">&quot;longitude&quot;</span><span class="punctuation">:</span><span class="number">-74.0060</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>在json中使用{}来定义一个对象,括号中的一切都代表存储数据的键值对</p>
</li>
<li><p>其中name是键(key),wangsong是值(value)</p>
</li>
<li><p>数组采用[]进行定义其中的元素用,分隔</p>
</li>
<li><p>每个键值对之间用逗号隔开直到最后一个</p>
</li>
<li><p>可以在json中进行对象的嵌套</p>
</li>
</ul>
<h1 id="让我们看看如何访问json文件"><a href="#让我们看看如何访问json文件" class="headerlink" title="让我们看看如何访问json文件"></a>让我们看看如何访问json文件</h1><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><p>使用json模块将json数据转换为字典进行使用</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">personal_data = json.loads(personal_data.json)</span><br><span class="line"><span class="comment"># accessing data</span></span><br><span class="line"><span class="built_in">print</span>(personal_data[<span class="string">&#x27;name&#x27;</span>]) <span class="comment"># &quot;wangsong&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="cpp"><a href="#cpp" class="headerlink" title="cpp"></a>cpp</h2><p>在cpp中想要使用json文件就需要配置<a target="_blank" rel="noopener" href="https://github.com/open-source-parsers/jsoncpp">jsoncppp</a>了</p>
<ol>
<li><p>拉取jsoncpp的源码</p>
</li>
<li><p>打开你下载的jsoncpp文件夹</p>
</li>
<li><p>双击运行amalgamate.py文件（需要有python环境）</p>
</li>
<li><p>操作后，会生成一个dist的文件夹里面就是我们需要的jsoncpp的源代码了，只有三个文件可以直接包含到项目中一起编译这个源文件还是跨平台的，在windows下和linux下都可以使用</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/29/fastaichapter8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/29/fastaichapter8/" class="post-title-link" itemprop="url">fastaichapter8</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-05-29 15:26:42" itemprop="dateCreated datePublished" datetime="2025-05-29T15:26:42+08:00">2025-05-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/29/fastaichapter7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/29/fastaichapter7/" class="post-title-link" itemprop="url">fastaichapter7</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-05-29 15:02:25 / 修改时间：15:11:53" itemprop="dateCreated datePublished" datetime="2025-05-29T15:02:25+08:00">2025-05-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第七章：训练一个最先进的模型"><a href="#第七章：训练一个最先进的模型" class="headerlink" title="第七章：训练一个最先进的模型"></a>第七章：训练一个最先进的模型</h1><p>本章介绍了更高级的技术，用于训练图像分类模型并获得最先进的结果。如果您想了解更多关于深度学习的其他应用，并稍后回来，您可以跳过它——后续章节不会假设您已掌握这些材料。</p>
<p>在此先做跳过，我对图像处理并非十分感兴趣。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/27/%E5%82%BB%E5%AD%90%E5%A6%82%E4%BD%95%E5%86%99%E4%BB%A3%E7%A0%81/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/27/%E5%82%BB%E5%AD%90%E5%A6%82%E4%BD%95%E5%86%99%E4%BB%A3%E7%A0%81/" class="post-title-link" itemprop="url">傻子如何写代码</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-05-27 15:30:07" itemprop="dateCreated datePublished" datetime="2025-05-27T15:30:07+08:00">2025-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-29 15:04:02" itemprop="dateModified" datetime="2025-05-29T15:04:02+08:00">2025-05-29</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="傻子的git教程"><a href="#傻子的git教程" class="headerlink" title="傻子的git教程"></a>傻子的git教程</h1><p>tip:我现在只会git clone 和git push 应该也就是和傻子差不多了哈哈哈哈！</p>
<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>安装很简单我就不多说了，就算安装过程中选错了选项也没关系后面都可以在配置文件中改<a target="_blank" rel="noopener" href="https://git-scm.com/book/zh/v2/%E8%B5%B7%E6%AD%A5-%E5%AE%89%E8%A3%85-Git">官方教程</a></p>
<p>验证安装：</p>
<ol>
<li>打开vscode即可验证</li>
<li>终端运行 git —version<h2 id="初次配置"><a href="#初次配置" class="headerlink" title="初次配置"></a>初次配置</h2></li>
</ol>
<p><a target="_blank" rel="noopener" href="https://git-scm.com/book/zh/v2/%e8%b5%b7%e6%ad%a5-%e5%88%9d%e6%ac%a1%e8%bf%90%e8%a1%8c-Git-%e5%89%8d%e7%9a%84%e9%85%8d%e7%bd%ae">官方教程</a></p>
<p>点开桌面上多出的神奇图标：</p>
<p><img src="/image/gitbash.png" alt="pic_git"></p>
<p>在他跳出的命令行进行对git的操作：</p>
<p><img src="/image/gitwin.png" alt="git_win"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git config --global user.name <span class="string">&quot;yourname&quot;</span></span><br><span class="line">$ git config --global user.email youremail</span><br></pre></td></tr></table></figure>
<h2 id="在本地创建一个git项目"><a href="#在本地创建一个git项目" class="headerlink" title="在本地创建一个git项目"></a>在本地创建一个git项目</h2><ol>
<li><p>新建一个文件夹</p>
</li>
<li><p>在文件夹中创建一个index.html</p>
</li>
</ol>
<p><img src="/image/index.png" alt="index"></p>
<ol>
<li>在vscode中打开文件夹此时会发现vscode的左边栏中的git图标点开后会发生变化。</li>
</ol>
<p><img src="/image/vsgit.png" alt="git_vs"></p>
<p>如果没有相关的库我们需要初始化资源也就是第一个选项，打开之后如下：</p>
<p><img src="/image/commit.png" alt="commit"></p>
<p>上传成功但是我并不是这种傻子我需要学习更细的git操作。</p>
<p>这些都是本地操作。</p>
<h2 id="下一步上传github"><a href="#下一步上传github" class="headerlink" title="下一步上传github"></a>下一步上传github</h2><p><img src="/image/gogithub.png" alt="GitHub"></p>
<p>登录github并授权但是我们会发现可能无法正常上传就算用了魔法上网也不行这时候就需要设置代理。</p>
<p>打开终端在终端中输入：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">git config --global http.proxy http://proxyuser:proxypwd@proxy.server.com:8080</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>一般来说可能会没用这个时候输入</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global --<span class="built_in">unset</span> http.proxy</span><br><span class="line">git config --global --<span class="built_in">unset</span> https.proxy</span><br></pre></td></tr></table></figure>
<p>取消代理</p>
<p>取消代理之后反而成功了现在出现了两个图标：<br><img src="/image/icongit.png" alt="icon"></p>
<p>蓝色的main代表我们当前项目所处的位置，粉色的云代表远程仓库当前的位置。</p>
<p><img src="/image/git02.png" alt="branch diff"></p>
<p>在本地目录下的命令行输入<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> </span><br></pre></td></tr></table></figure><br>可以查看存档所有提交的版本</p>
<h1 id="常见操作更正"><a href="#常见操作更正" class="headerlink" title="常见操作更正"></a>常见操作更正</h1><p>一般情况下我们并不会在本地创建仓库在发送到github，我们一般是已有github库后再本地进行操作。</p>
<ol>
<li><p>在github中新建仓库，写上你想要的名字，选择公开或私有</p>
</li>
<li><p>然后操作和之前一样但是在上传之前打开选项卡选择remote将本地的仓库连接到线上的仓库</p>
</li>
</ol>
<p><img src="/image/remote.png" alt="remote"></p>
<h1 id="和傻子一起写代码"><a href="#和傻子一起写代码" class="headerlink" title="和傻子一起写代码"></a>和傻子一起写代码</h1><h2 id="快使用-gitignore"><a href="#快使用-gitignore" class="headerlink" title="快使用.gitignore"></a>快使用.gitignore</h2><p>.gitignore的作用是将我们不需要上传的文件排除出去(我是呆瓜实习生的时候就将有几千个文件的devel和build上传了搞得项目一团糟)他非常的重要!</p>
<p>只要在.gitignore中稍加添加你就可以把不必要的东西排除在上传范围之中。</p>
<p>未在.gitignore中写入env和personal_data.yaml:<br><img src="/image/noignore.png" alt="noignore"><br>写入之后:<br><img src="/image/ignore.png" alt="ignore"></p>
<p>正常上传，如果你要链接新的远程仓库会提示给你需要提供remote name 这个东西是在你同一个本地程序连接到多个远程仓库时区分不同远程仓库用的。</p>
<h2 id="拉人"><a href="#拉人" class="headerlink" title="拉人"></a>拉人</h2><p>在设置的general的collaborators中添加需要协同的人就好了。</p>
<h2 id="拉取最新代码"><a href="#拉取最新代码" class="headerlink" title="拉取最新代码"></a>拉取最新代码</h2><p>需要注意</p>
<p><img src="/image/getcode.png" alt="getcode"></p>
<p>首先点击<em>从所有远程库中抓取</em>将远程库的代码抓取到本地</p>
<p>然后再点击<em>拉取</em>才能将本地的代码更新到远程端版本</p>
<p><img src="/image/pullcode.png" alt="pullcode"></p>
<h2 id="设置提交门槛不要让什么奇奇怪怪的东西都提交上来"><a href="#设置提交门槛不要让什么奇奇怪怪的东西都提交上来" class="headerlink" title="设置提交门槛不要让什么奇奇怪怪的东西都提交上来"></a>设置提交门槛不要让什么奇奇怪怪的东西都提交上来</h2><p>在github的仓库页面的rules中添加new ruleset</p>
<ol>
<li><p>添加规则名称</p>
</li>
<li><p>启用规则</p>
</li>
<li><p>添加限制的代码分支</p>
</li>
<li><p>一般来说打开<em>合并代码时需要提交请求</em></p>
</li>
<li><p>升级私有项目到团队组织项目才能生效（需要充钱升级）</p>
</li>
</ol>
<p>想要提交就必须</p>
<ol>
<li><p>创建自己的新分支，并提交到分支</p>
</li>
<li><p>在github上请求一个pr（pull requests）</p>
</li>
<li><p>等待审查</p>
</li>
</ol>
<p>（但是你作为仓库的创建人也会被限制）</p>
<h2 id="受够了nomachine和to-desk连接远程服务器快使用remote-ssh"><a href="#受够了nomachine和to-desk连接远程服务器快使用remote-ssh" class="headerlink" title="受够了nomachine和to desk连接远程服务器快使用remote ssh"></a>受够了nomachine和to desk连接远程服务器快使用remote ssh</h2><ol>
<li><p>安装remote-ssh扩展</p>
</li>
<li><p>查看你需要远程的电脑的ip地址然后记得输入正确的用户名和密码</p>
</li>
</ol>
<p>如果连接失败很可能是没有安装ssh服务在被控电脑上输入 <code>sudo apt-get install openssh-server</code>安装相关服务就好了。</p>
<h3 id="两种启动方式："><a href="#两种启动方式：" class="headerlink" title="两种启动方式："></a>两种启动方式：</h3><ul>
<li><p>你可以选择在cmd终端中直接输入 <code>ssh username@ipv4</code>后输入对应账户密码启动</p>
<p>  <img src="/image/sshcmd.png" alt="sshcmd"></p>
</li>
<li><p>你也可以在vscode中的ssh config保存你的相关远程电脑配置后直接点击进行连接（更方面但是因为某些原因更容易失败）</p>
<p>  <img src="/image/vscodessh.png" alt="vscodessh"></p>
</li>
</ul>
<h2 id="问题接踵而至虽然ssh很快很强但是他只有终端可以使用看来是时候告别图形化操作界面了"><a href="#问题接踵而至虽然ssh很快很强但是他只有终端可以使用看来是时候告别图形化操作界面了" class="headerlink" title="问题接踵而至虽然ssh很快很强但是他只有终端可以使用看来是时候告别图形化操作界面了"></a>问题接踵而至虽然ssh很快很强但是他只有终端可以使用看来是时候告别图形化操作界面了</h2><h3 id="学习一些ubuntu的操作指令"><a href="#学习一些ubuntu的操作指令" class="headerlink" title="学习一些ubuntu的操作指令"></a>学习一些ubuntu的操作指令</h3><p>当你看到黑乎乎没有任何内容的命令行时，不要脑袋空空，你可以在里面进行图形化界面能够进行的任何事情，让我们从简单的开始。</p>
<ol>
<li><p>现在当务之急是要知道你在哪里，快使用<code>pwd</code>获取你现在的位置:</p>
<p> <img src="/image/vscodepwd.png" alt="vscodepwd"></p>
</li>
<li><p>知道了你在哪里接下来我们就需要知道自己手头有什么，用<code>ls</code>查看吧,或者是用<code>ll</code>和<code>ls -l -a</code>可以达到查看所有文件包括隐藏文件的效果。<br> <img src="/image/vscodels.png" alt="vscodels"><br> <img src="/image/vscodell.png" alt="vscodell"></p>
</li>
<li><p>查看根目录<code>cd /</code>其中比较关键的一些是bin,etc,sys这些目录，保存了ubuntu系统的一些关键信息。<br> <img src="/image/rootm.png" alt="rootm"></p>
</li>
<li><p>创建目录<code>mkdir yourfilename</code>进行目录创建:<br> <img src="/image/mkdir.png" alt="mkdir"></p>
</li>
<li><p>我们可以在文件夹里创建我们自己需要的东西，例如像现在我将创建一个markdown文件,输入<code>vi yourfilename.md</code>去创建,然后会自动进行vi进行查看，按下i进入编辑模式在里面进行编辑，然后按下esc输入:wq（其中w代表写入,q代表保存）继成功编写，你可以利用<code>cat yourflie</code>在命令行中简单查看内容。<br>从最简单的开始让我们打开一个源码文件看一下，使用<code>vim youfilename.py/.cpp/</code>打开:<br> <img src="/image/vi.png" alt="vi"></p>
<p> 相同的操作我们来写个python和cpp吧<br> <img src="/image/vipy.png" alt="vipy"><br> <img src="/image/vicpp.png" alt="vicpp"></p>
</li>
<li><p>现在是重头戏，总所周知写代码是不可能写代码的，程序员最重要的技能就是copy和paste。使用<code>cp youwanttocopything newnameforyoupaste</code>,复制文件夹就比较复杂,首先<code>sudo mv 文件名 目标文件夹路径</code>（前提：必须在要移动的文件的目录下执行该命令，而且该文件夹下不能有文件夹，我的part2文件夹放在home目录下）,复制一个<strong>文件夹(该文件下可以有文件夹)</strong>到另一个文件夹下<code>sudo cp -r 文件名 目标文件夹路径</code><br> <img src="/image/cp.png" alt="cp"></p>
</li>
<li><p>修改文件名<code>mv originname targername</code>:<br> <img src="/image/mv.png" alt="mv"></p>
</li>
<li><p>删除文件<code>rm yourflie</code> 删除目录<code>rm -r listname</code>:<br> <img src="/image/rm.png" alt="rm"></p>
</li>
</ol>
<h3 id="接受vim告别vscode"><a href="#接受vim告别vscode" class="headerlink" title="接受vim告别vscode"></a>接受vim告别vscode</h3><p>虽然我不太可能用vim进行一个项目的开发但是当我们需要在服务器上进行调试的时候我们还是需要使用vim浅浅的查看和修改一下我们的代码这是必须的（你不能指望在ssh给的命令行里使用.code打开任何东西）</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/23/fastaichapter6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/23/fastaichapter6/" class="post-title-link" itemprop="url">fastaichapter6</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-05-23 10:12:02" itemprop="dateCreated datePublished" datetime="2025-05-23T10:12:02+08:00">2025-05-23</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-27 14:41:56" itemprop="dateModified" datetime="2025-05-27T14:41:56+08:00">2025-05-27</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="其他计算机视觉问题"><a href="#其他计算机视觉问题" class="headerlink" title="其他计算机视觉问题"></a>其他计算机视觉问题</h1><p>上一章学习了实践中训练模型的一些基本技术，例如：学习率和周期律的选择。这一章我们需要深入学习其他两种机器视觉问题。</p>
<ul>
<li><p>多标签分类：发生在你想要预测每个图像的多个标签（有时甚至没有标签）</p>
</li>
<li><p>回归： 你的标签是一个或多个数字——数量而不是类别。</p>
</li>
</ul>
<p>这需要深入学习：输出激活，目标和损失函数。</p>
<h1 id="多标签分类"><a href="#多标签分类" class="headerlink" title="多标签分类"></a>多标签分类</h1><p><em>多标签分类</em>指的是识别图像中可能不只包含一种对象类别的问题。可能有多种对象，或者在你寻找的类别中根本没有对象。</p>
<p>tip:模型架构与前一章没有大改变，只有损失函数改变了。</p>
<h2 id="数据"><a href="#数据" class="headerlink" title="数据"></a>数据</h2><p>对于我们的示例，我们将使用 PASCAL 数据集，该数据集中的每个图像可以有多种分类对象。</p>
<p>我们首先按照通常的方式下载和提取数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai.vision.<span class="built_in">all</span> <span class="keyword">import</span> *</span><br><span class="line">path = untar_data(URLs.PASCAL_2007)</span><br></pre></td></tr></table></figure>
<p>这个数据集与我们之前看到的不同，它不是按文件名或文件夹结构化的，而是附带一个 CSV 文件，告诉我们每个图像要使用的标签。我们可以通过将其读入 Pandas DataFrame 来检查 CSV 文件：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(path/<span class="string">&#x27;train.csv&#x27;</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>文件名</th>
<th>标签</th>
<th>是否有效</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>000005.jpg</td>
<td>椅子</td>
<td>True</td>
</tr>
<tr>
<td>1</td>
<td>000007.jpg</td>
<td>汽车</td>
<td>True</td>
</tr>
<tr>
<td>2</td>
<td>000009.jpg</td>
<td>马 人</td>
<td>True</td>
</tr>
<tr>
<td>3</td>
<td>000012.jpg</td>
<td>汽车</td>
<td>False</td>
</tr>
<tr>
<td>4</td>
<td>000016.jpg</td>
<td>自行车</td>
<td>True</td>
</tr>
</tbody>
</table>
</div>
<p>正如你所看到的，每个图像中的类别列表显示为一个以空格分隔的字符串。</p>
<p>既然我们已经看到了数据的样子，让我们准备好进行模型训练。</p>
<h2 id="构建数据块"><a href="#构建数据块" class="headerlink" title="构建数据块"></a>构建数据块</h2><p>将<code>DataFrame</code>对象转换为<code>DataLoaders</code>对象:使用数据块API 来创建<code>DataLoaders</code>对象，因为它提供了灵活性和简单性的良好组合。</p>
<p>以该数据集为例，展示使用数据块 API 构建<code>DataLoaders</code>对象的实践步骤。</p>
<p><code>数据集</code></p>
<p>返回单个项目的独立变量和依赖变量的元组的集合</p>
<p><code>数据加载器</code></p>
<p>提供一系列小批量的迭代器，其中每个小批量是一批独立变量和一批因变量的组合</p>
<p>除此之外，fastai 还提供了两个类来将您的训练和验证集合在一起：</p>
<p><code>Datasets</code></p>
<p>包含一个训练<code>Dataset</code>和一个验证<code>Dataset</code>的迭代器</p>
<p><code>DataLoaders</code></p>
<p>包含一个训练<code>DataLoader</code>和一个验证<code>DataLoader</code>的对象</p>
<p>由于<code>DataLoader</code>是建立在<code>Dataset</code>之上并为其添加附加功能（将多个项目整合成一个小批量），通常最容易的方法是首先创建和测试<code>Datasets</code>，然后再查看<code>DataLoaders</code>。</p>
<p>当我们创建<code>DataBlock</code>时，我们逐步逐步构建，并使用笔记本检查我们的数据。这是一个很好的方式，可以确保您在编码时保持动力，并留意任何问题。易于调试，因为您知道如果出现问题，它就在您刚刚输入的代码行中！</p>
<p>让我们从没有参数创建的数据块开始，这是最简单的情况：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dblock = DataBlock()</span><br></pre></td></tr></table></figure></p>
<p>我们可以从中创建一个<code>Datasets</code>对象。唯一需要的是一个源——在这种情况下是我们的 DataFrame：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dsets = dblock.datasets(df)</span><br></pre></td></tr></table></figure>
<p>这包含一个<code>train</code>和一个<code>valid</code>数据集，我们可以对其进行索引：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dsets.train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">(fname       <span class="number">008663.j</span>pg</span><br><span class="line"> labels      car person</span><br><span class="line"> is_valid    <span class="literal">False</span></span><br><span class="line"> Name: <span class="number">4346</span>, dtype: <span class="built_in">object</span>,</span><br><span class="line"> fname       <span class="number">008663.j</span>pg</span><br><span class="line"> labels      car person</span><br><span class="line"> is_valid    <span class="literal">False</span></span><br><span class="line"> Name: <span class="number">4346</span>, dtype: <span class="built_in">object</span>)</span><br></pre></td></tr></table></figure>
<p>正如您所看到的，这只是简单地两次返回 DataFrame 的一行。这是因为默认情况下，数据块假定我们有两个东西：输入和目标。我们需要从 DataFrame 中获取适当的字段，可以通过传递<code>get_x</code>和<code>get_y</code>函数来实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dblock = DataBlock(get_x = <span class="keyword">lambda</span> r: r[<span class="string">&#x27;fname&#x27;</span>], get_y = <span class="keyword">lambda</span> r: r[<span class="string">&#x27;labels&#x27;</span>])</span><br><span class="line">dsets = dblock.datasets(df)</span><br><span class="line">dsets.train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&#x27;005620.jpg&#x27;</span>, <span class="string">&#x27;aeroplane&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>正如您所看到的，我们并没有以通常的方式定义函数，而是使用了 Python 的<a target="_blank" rel="noopener" href="https://www.runoob.com/python3/python-lambda.html">lambda</a>关键字。这只是定义并引用函数的一种快捷方式。以下更冗长的方法是相同的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_x</span>(<span class="params">r</span>): <span class="keyword">return</span> r[<span class="string">&#x27;fname&#x27;</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_y</span>(<span class="params">r</span>): <span class="keyword">return</span> r[<span class="string">&#x27;labels&#x27;</span>]</span><br><span class="line">dblock = DataBlock(get_x = get_x, get_y = get_y)</span><br><span class="line">dsets = dblock.datasets(df)</span><br><span class="line">dsets.train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&#x27;002549.jpg&#x27;</span>, <span class="string">&#x27;tvmonitor&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>tip:Lambda 函数非常适合快速迭代，但不兼容序列化，因此我们建议您在训练后要导出您的<code>Learner</code>时使用更冗长的方法（如果您只是在尝试实验，lambda 是可以的）。</p>
<p>我们可以看到独立变量需要转换为完整路径，以便我们可以将其作为图像打开，而因变量需要根据空格字符（这是 Python 的<code>split</code>函数的默认值）进行拆分，以便它变成一个列表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_x</span>(<span class="params">r</span>): <span class="keyword">return</span> path/<span class="string">&#x27;train&#x27;</span>/r[<span class="string">&#x27;fname&#x27;</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_y</span>(<span class="params">r</span>): <span class="keyword">return</span> r[<span class="string">&#x27;labels&#x27;</span>].split(<span class="string">&#x27; &#x27;</span>)</span><br><span class="line">dblock = DataBlock(get_x = get_x, get_y = get_y)</span><br><span class="line">dsets = dblock.datasets(df)</span><br><span class="line">dsets.train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(Path(<span class="string">&#x27;/home/sgugger/.fastai/data/pascal_2007/train/008663.jpg&#x27;</span>),</span><br><span class="line"> [<span class="string">&#x27;car&#x27;</span>, <span class="string">&#x27;person&#x27;</span>])</span><br></pre></td></tr></table></figure>
<p>要实际打开图像并将其转换为张量，我们需要使用一组转换；块类型将为我们提供这些。我们可以使用先前使用过的相同块类型，只有一个例外：<code>ImageBlock</code>将再次正常工作，因为我们有一个指向有效图像的路径，但<code>CategoryBlock</code>不会起作用。问题在于该块返回一个单个整数，但我们需要为每个项目有多个标签。为了解决这个问题，我们使用<code>MultiCategoryBlock</code>。这种类型的块期望接收一个字符串列表，就像我们在这种情况下所做的那样，所以让我们来测试一下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),</span><br><span class="line">                   get_x = get_x, get_y = get_y)</span><br><span class="line">dsets = dblock.datasets(df)</span><br><span class="line">dsets.train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(PILImage mode=RGB size=500x375,</span><br><span class="line"> TensorMultiCategory([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>,</span><br><span class="line"> &gt; <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]))</span><br></pre></td></tr></table></figure>
<p>正如您所看到的，我们的类别列表的编码方式与常规的<code>CategoryBlock</code>不同。在那种情况下，我们有一个整数表示哪个类别存在，基于它在我们的词汇表中的位置。然而，在这种情况下，我们有一系列 0，其中任何位置上有一个 1 表示该类别存在。例如，如果第二和第四位置上有一个 1，那意味着词汇项二和四在这个图像中存在。这被称为<em>独热编码</em>。我们不能简单地使用类别索引列表的原因是每个列表的长度都不同，而 PyTorch 需要张量，其中所有内容必须是相同长度。</p>
<h1 id="专业术语：-独热编码"><a href="#专业术语：-独热编码" class="headerlink" title="专业术语： 独热编码"></a>专业术语： 独热编码</h1><p>使用一个 0 向量，其中每个位置都表示数据中表示的位置，以编码一个整数列表。</p>
<p>使用<code>torch.where</code>函数表明这个例子中的类代表什么。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">idxs = torch.where(dsets.train[<span class="number">0</span>][<span class="number">1</span>]==<span class="number">1.</span>)[<span class="number">0</span>]</span><br><span class="line">dsets.train.vocab[idxs]</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#1) [&#x27;dog&#x27;]</span></span><br></pre></td></tr></table></figure>
<p>使用 NumPy 数组、PyTorch 张量和 fastai 的<code>L</code>类，我们可以直接使用列表或向量进行索引，这使得很多代码（比如这个例子）更清晰、更简洁。</p>
<p>到目前为止，我们忽略了列<code>is_valid</code>，这意味着<code>DataBlock</code>一直在使用默认的随机拆分。要明确选择我们验证集的元素，我们需要编写一个函数并将其传递给<code>splitter</code>（或使用 fastai 的预定义函数或类之一）。它将获取项目（这里是我们整个 DataFrame）并必须返回两个（或更多）整数列表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">splitter</span>(<span class="params">df</span>):</span><br><span class="line">    train = df.index[~df[<span class="string">&#x27;is_valid&#x27;</span>]].tolist()</span><br><span class="line">    valid = df.index[df[<span class="string">&#x27;is_valid&#x27;</span>]].tolist()</span><br><span class="line">    <span class="keyword">return</span> train,valid</span><br><span class="line"></span><br><span class="line">dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),</span><br><span class="line">                   splitter=splitter,</span><br><span class="line">                   get_x=get_x,</span><br><span class="line">                   get_y=get_y)</span><br><span class="line"></span><br><span class="line">dsets = dblock.datasets(df)</span><br><span class="line">dsets.train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(PILImage mode=RGB size=500x333,</span><br><span class="line"> TensorMultiCategory([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>,</span><br><span class="line"> &gt; <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]))</span><br></pre></td></tr></table></figure>
<p>正如我们讨论过的，<code>DataLoader</code>将<code>Dataset</code>中的项目整理成一个小批量。这是一个张量的元组，其中每个张量简单地堆叠了<code>Dataset</code>项目中该位置的项目。</p>
<p>现在我们已经确认了单个项目看起来没问题，还有一步，我们需要确保我们可以创建我们的<code>DataLoaders</code>，即确保每个项目的大小相同。为了做到这一点，我们可以使用<code>RandomResizedCrop</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),</span><br><span class="line">                   splitter=splitter,</span><br><span class="line">                   get_x=get_x,</span><br><span class="line">                   get_y=get_y,</span><br><span class="line">                   item_tfms = RandomResizedCrop(<span class="number">128</span>, min_scale=<span class="number">0.35</span>))</span><br><span class="line">dls = dblock.dataloaders(df)</span><br></pre></td></tr></table></figure>
<p>现在我们可以显示我们数据的一个样本：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dls.show_batch(nrows=<span class="number">1</span>, ncols=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_06in01.png" alt=""></p>
<p>请记住，如果在从<code>DataBlock</code>创建<code>DataLoaders</code>时出现任何问题，或者如果您想查看<code>DataBlock</code>的确切情况，您可以使用我们在上一章中介绍的<code>summary</code>方法。</p>
<p>我们的数据现在已经准备好用于训练模型。正如我们将看到的，当我们创建我们的<code>Learner</code>时，没有任何变化，但在幕后，fastai 库将为我们选择一个新的损失函数：二元交叉熵。</p>
<h2 id="二元交叉熵"><a href="#二元交叉熵" class="headerlink" title="二元交叉熵"></a>二元交叉熵</h2><p>创建<code>Learner</code>时其分为四部分：模型，<code>DataLoaders</code>对象，优化器和要使用的损失函数。</p>
<p>本次我们已经构建好了<code>DataLoaders</code>，利用fastai的<code>resnet</code>模型，创建一个<code>SGD</code>优化器，在这章中我们专注于创建一个适合的损失函数。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet18)</span><br></pre></td></tr></table></figure>
<p><code>Learner</code>中的模型通常是从<code>nn.Module</code>继承的类的对象，并且我们可以使用括号调用它，它将返回模型的激活。你应该将独立变量作为一个小批量传递给它。我们可以尝试从我们的<code>DataLoader</code>中获取一个小批量，然后将其传递给模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x,y = dls.train.one_batch()</span><br><span class="line">activs = learn.model(x)</span><br><span class="line">activs.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">64</span>, <span class="number">20</span>])</span><br></pre></td></tr></table></figure>
<p>批量大小为 64，我们需要计算 20 个类别中的每一个的概率。其中一个激活的样子如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activs[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">2.0258</span>, -<span class="number">1.3543</span>,  <span class="number">1.4640</span>,  <span class="number">1.7754</span>, -<span class="number">1.2820</span>, -<span class="number">5.8053</span>,  <span class="number">3.6130</span>,  <span class="number">0.7193</span>,</span><br><span class="line"> &gt; -<span class="number">4.3683</span>, -<span class="number">2.5001</span>, -<span class="number">2.8373</span>, -<span class="number">1.8037</span>,  <span class="number">2.0122</span>,  <span class="number">0.6189</span>,  <span class="number">1.9729</span>,  <span class="number">0.8999</span>,</span><br><span class="line"> &gt; -<span class="number">2.6769</span>, -<span class="number">0.3829</span>,  <span class="number">1.2212</span>,  <span class="number">1.6073</span>],</span><br><span class="line">       device=<span class="string">&#x27;cuda:0&#x27;</span>, grad_fn=&lt;SelectBackward&gt;)</span><br></pre></td></tr></table></figure>
<h1 id="获取模型激活"><a href="#获取模型激活" class="headerlink" title="获取模型激活"></a>获取模型激活</h1><p>手动获取一个小批量并将其传递到模型中，并查看激活和损失，对于调试模型非常重要。这对学习也非常有帮助，这样你就可以清楚地看到发生了什么。</p>
<p>它们还没有缩放到 0 到 1 之间，但我们学会了如何在第四章中使用<code>sigmoid</code>函数来做到这一点。我们还看到了如何基于此计算损失——这是我们在第四章中的损失函数，加上了在前一章中讨论的<code>log</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_cross_entropy</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    inputs = inputs.sigmoid()</span><br><span class="line">    <span class="keyword">return</span> -torch.where(targets==<span class="number">1</span>, inputs, <span class="number">1</span>-inputs).log().mean()</span><br></pre></td></tr></table></figure>
<p>请注意，由于我们有一个独热编码的因变量，我们不能直接使用<code>nll_loss</code>或<code>softmax</code>（因此我们不能使用<code>cross_entropy</code>）：</p>
<ul>
<li><p>正如我们所看到的，<code>softmax</code>要求所有预测总和为 1，并且倾向于使一个激活远远大于其他激活（因为使用了<code>exp</code>）；然而，我们可能有多个我们确信出现在图像中的对象，因此限制激活的最大总和为 1 并不是一个好主意。出于同样的原因，如果我们认为<em>任何</em>类别都不出现在图像中，我们可能希望总和<em>小于</em>1。</p>
</li>
<li><p>正如我们所看到的，<code>nll_loss</code>返回的是一个激活值：与项目的单个标签对应的单个激活值。当我们有多个标签时，这是没有意义的。</p>
</li>
</ul>
<p>另一方面，<code>binary_cross_entropy</code>函数，即<code>mnist_loss</code>加上<code>log</code>，正是我们所需要的，这要归功于 PyTorch 的逐元素操作的魔力。每个激活将与每个列的每个目标进行比较，因此我们不必做任何事情使此函数适用于多个列。</p>
<h1 id="Jeremy-Says"><a href="#Jeremy-Says" class="headerlink" title="Jeremy Says"></a>Jeremy Says</h1><p>我真的很喜欢使用像 PyTorch 这样的库，具有广播和逐元素操作，因为我经常发现我可以编写的代码同样适用于单个项目或一批项目，而无需更改。<code>binary_cross_entropy</code>就是一个很好的例子。通过使用这些操作，我们不必自己编写循环，可以依赖 PyTorch 根据我们正在处理的张量的秩适当地执行我们需要的循环。</p>
<p>PyTorch 已经为我们提供了这个函数。实际上，它提供了许多版本，名称相当令人困惑！</p>
<p><code>F.binary_cross_entropy</code>及其模块等效<code>nn.BCELoss</code>计算一个独热编码目标的交叉熵，但不包括初始的<code>sigmoid</code>。通常，对于独热编码目标，您将希望使用<code>F.binary_cross_entropy_with_logits</code>（或<code>nn.BCEWithLogitsLoss</code>），它们在一个函数中同时执行 sigmoid 和二元交叉熵，就像前面的例子一样。</p>
<p>对于单标签数据集（如 MNIST 或 Pet 数据集），其中目标被编码为单个整数，相应的是<code>F.nll_loss</code>或<code>nn.NLLLoss</code>（没有初始 softmax 的版本），以及<code>F.cross_entropy</code>或<code>nn.CrossEntropyLoss</code>（具有初始 softmax 的版本）。</p>
<p>由于我们有一个独热编码的目标，我们将使用<code>BCEWithLogitsLoss</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss_func = nn.BCEWithLogitsLoss()</span><br><span class="line">loss = loss_func(activs, y)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.0082</span>, device=<span class="string">&#x27;cuda:5&#x27;</span>, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;)</span><br></pre></td></tr></table></figure>
<p>我们不需要告诉 fastai 使用这个损失函数（尽管如果我们想要的话可以这样做），因为它将自动为我们选择。fastai 知道<code>DataLoaders</code>具有多个类别标签，因此默认情况下将使用<code>nn.BCEWithLogitsLoss</code>。</p>
<p>与前一章相比的一个变化是我们使用的指标：因为这是一个多标签问题，我们不能使用准确度函数。为什么呢？嗯，准确度是这样比较我们的输出和我们的目标的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">inp, targ, axis=-<span class="number">1</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot;</span></span><br><span class="line">    pred = inp.argmax(dim=axis)</span><br><span class="line">    <span class="keyword">return</span> (pred == targ).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>预测的类是具有最高激活的类（这就是<code>argmax</code>的作用）。这里不起作用，因为我们可能在单个图像上有多个预测。在对我们的激活应用 sigmoid（使它们在 0 和 1 之间）之后，我们需要通过选择<em>阈值</em>来决定哪些是 0，哪些是 1。高于阈值的每个值将被视为 1，低于阈值的每个值将被视为 0：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy_multi</span>(<span class="params">inp, targ, thresh=<span class="number">0.5</span>, sigmoid=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;Compute accuracy when `inp` and `targ` are the same size.&quot;</span></span><br><span class="line">    <span class="keyword">if</span> sigmoid: inp = inp.sigmoid()</span><br><span class="line">    <span class="keyword">return</span> ((inp&gt;thresh)==targ.<span class="built_in">bool</span>()).<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>如果我们直接将<code>accuracy_multi</code>作为指标传递，它将使用<code>threshold</code>的默认值，即 0.5。我们可能希望调整该默认值并创建一个具有不同默认值的新版本的<code>accuracy_multi</code>。为了帮助解决这个问题，Python 中有一个名为<code>partial</code>的函数。它允许我们<em>绑定</em>一个带有一些参数或关键字参数的函数，从而创建该函数的新版本，每当调用它时，总是包含这些参数。例如，这里是一个接受两个参数的简单函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">say_hello</span>(<span class="params">name, say_what=<span class="string">&quot;Hello&quot;</span></span>): <span class="keyword">return</span> <span class="string">f&quot;<span class="subst">&#123;say_what&#125;</span> <span class="subst">&#123;name&#125;</span>.&quot;</span></span><br><span class="line">say_hello(<span class="string">&#x27;Jeremy&#x27;</span>),say_hello(<span class="string">&#x27;Jeremy&#x27;</span>, <span class="string">&#x27;Ahoy!&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&#x27;Hello Jeremy.&#x27;</span>, <span class="string">&#x27;Ahoy! Jeremy.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>我们可以通过使用<a target="_blank" rel="noopener" href="https://www.runoob.com/w3cnote/python-partial.html">partial</a>切换到该函数的法语版本：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f = partial(say_hello, say_what=<span class="string">&quot;Bonjour&quot;</span>)</span><br><span class="line">f(<span class="string">&quot;Jeremy&quot;</span>),f(<span class="string">&quot;Sylvain&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="string">&#x27;Bonjour Jeremy.&#x27;</span>, <span class="string">&#x27;Bonjour Sylvain.&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们可以训练我们的模型。让我们尝试将准确度阈值设置为 0.2 作为我们的指标：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=<span class="number">0.2</span>))</span><br><span class="line">learn.fine_tune(<span class="number">3</span>, base_lr=<span class="number">3e-3</span>, freeze_epochs=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>accuracy_multi</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.903610</td>
<td>0.659728</td>
<td>0.263068</td>
<td>00:07</td>
</tr>
<tr>
<td>1</td>
<td>0.724266</td>
<td>0.346332</td>
<td>0.525458</td>
<td>00:07</td>
</tr>
<tr>
<td>2</td>
<td>0.415597</td>
<td>0.125662</td>
<td>0.937590</td>
<td>00:07</td>
</tr>
<tr>
<td>3</td>
<td>0.254987</td>
<td>0.116880</td>
<td>0.945418</td>
<td>00:07</td>
</tr>
<tr>
<td>epoch</td>
<td>train_loss</td>
<td>valid_loss</td>
<td>accuracy_multi</td>
<td>time</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>0</td>
<td>0.123872</td>
<td>0.132634</td>
<td>0.940179</td>
<td>00:08</td>
</tr>
<tr>
<td>1</td>
<td>0.112387</td>
<td>0.113758</td>
<td>0.949343</td>
<td>00:08</td>
</tr>
<tr>
<td>2</td>
<td>0.092151</td>
<td>0.104368</td>
<td>0.951195</td>
<td>00:08</td>
</tr>
</tbody>
</table>
</div>
<p>选择阈值很重要。如果选择的阈值太低，通常会选择错误标记的对象。我们可以通过改变我们的度量标准然后调用<code>validate</code>来看到这一点，它会返回验证损失和度量标准：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.metrics = partial(accuracy_multi, thresh=<span class="number">0.1</span>)</span><br><span class="line">learn.validate()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#2) [0.10436797887086868,0.93057781457901]</span></span><br></pre></td></tr></table></figure>
<p>如果选择的阈值太高，将只选择模型非常有信心的对象：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn.metrics = partial(accuracy_multi, thresh=<span class="number">0.99</span>)</span><br><span class="line">learn.validate()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#2) [0.10436797887086868,0.9416930675506592]</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过尝试几个级别并查看哪个效果最好来找到最佳阈值。如果我们只抓取一次预测，这将快得多：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preds,targs = learn.get_preds()</span><br></pre></td></tr></table></figure>
<p>然后我们可以直接调用度量标准。请注意，默认情况下，<code>get_preds</code>会为我们应用输出激活函数（在本例中为 sigmoid），因此我们需要告诉<code>accuracy_multi</code>不要应用它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accuracy_multi(preds, targs, thresh=<span class="number">0.9</span>, sigmoid=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TensorMultiCategory(<span class="number">0.9554</span>)</span><br></pre></td></tr></table></figure>
<p>现在我们可以使用这种方法找到最佳阈值水平：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xs = torch.linspace(<span class="number">0.05</span>,<span class="number">0.95</span>,<span class="number">29</span>)</span><br><span class="line">accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=<span class="literal">False</span>) <span class="keyword">for</span> i <span class="keyword">in</span> xs]</span><br><span class="line">plt.plot(xs,accs);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_06in02.png" alt=""></p>
<p>在这种情况下，我们使用验证集来选择一个超参数（阈值），这就是验证集的目的。有时学生们表达了他们的担忧，即我们可能会对验证集<em>过拟合</em>，因为我们正在尝试很多值来找出哪个是最好的。然而，正如你在图中看到的，改变阈值在这种情况下会产生一个平滑的曲线，因此我们显然没有选择不合适的异常值。这是一个很好的例子，说明你必须小心理论（不要尝试很多超参数值，否则可能会过拟合验证集）与实践（如果关系是平滑的，这样做是可以的）之间的区别。</p>
<h1 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h1><p>很容易将深度学习模型视为被分类到领域中，如<em>计算机视觉</em>、<em>NLP</em>等等。事实上，这就是 fastai 对其应用程序进行分类的方式——主要是因为大多数人习惯于这样思考事物。</p>
<p>但实际上，这隐藏了一个更有趣和更深入的视角。一个模型由其独立和依赖变量以及其损失函数定义。这意味着实际上有比简单的基于领域的分割更广泛的模型数组。也许我们有一个独立变量是图像，一个依赖变量是文本（例如，从图像生成标题）；或者我们有一个独立变量是文本，一个依赖变量是图像（例如，从标题生成图像——这实际上是深度学习可以做到的！）；或者我们有图像、文本和表格数据作为独立变量，我们试图预测产品购买……可能性真的是无穷无尽的。</p>
<p>要能够超越固定应用程序，为新问题制定自己的新颖解决方案，真正理解数据块 API（也许还有我们将在本书后面看到的中间层 API）是有帮助的。举个例子，让我们考虑<em>图像回归</em>的问题。这指的是从一个独立变量是图像，依赖变量是一个或多个浮点数的数据集中学习。通常我们看到人们将图像回归视为一个完全独立的应用程序——但正如你在这里看到的，我们可以将其视为数据块 API 上的另一个 CNN。</p>
<p>我们将直接跳到图像回归的一个有点棘手的变体，因为我们知道你已经准备好了！我们将做一个关键点模型。<em>关键点</em>指的是图像中表示的特定位置——在这种情况下，我们将使用人物的图像，并且我们将寻找每个图像中人脸的中心。这意味着我们实际上将为每个图像预测<em>两个</em>值：人脸中心的行和列。</p>
<h2 id="数据组装"><a href="#数据组装" class="headerlink" title="数据组装"></a>数据组装</h2><p>我们将在这一部分使用<a target="_blank" rel="noopener" href="https://oreil.ly/-4cO-">Biwi Kinect Head Pose 数据集</a>。我们将像往常一样开始下载数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path = untar_data(URLs.BIWI_HEAD_POSE)</span><br></pre></td></tr></table></figure>
<p>让我们看看我们有什么！</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path.ls()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#50) [Path(&#x27;13.obj&#x27;),Path(&#x27;07.obj&#x27;),Path(&#x27;06.obj&#x27;),Path(&#x27;13&#x27;),Path(&#x27;10&#x27;),Path(&#x27;</span></span><br><span class="line"> &gt; 02<span class="string">&#x27;),Path(&#x27;</span><span class="number">11</span><span class="string">&#x27;),Path(&#x27;</span>01<span class="string">&#x27;),Path(&#x27;</span><span class="number">20.</span>obj<span class="string">&#x27;),Path(&#x27;</span><span class="number">17</span><span class="string">&#x27;)...]</span></span><br></pre></td></tr></table></figure>
<p>有 24 个从 01 到 24 编号的目录（它们对应不同的被摄人物），以及每个目录对应的<em>.obj</em>文件（我们这里不需要）。让我们看看其中一个目录的内容：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(path/<span class="string">&#x27;01&#x27;</span>).ls()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#1000) [Path(&#x27;01/frame_00281_pose.txt&#x27;),Path(&#x27;01/frame_00078_pose.txt&#x27;),Path(&#x27;0</span></span><br><span class="line"> &gt; <span class="number">1</span>/frame_00349_rgb.jpg<span class="string">&#x27;),Path(&#x27;</span>01/frame_00304_pose.txt<span class="string">&#x27;),Path(&#x27;</span>01/frame_00207_</span><br><span class="line"> &gt; pose.txt<span class="string">&#x27;),Path(&#x27;</span>01/frame_00116_rgb.jpg<span class="string">&#x27;),Path(&#x27;</span>01/frame_00084_rgb.jpg<span class="string">&#x27;),Path</span></span><br><span class="line"><span class="string"> &gt; (&#x27;</span>01/frame_00070_rgb.jpg<span class="string">&#x27;),Path(&#x27;</span>01/frame_00125_pose.txt<span class="string">&#x27;),Path(&#x27;</span>01/frame_003</span><br><span class="line"> &gt; 24_rgb.jpg<span class="string">&#x27;)...]</span></span><br></pre></td></tr></table></figure>
<p>在子目录中，我们有不同的帧。每个帧都带有一个图像（<em>_rgb.jpg</em>）和一个姿势文件（<em>_pose.txt</em>）。我们可以使用<code>get_image_files</code>轻松递归获取所有图像文件，然后编写一个函数，将图像文件名转换为其关联的姿势文件：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">img_files = get_image_files(path)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">img2pose</span>(<span class="params">x</span>): <span class="keyword">return</span> Path(<span class="string">f&#x27;<span class="subst">&#123;<span class="built_in">str</span>(x)[:-<span class="number">7</span>]&#125;</span>pose.txt&#x27;</span>)</span><br><span class="line">img2pose(img_files[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Path(<span class="string">&#x27;13/frame_00349_pose.txt&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>让我们来看看我们的第一张图片：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">im = PILImage.create(img_files[<span class="number">0</span>])</span><br><span class="line">im.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">480</span>, <span class="number">640</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">im.to_thumb(<span class="number">160</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_06in03.png" alt=""></p>
<p><a target="_blank" rel="noopener" href="https://oreil.ly/wHL28">Biwi 数据集网站</a>用于解释与每个图像关联的姿势文本文件的格式，显示头部中心的位置。这些细节对我们来说并不重要，所以我们只会展示我们用来提取头部中心点的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cal = np.genfromtxt(path/<span class="string">&#x27;01&#x27;</span>/<span class="string">&#x27;rgb.cal&#x27;</span>, skip_footer=<span class="number">6</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_ctr</span>(<span class="params">f</span>):</span><br><span class="line">    ctr = np.genfromtxt(img2pose(f), skip_header=<span class="number">3</span>)</span><br><span class="line">    c1 = ctr[<span class="number">0</span>] * cal[<span class="number">0</span>][<span class="number">0</span>]/ctr[<span class="number">2</span>] + cal[<span class="number">0</span>][<span class="number">2</span>]</span><br><span class="line">    c2 = ctr[<span class="number">1</span>] * cal[<span class="number">1</span>][<span class="number">1</span>]/ctr[<span class="number">2</span>] + cal[<span class="number">1</span>][<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> tensor([c1,c2])</span><br></pre></td></tr></table></figure>
<p>这个函数将坐标作为两个项目的张量返回：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_ctr(img_files[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">384.6370</span>, <span class="number">259.4787</span>])</span><br></pre></td></tr></table></figure>
<p>我们可以将此函数传递给<code>DataBlock</code>作为<code>get_y</code>，因为它负责为每个项目标记。我们将将图像调整为其输入大小的一半，以加快训练速度。</p>
<p>一个重要的要点是我们不应该只使用随机分割器。在这个数据集中，同一个人出现在多个图像中，但我们希望确保我们的模型可以泛化到它尚未见过的人。数据集中的每个文件夹包含一个人的图像。因此，我们可以创建一个分割器函数，仅为一个人返回<code>True</code>，从而使验证集仅包含该人的图像。</p>
<p>与以前的数据块示例的唯一区别是第二个块是<code>PointBlock</code>。这是必要的，以便 fastai 知道标签代表坐标；这样，它就知道在进行数据增强时，应该对这些坐标执行与图像相同的增强：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">biwi = DataBlock(</span><br><span class="line">    blocks=(ImageBlock, PointBlock),</span><br><span class="line">    get_items=get_image_files,</span><br><span class="line">    get_y=get_ctr,</span><br><span class="line">    splitter=FuncSplitter(<span class="keyword">lambda</span> o: o.parent.name==<span class="string">&#x27;13&#x27;</span>),</span><br><span class="line">    batch_tfms=[*aug_transforms(size=(<span class="number">240</span>,<span class="number">320</span>)),</span><br><span class="line">                Normalize.from_stats(*imagenet_stats)]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h1 id="点和数据增强"><a href="#点和数据增强" class="headerlink" title="点和数据增强"></a>点和数据增强</h1><p>我们不知道其他库（除了 fastai）会自动且正确地将数据增强应用于坐标。因此，如果您使用另一个库，可能需要禁用这些问题的数据增强。</p>
<p>在进行任何建模之前，我们应该查看我们的数据以确认它看起来没问题：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dls = biwi.dataloaders(path)</span><br><span class="line">dls.show_batch(max_n=<span class="number">9</span>, figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br></pre></td></tr></table></figure>
<p><img src="img/dlcf_06in04.png" alt=""></p>
<p>看起来不错！除了通过视觉查看批次外，还可以查看底层张量（尤其是作为学生；这将有助于澄清您对模型实际看到的内容的理解）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xb,yb = dls.one_batch()</span><br><span class="line">xb.shape,yb.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">64</span>, <span class="number">3</span>, <span class="number">240</span>, <span class="number">320</span>]), torch.Size([<span class="number">64</span>, <span class="number">1</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<p>确保您了解为什么这些是我们小批量的形状。</p>
<p>这是依赖变量的一个示例行：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yb[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.0111</span>, <span class="number">0.1810</span>]], device=<span class="string">&#x27;cuda:5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>正如您所看到的，我们不必使用单独的<em>图像回归</em>应用程序；我们所要做的就是标记数据并告诉 fastai 独立变量和因变量代表什么类型的数据。</p>
<p>创建我们的<code>Learner</code>也是一样的。我们将使用与之前相同的函数，只有一个新参数，然后我们就可以准备训练我们的模型了。</p>
<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p>像往常一样，我们可以使用<code>cnn_learner</code>来创建我们的<code>Learner</code>。还记得在第一章中我们如何使用<code>y_range</code>告诉 fastai 我们目标的范围吗？我们将在这里做同样的事情（fastai 和 PyTorch 中的坐标始终在-1 和+1 之间重新缩放）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet18, y_range=(-<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p><code>y_range</code>在 fastai 中使用<code>sigmoid_range</code>实现，其定义如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid_range</span>(<span class="params">x, lo, hi</span>): <span class="keyword">return</span> torch.sigmoid(x) * (hi-lo) + lo</span><br></pre></td></tr></table></figure>
<p>如果定义了<code>y_range</code>，则将其设置为模型的最终层。花点时间思考一下这个函数的作用，以及为什么它强制模型在范围<code>(lo,hi)</code>内输出激活。</p>
<p>这是它的样子：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(partial(sigmoid_range,lo=-<span class="number">1</span>,hi=<span class="number">1</span>), <span class="built_in">min</span>=-<span class="number">4</span>, <span class="built_in">max</span>=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_06in05.png" alt=""></p>
<p>我们没有指定损失函数，这意味着我们得到了 fastai 选择的默认值。让我们看看它为我们选择了什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dls.loss_func</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">FlattenedLoss of MSELoss()</span><br></pre></td></tr></table></figure>
<p>这是有道理的，因为当坐标被用作因变量时，大多数情况下我们可能会尽可能地预测接近某个值；这基本上就是 <code>MSELoss</code>（均方误差损失）所做的。如果你想使用不同的损失函数，你可以通过使用 <code>loss_func</code> 参数将其传递给 <code>cnn_learner</code>。</p>
<p>还要注意，我们没有指定任何指标。这是因为均方误差已经是这个任务的一个有用指标（尽管在我们取平方根之后可能更易解释）。</p>
<p>我们可以使用学习率查找器选择一个好的学习率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.lr_find()</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_06in06.png" alt=""></p>
<p>我们将尝试一个学习率为 2e-2：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">2e-2</span></span><br><span class="line">learn.fit_one_cycle(<span class="number">5</span>, lr)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.045840</td>
<td>0.012957</td>
<td>00:36</td>
</tr>
<tr>
<td>1</td>
<td>0.006369</td>
<td>0.001853</td>
<td>00:36</td>
</tr>
<tr>
<td>2</td>
<td>0.003000</td>
<td>0.000496</td>
<td>00:37</td>
</tr>
<tr>
<td>3</td>
<td>0.001963</td>
<td>0.000360</td>
<td>00:37</td>
</tr>
<tr>
<td>4</td>
<td>0.001584</td>
<td>0.000116</td>
<td>00:36</td>
</tr>
</tbody>
</table>
</div>
<p>通常情况下，当我们运行这个时，我们得到的损失大约是 0.0001，这对应于这个平均坐标预测误差：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">math.sqrt(<span class="number">0.0001</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.01</span></span><br></pre></td></tr></table></figure>
<p>这听起来非常准确！但是重要的是要用 <code>Learner.show_results</code> 查看我们的结果。左侧是实际（<em>真实</em>）坐标，右侧是我们模型的预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.show_results(ds_idx=<span class="number">1</span>, max_n=<span class="number">3</span>, figsize=(<span class="number">6</span>,<span class="number">8</span>))</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_06in07.png" alt=""></p>
<p>令人惊讶的是，仅仅几分钟的计算，我们就创建了一个如此准确的关键点模型，而且没有任何特定领域的应用。这就是在灵活的 API 上构建并使用迁移学习的力量！特别引人注目的是，我们能够如此有效地使用迁移学习，即使在完全不同的任务之间；我们的预训练模型是用来进行图像分类的，而我们对图像回归进行了微调。</p>
<h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>在乍一看完全不同的问题（单标签分类、多标签分类和回归）中，我们最终使用相同的模型，只是输出的数量不同。唯一改变的是损失函数，这就是为什么重要的是要仔细检查你是否为你的问题使用了正确的损失函数。</p>
<p>fastai 将自动尝试从您构建的数据中选择正确的损失函数，但如果您使用纯 PyTorch 构建您的 <code>DataLoader</code>，请确保您认真考虑您选择的损失函数，并记住您很可能想要以下内容：</p>
<ul>
<li><p><code>nn.CrossEntropyLoss</code> 用于单标签分类</p>
</li>
<li><p><code>nn.BCEWithLogitsLoss</code> 用于多标签分类</p>
</li>
<li><p><code>nn.MSELoss</code> 用于回归</p>
</li>
</ul>
<h1 id="课后题"><a href="#课后题" class="headerlink" title="课后题"></a>课后题</h1><ol>
<li>多标签分类如何提高熊分类器的可用性？</li>
</ol>
<ul>
<li>当图片名中没有熊或是出现的不是熊时他能够进行辨别</li>
</ul>
<ol>
<li>在多标签分类问题中，我们如何对因变量进行编码？</li>
</ol>
<ul>
<li>因变量需要根据空格字符（这是 Python 的<code>split</code>函数的默认值）进行拆分，以便它变成一个列表</li>
</ul>
<ol>
<li>如何访问 DataFrame 的行和列，就像它是一个矩阵一样？</li>
</ol>
<ul>
<li>访问 DataFrame 行和列：使用 iloc 按整数位置或 loc 按标签索引（如 df.iloc[行,列] 或 df.loc[行标签,列标签]）。</li>
</ul>
<ol>
<li>如何从 DataFrame 中按名称获取列？</li>
</ol>
<ul>
<li>按名称获取列：通过 df[“列名”] 获取单列（Series），或 df[[“列名”]] 保持 DataFrame 结构，列名特殊时用方括号。</li>
</ul>
<ol>
<li><code>Dataset</code> 和 <code>DataLoader</code> 之间有什么区别？</li>
</ol>
<ul>
<li>由于<code>DataLoader</code>是建立在<code>Dataset</code>之上并为其添加附加功能（将多个项目整合成一个小批量），通常最容易的方法是首先创建和测试<code>Datasets</code>，然后再查看<code>DataLoaders</code>。</li>
</ul>
<ol>
<li><code>Datasets</code> 对象通常包含什么？</li>
</ol>
<ul>
<li>包含一个训练<code>Dataset</code>和一个验证<code>Dataset</code>的迭代器</li>
</ul>
<ol>
<li><code>DataLoaders</code> 对象通常包含什么？</li>
</ol>
<ul>
<li>包含一个训练<code>DataLoader</code>和一个验证<code>DataLoader</code>的对象</li>
</ul>
<ol>
<li><code>lambda</code> 在 Python 中是做什么的？</li>
</ol>
<ul>
<li>这只是定义并引用函数的一种快捷方式</li>
</ul>
<ol>
<li>如何使用数据块 API 自定义独立变量和因变量的创建方法？</li>
</ol>
<ul>
<li>使用数据块 API 时，通过 get_x 和 get_y 参数分别定义处理输入（独立变量）和输出（因变量）的自定义方法，例如：<br>DataBlock(get_x=自定义函数, get_y=自定义函数)。</li>
</ul>
<ol>
<li>当使用一个独热编码的目标时，为什么 softmax 不是一个合适的输出激活函数？</li>
</ol>
<ul>
<li>因为所有类别概率总和并不为1</li>
</ul>
<ol>
<li>当使用一个独热编码的目标时，为什么 <code>nll_loss</code> 不是一个合适的损失函数？</li>
</ol>
<ul>
<li><code>nll_loss</code>返回的是一个激活值：与项目的单个标签对应的单个激活值。当我们有多个标签时，这是没有意义的。</li>
</ul>
<ol>
<li><code>nn.BCELoss</code> 和 <code>nn.BCEWithLogitsLoss</code> 之间有什么区别？</li>
</ol>
<ul>
<li><p><code>nn.BCELoss</code>计算一个独热编码目标的交叉熵，但不包括初始的<code>sigmoid</code>。</p>
</li>
<li><p><code>nn.BCEWithLogitsLoss</code>它在一个函数中同时执行 sigmoid 和二元交叉熵，就像前面的例子一样。</p>
</li>
</ul>
<ol>
<li>为什么在多标签问题中不能使用常规准确率？</li>
</ol>
<ul>
<li>在多标签问题中，常规准确率要求 所有标签完全正确匹配，而实际场景中部分标签预测正确即可有效，导致常规准确率过于严格且无法反映模型部分预测的有效性。</li>
</ul>
<ol>
<li>何时可以在验证集上调整超参数？</li>
</ol>
<ul>
<li>在验证集上调整超参数是标准做法，但需确保 测试集完全独立且仅用于最终评估，避免调整过程中信息泄漏导致过拟合和评估偏差。</li>
</ul>
<ol>
<li><code>y_range</code> 在 fastai 中是如何实现的？（看看你是否可以自己实现并在不偷看的情况下测试！）</li>
</ol>
<ul>
<li>在 fastai 中，y_range 通过 在模型输出层添加 Sigmoid 激活函数并线性缩放到指定范围 实现，适用于回归任务以约束预测值的合理区间（如 (a, b)）。</li>
</ul>
<ol>
<li>回归问题是什么？对于这样的问题应该使用什么损失函数？</li>
</ol>
<ul>
<li>回归问题是预测连续数值的任务（如价格、温度），常用 均方误差（MSE） 或 平均绝对误差（MAE） 作为损失函数，其中 MSE 是最基础且广泛使用的选择。</li>
</ul>
<ol>
<li>为了确保 fastai 库将相同的数据增强应用于您的输入图像和目标点坐标，您需要做什么？</li>
</ol>
<ul>
<li>使用PointBlock并将空间变换（如翻转、旋转）添加到item_tfms，同时确保目标点坐标已归一化，即可使fastai对图像和坐标同步应用相同数据增强。</li>
</ul>
<h2 id="进一步研究"><a href="#进一步研究" class="headerlink" title="进一步研究"></a>进一步研究</h2><ol>
<li><p>阅读关于 Pandas DataFrames 的教程，并尝试一些看起来有趣的方法。查看书籍网站上推荐的教程。</p>
</li>
<li><p>使用多标签分类重新训练熊分类器。看看你是否可以使其有效地处理不包含任何熊的图像，包括在 Web 应用程序中显示该信息。尝试一张包含两种熊的图像。检查在单标签数据集上使用多标签分类是否会影响准确性。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/13/fastaichapter5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/13/fastaichapter5/" class="post-title-link" itemprop="url">fastaichapter5</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-05-13 09:01:09" itemprop="dateCreated datePublished" datetime="2025-05-13T09:01:09+08:00">2025-05-13</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-22 18:23:41" itemprop="dateModified" datetime="2025-05-22T18:23:41+08:00">2025-05-22</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第五章：图像分类"><a href="#第五章：图像分类" class="headerlink" title="第五章：图像分类"></a>第五章：图像分类</h1><p>从本章开始学习深度学习机制，创建架构，从训练中获得最佳结果，加快速度和查看神经网络的内部情况，找到可能的问题并解决他们。</p>
<h3 id="本章目标"><a href="#本章目标" class="headerlink" title="本章目标"></a>本章目标</h3><p>我们将从重复第一章中查看的相同基本应用程序开始，但我们将做两件事：</p>
<ul>
<li><p>让它们变得更好。</p>
</li>
<li><p>将它们应用于更多类型的数据。</p>
</li>
</ul>
<h1 id="从狗和猫识别到宠物品种识别"><a href="#从狗和猫识别到宠物品种识别" class="headerlink" title="从狗和猫识别到宠物品种识别"></a>从狗和猫识别到宠物品种识别</h1><p>下载宠物数据</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastai2.vision.<span class="built_in">all</span> <span class="keyword">import</span> *</span><br><span class="line">path = untar_data(URLs.PETS)</span><br></pre></td></tr></table></figure>
<p><img src="/image/colab_0501.png" alt="数据集下载"></p>
<h4 id="了解数据布局"><a href="#了解数据布局" class="headerlink" title="了解数据布局"></a>了解数据布局</h4><p>现在，如果我们要理解如何从每个图像中提取每只宠物的品种，我们需要了解数据是如何布局的。数据布局的细节是深度学习难题的重要组成部分。数据通常以以下两种方式之一提供：</p>
<ul>
<li><p>表示数据项的个别文件，例如文本文档或图像，可能组织成文件夹或具有表示有关这些项信息的文件名</p>
</li>
<li><p>数据表（例如，以 CSV 格式）中的数据，其中每行是一个项目，可能包括文件名，提供表中数据与其他格式（如文本文档和图像）中数据之间的连接</p>
</li>
</ul>
<p>有一些例外情况——特别是在基因组学等领域，可能存在二进制数据库格式或甚至网络流——但总体而言，您将处理的绝大多数数据集将使用这两种格式的某种组合。</p>
<p>要查看数据集中的内容，我们可以使用<code>ls</code>方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path.ls()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#3) [Path(&#x27;annotations&#x27;),Path(&#x27;images&#x27;),Path(&#x27;models&#x27;)]</span></span><br></pre></td></tr></table></figure>
<p>与文档中结果不同的是现在下载的这个数据集内容如下：<br><img src="/image/colab_0502.png" alt="数据集查看"></p>
<p>我们可以看到这个数据集为我们提供了<em>images</em>和<em>annotations</em>目录。数据集的<a target="_blank" rel="noopener" href="https://oreil.ly/xveoN">网站</a>告诉我们<em>annotations</em>目录包含有关宠物所在位置而不是它们是什么的信息。</p>
<p>查看images的内容：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(path/<span class="string">&quot;images&quot;</span>).ls()</span><br></pre></td></tr></table></figure></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#7394) [Path(&#x27;images/great_pyrenees_173.jpg&#x27;),Path(&#x27;images/wheaten_terrier_46.j</span></span><br><span class="line"> &gt; pg<span class="string">&#x27;),Path(&#x27;</span>images/Ragdoll_262.jpg<span class="string">&#x27;),Path(&#x27;</span>images/german_shorthaired_3.jpg<span class="string">&#x27;),P</span></span><br><span class="line"><span class="string"> &gt; ath(&#x27;</span>images/american_bulldog_196.jpg<span class="string">&#x27;),Path(&#x27;</span>images/boxer_188.jpg<span class="string">&#x27;),Path(&#x27;</span>ima</span><br><span class="line"> &gt; ges/staffordshire_bull_terrier_173.jpg<span class="string">&#x27;),Path(&#x27;</span>images/basset_hound_71.jpg<span class="string">&#x27;),P</span></span><br><span class="line"><span class="string"> &gt; ath(&#x27;</span>images/staffordshire_bull_terrier_37.jpg<span class="string">&#x27;),Path(&#x27;</span>images/yorkshire_terrie</span><br><span class="line"> &gt; r_18.jpg<span class="string">&#x27;)...]</span></span><br></pre></td></tr></table></figure>
<p><img src="/image/colab_0503.png" alt="images数据集查看"></p>
<p>在 fastai 中，大多数返回集合的函数和方法使用一个名为<code>L</code>的类。这个类可以被认为是普通 Python <code>list</code>类型的增强版本，具有用于常见操作的附加便利。例如，当我们在笔记本中显示这个类的对象时，它会以这里显示的格式显示。首先显示的是集合中的项目数，前面带有<code>#</code>。在前面的输出中，你还会看到列表后面有省略号。这意味着只显示了前几个项目，这是件好事，因为我们不希望屏幕上出现超过 7000 个文件名！</p>
<p>通过检查这些文件名，我们可以看到它们似乎是如何结构化的。每个文件名包含宠物品种，然后是一个下划线（<code>_</code>），一个数字，最后是文件扩展名。我们需要创建一段代码，从单个<code>Path</code>中提取品种。Jupyter 笔记本使这变得容易，因为我们可以逐渐构建出可用的东西，然后用于整个数据集。在这一点上，我们必须小心不要做太多假设。例如，如果你仔细观察，你可能会注意到一些宠物品种包含多个单词，因此我们不能简单地在找到的第一个<code>_</code>字符处中断。为了让我们能够测试我们的代码，让我们挑选出一个这样的文件名：</p>
<font color = orange>如何利用好文件名</font>

<h3 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h3><p>从这样的字符串中提取信息的最强大和灵活的方法是使用<em>regular expression</em>，也称为<em>regex</em>。<a target="_blank" rel="noopener" href="https://www.runoob.com/regexp/regexp-syntax.html">正则表达式</a>是一种特殊的字符串，用正则表达式语言编写，它指定了一个一般规则，用于决定另一个字符串是否通过测试（即“匹配”正则表达式），并且可能用于从另一个字符串中提取特定部分。在这种情况下，我们需要一个正则表达式从文件名中提取宠物品种。</p>
<p>正则表达式示例：使用<code>findall</code>方法来对<code>fname</code>对象的文件名尝试一个正则表达式。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">r&#x27;(.+)_\d+.jpg$&#x27;</span>, fname.name)<span class="comment">#这个正则表达式提取出所有字符，直到最后一个下划线字符，只要后续字符是数字，然后是 JPEG 文件扩展名。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">&#x27;great_pyrenees&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p><img src="/image/colab_0504.png" alt="文件名正则表达式"></p>
<p>利用<code>RegexLabeller</code>类标记整个数据集，使用了数据块 API，我们在第二章中看到过（实际上，我们几乎总是使用数据块 API——它比我们在第一章中看到的简单工厂方法更灵活）</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pets = DataBlock(blocks = (ImageBlock, CategoryBlock),</span><br><span class="line">                 get_items=get_image_files,</span><br><span class="line">                 splitter=RandomSplitter(seed=<span class="number">42</span>),</span><br><span class="line">                 get_y=using_attr(RegexLabeller(<span class="string">r&#x27;(.+)_\d+.jpg$&#x27;</span>), <span class="string">&#x27;name&#x27;</span>),</span><br><span class="line">                 item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">                 batch_tfms=aug_transforms(size=<span class="number">224</span>, min_scale=<span class="number">0.75</span>))</span><br><span class="line">dls = pets.dataloaders(path/<span class="string">&quot;images&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这个<code>DataBlock</code>调用中一个重要的部分是我们以前没有见过的这两行：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">batch_tfms=aug_transforms(size=<span class="number">224</span>, min_scale=<span class="number">0.75</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/colab_0505.png" alt="使用正则表达式标记数据"></p>
<p>这些行实现了一个我们称之为<em>预调整</em>的 fastai 数据增强策略。预调整是一种特殊的图像增强方法，旨在最大限度地减少数据破坏，同时保持良好的性能。</p>
<h1 id="预调整"><a href="#预调整" class="headerlink" title="预调整"></a>预调整</h1><h4 id="目的："><a href="#目的：" class="headerlink" title="目的："></a>目的：</h4><ol>
<li>我们需要的图像具有相同的尺寸，这样它们可以整合成张量传递给 GPU。</li>
<li>我们还希望最小化我们执行的不同增强计算的数量。性能要求表明，我们应该尽可能将我们的增强变换组合成更少的变换（以减少计算数量和损失操作的数量）。</li>
</ol>
<h4 id="预调整策略："><a href="#预调整策略：" class="headerlink" title="预调整策略："></a>预调整策略：</h4><ol>
<li><p>将图像调整为相对“大”的尺寸，即明显大于目标训练尺寸。</p>
</li>
<li><p>将所有常见的增强操作（包括调整大小到最终目标大小）组合成一个，并在 GPU 上一次性执行组合操作，而不是单独执行操作并多次插值。</p>
</li>
</ol>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li><p>调整大小，创建足够大的图像，使其内部区域有多余的边距，以允许进一步的增强变换而不会产生空白区域<br>这个转换通过调整大小为一个正方形，使用一个大的裁剪尺寸来实现。在训练集上，裁剪区域是随机选择的，裁剪的大小被选择为覆盖图像宽度或高度中较小的那个。</p>
</li>
<li><p>GPU 用于所有数据增强，并且所有潜在破坏性操作都一起完成，最后进行单次插值。</p>
</li>
</ol>
<p><img src="/image/dlcf_0501.png" alt="训练集上的预调整"></p>
<p>这张图片展示了两个步骤：</p>
<ol>
<li><p><em>裁剪全宽或全高</em>：这在<code>item_tfms</code>中，因此它应用于每个单独的图像，然后再复制到 GPU。它用于确保所有图像具有相同的尺寸。在训练集上，裁剪区域是随机选择的。在验证集上，总是选择图像的中心正方形。</p>
</li>
<li><p><em>随机裁剪和增强</em>：这在<code>batch_tfms</code>中，因此它一次在 GPU 上应用于整个批次，这意味着速度快。在验证集上，只有调整大小到模型所需的最终大小。在训练集上，首先进行随机裁剪和任何其他增强。</p>
</li>
</ol>
<p>要在 fastai 中实现此过程，您可以使用<code>Resize</code>作为具有大尺寸的项目转换，以及<code>RandomResizedCrop</code>作为具有较小尺寸的批处理转换。如果在<code>aug_transforms</code>函数中包含<code>min_scale</code>参数，<code>RandomResizedCrop</code>将为您添加，就像在上一节中的<code>DataBlock</code>调用中所做的那样。或者，您可以在初始<code>Resize</code>中使用<code>pad</code>或<code>squish</code>而不是<code>crop</code>（默认值）。</p>
<p>图 5-2 显示了一个图像经过缩放、插值、旋转，然后再次插值（这是所有其他深度学习库使用的方法），显示在右侧，以及一个图像经过缩放和旋转作为一个操作，然后插值一次（fastai 方法），显示在左侧。</p>
<p><img src="/image/dlcf_0502.png" alt=""></p>
<h6 id="图-5-2。fastai-数据增强策略（左）与传统方法（右）的比较"><a href="#图-5-2。fastai-数据增强策略（左）与传统方法（右）的比较" class="headerlink" title="图 5-2。fastai 数据增强策略（左）与传统方法（右）的比较"></a>图 5-2。fastai 数据增强策略（左）与传统方法（右）的比较</h6><p>您可以看到右侧的图像定义不够清晰，在左下角有反射填充伪影；此外，左上角的草完全消失了。我们发现，在实践中，使用预调整显著提高了模型的准确性，通常也会加快速度。</p>
<p>fastai 库还提供了简单的方法来检查您的数据在训练模型之前的外观，这是一个非常重要的步骤。我们将在下一步中看到这些。</p>
<h2 id="检查和调试-DataBlock"><a href="#检查和调试-DataBlock" class="headerlink" title="检查和调试 DataBlock"></a>检查和调试 DataBlock</h2><p>我们永远不能假设我们的代码完美运行。编写<code>DataBlock</code>就像编写蓝图一样。如果您的代码中有语法错误，您将收到错误消息，但是您无法保证您的模板会按照您的意图在数据源上运行。因此，在训练模型之前，您应该始终检查您的数据。</p>
<p>您可以使用<code>show_batch</code>方法来执行此操作：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dls.show_batch(nrows=<span class="number">1</span>, ncols=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_05in01.png" alt=""></p>
<p>检查一下数据吧，确保图像中的狗和标注的品种相对应。（制作数据集的大概率可能会犯一些小错误，哪怕你也是所以检查一下总没错的）。</p>
<p>如果在构建<code>DataBlock</code>时出现错误，您可能在此步骤之前不会看到它。为了调试这个问题，我们鼓励您使用<code>summary</code>方法。它将尝试从您提供的源创建一个批次，并提供大量细节。此外，如果失败，您将准确地看到错误发生的位置，并且库将尝试为您提供一些帮助。例如，一个常见的错误是忘记使用<code>Resize</code>转换，因此最终得到不同大小的图片并且无法将它们整理成批次。在这种情况下，摘要将如下所示（请注意，自撰写时可能已更改确切文本，但它将给您一个概念）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),</span><br><span class="line">                 get_items=get_image_files,</span><br><span class="line">                 splitter=RandomSplitter(seed=<span class="number">42</span>),</span><br><span class="line">                 get_y=using_attr(RegexLabeller(<span class="string">r&#x27;(.+)_\d+.jpg$&#x27;</span>), <span class="string">&#x27;name&#x27;</span>))</span><br><span class="line">pets1.summary(path/<span class="string">&quot;images&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">Setting-up <span class="built_in">type</span> transforms pipelines</span><br><span class="line">Collecting items <span class="keyword">from</span> /home/sgugger/.fastai/data/oxford-iiit-pet/images</span><br><span class="line">Found <span class="number">7390</span> items</span><br><span class="line"><span class="number">2</span> datasets of sizes <span class="number">5912</span>,<span class="number">1478</span></span><br><span class="line">Setting up Pipeline: PILBase.create</span><br><span class="line">Setting up Pipeline: partial -&gt; Categorize</span><br><span class="line"></span><br><span class="line">Building one sample</span><br><span class="line">  Pipeline: PILBase.create</span><br><span class="line">    starting <span class="keyword">from</span></span><br><span class="line">      /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg</span><br><span class="line">    applying PILBase.create gives</span><br><span class="line">      PILImage mode=RGB size=375x500</span><br><span class="line">  Pipeline: partial -&gt; Categorize</span><br><span class="line">    starting <span class="keyword">from</span></span><br><span class="line">      /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg</span><br><span class="line">    applying partial gives</span><br><span class="line">      american_bulldog</span><br><span class="line">    applying Categorize gives</span><br><span class="line">      TensorCategory(<span class="number">12</span>)</span><br><span class="line"></span><br><span class="line">Final sample: (PILImage mode=RGB size=375x500, TensorCategory(<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line">Setting up after_item: Pipeline: ToTensor</span><br><span class="line">Setting up before_batch: Pipeline:</span><br><span class="line">Setting up after_batch: Pipeline: IntToFloatTensor</span><br><span class="line"></span><br><span class="line">Building one batch</span><br><span class="line">Applying item_tfms to the first sample:</span><br><span class="line">  Pipeline: ToTensor</span><br><span class="line">    starting <span class="keyword">from</span></span><br><span class="line">      (PILImage mode=RGB size=375x500, TensorCategory(<span class="number">12</span>))</span><br><span class="line">    applying ToTensor gives</span><br><span class="line">      (TensorImage of size 3x500x375, TensorCategory(<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line">Adding the <span class="built_in">next</span> <span class="number">3</span> samples</span><br><span class="line"></span><br><span class="line">No before_batch transform to apply</span><br><span class="line"></span><br><span class="line">Collating items <span class="keyword">in</span> a batch</span><br><span class="line">Error! It<span class="string">&#x27;s not possible to collate your items in a batch</span></span><br><span class="line"><span class="string">Could not collate the 0-th members of your tuples because got the following</span></span><br><span class="line"><span class="string">shapes:</span></span><br><span class="line"><span class="string">torch.Size([3, 500, 375]),torch.Size([3, 375, 500]),torch.Size([3, 333, 500]),</span></span><br><span class="line"><span class="string">torch.Size([3, 375, 500])</span></span><br></pre></td></tr></table></figure>
<p>您可以看到我们如何收集数据并拆分数据，如何从文件名转换为<em>样本</em>（元组（图像，类别）），然后应用了哪些项目转换以及如何在批处理中无法整理这些样本（因为形状不同）。</p>
<p>一旦您认为数据看起来正确，我们通常建议下一步应该使用它来训练一个简单的模型。我们经常看到人们将实际模型的训练推迟得太久。结果，他们不知道他们的基准结果是什么样的。也许您的问题不需要大量花哨的领域特定工程。或者数据似乎根本无法训练模型。这些都是您希望尽快了解的事情。</p>
<p>对于这个初始测试，我们将使用与第一章中使用的相同简单模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet34, metrics=error_rate)</span><br><span class="line">learn.fine_tune(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>error_rate</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.491732</td>
<td>0.337355</td>
<td>0.108254</td>
<td>00:18</td>
</tr>
<tr>
<td>epoch</td>
<td>train_loss</td>
<td>valid_loss</td>
<td>error_rate</td>
<td>time</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>0</td>
<td>0.503154</td>
<td>0.293404</td>
<td>0.096076</td>
<td>00:23</td>
</tr>
<tr>
<td>1</td>
<td>0.314759</td>
<td>0.225316</td>
<td>0.066306</td>
<td>00:23</td>
</tr>
</tbody>
</table>
</div>
<p>正如我们之前简要讨论过的，当我们拟合模型时显示的表格展示了每个训练周期后的结果。记住，一个周期是对数据中所有图像的完整遍历。显示的列是训练集中项目的平均损失、验证集上的损失，以及我们请求的任何指标——在这种情况下是错误率。</p>
<p>请记住<em>损失</em>是我们决定用来优化模型参数的任何函数。但是我们实际上并没有告诉 fastai 我们想要使用什么损失函数。那么它在做什么呢？fastai 通常会根据您使用的数据和模型类型尝试选择适当的损失函数。在这种情况下，我们有图像数据和分类结果，所以 fastai 会默认使用<em>交叉熵损失</em>。</p>
<h1 id="交叉熵损失"><a href="#交叉熵损失" class="headerlink" title="交叉熵损失"></a>交叉熵损失</h1><p><em>交叉熵损失</em>是一个类似于我们在上一章中使用的损失函数，但是（正如我们将看到的）有两个好处：</p>
<ul>
<li><p>即使我们的因变量有两个以上的类别，它也能正常工作。</p>
</li>
<li><p>这将导致更快速、更可靠的训练。</p>
</li>
</ul>
<p>要理解交叉熵损失如何处理具有两个以上类别的因变量，我们首先必须了解损失函数看到的实际数据和激活是什么样子的。</p>
<h2 id="查看激活和标签"><a href="#查看激活和标签" class="headerlink" title="查看激活和标签"></a>查看激活和标签</h2><p>让我们看看我们模型的激活。要从我们的<code>DataLoaders</code>中获取一批真实数据，我们可以使用<code>one_batch</code>方法：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x,y = dls.one_batch()</span><br></pre></td></tr></table></figure>
<p>正如您所见，这返回了因变量和自变量，作为一个小批量。让我们看看我们的因变量中包含什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">TensorCategory([<span class="number">11</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">5</span>, <span class="number">20</span>,  <span class="number">4</span>, <span class="number">22</span>, <span class="number">31</span>, <span class="number">23</span>, <span class="number">10</span>, <span class="number">20</span>,  <span class="number">2</span>,  <span class="number">3</span>, <span class="number">27</span>, <span class="number">18</span>, <span class="number">23</span>,</span><br><span class="line"> &gt; <span class="number">33</span>,  <span class="number">5</span>, <span class="number">24</span>,  <span class="number">7</span>,  <span class="number">6</span>, <span class="number">12</span>,  <span class="number">9</span>, <span class="number">11</span>, <span class="number">35</span>, <span class="number">14</span>, <span class="number">10</span>, <span class="number">15</span>,  <span class="number">3</span>,  <span class="number">3</span>, <span class="number">21</span>,  <span class="number">5</span>, <span class="number">19</span>, <span class="number">14</span>, <span class="number">12</span>,</span><br><span class="line"> &gt; <span class="number">15</span>, <span class="number">27</span>,  <span class="number">1</span>, <span class="number">17</span>, <span class="number">10</span>,  <span class="number">7</span>,  <span class="number">6</span>, <span class="number">15</span>, <span class="number">23</span>, <span class="number">36</span>,  <span class="number">1</span>, <span class="number">35</span>,  <span class="number">6</span>,</span><br><span class="line">         <span class="number">4</span>, <span class="number">29</span>, <span class="number">24</span>, <span class="number">32</span>,  <span class="number">2</span>, <span class="number">14</span>, <span class="number">26</span>, <span class="number">25</span>, <span class="number">21</span>,  <span class="number">0</span>, <span class="number">29</span>, <span class="number">31</span>, <span class="number">18</span>,  <span class="number">7</span>,  <span class="number">7</span>, <span class="number">17</span>],</span><br><span class="line"> &gt; device=<span class="string">&#x27;cuda:5&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>实际预测是 37 个介于 0 和 1 之间的概率，总和为 1：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(preds[<span class="number">0</span>]),preds[<span class="number">0</span>].<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">37</span>, tensor(<span class="number">1.0000</span>))</span><br></pre></td></tr></table></figure>
<p>为了将我们模型的激活转换为这样的预测，我们使用了一个叫做<em>softmax</em>的激活函数</p>
<h2 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h2><p>在我们的分类模型中，我们在最后一层使用 softmax 激活函数，以确保激活值都在 0 到 1 之间，并且它们总和为 1。</p>
<p>Softmax 类似于我们之前看到的 sigmoid 函数。作为提醒，sigmoid 看起来像这样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(torch.sigmoid, <span class="built_in">min</span>=-<span class="number">4</span>,<span class="built_in">max</span>=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_05in02.png" alt=""></p>
<p>我们可以将这个函数应用于神经网络的一个激活列，并得到一个介于 0 和 1 之间的数字列，因此对于我们的最后一层来说，这是一个非常有用的激活函数。</p>
<p>现在想象一下，如果我们希望目标中有更多类别（比如我们的 37 种宠物品种）。这意味着我们需要比单个列更多的激活：我们需要一个激活<em>每个类别</em>。例如，我们可以创建一个预测 3 和 7 的神经网络，返回两个激活，每个类别一个——这将是创建更一般方法的一个很好的第一步。让我们只是使用一些标准差为 2 的随机数（因此我们将<code>randn</code>乘以 2）作为示例，假设我们有六个图像和两个可能的类别（其中第一列代表 3，第二列代表 7）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">acts = torch.randn((<span class="number">6</span>,<span class="number">2</span>))*<span class="number">2</span></span><br><span class="line">acts</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="number">0.6734</span>,  <span class="number">0.2576</span>],</span><br><span class="line">        [ <span class="number">0.4689</span>,  <span class="number">0.4607</span>],</span><br><span class="line">        [-<span class="number">2.2457</span>, -<span class="number">0.3727</span>],</span><br><span class="line">        [ <span class="number">4.4164</span>, -<span class="number">1.2760</span>],</span><br><span class="line">        [ <span class="number">0.9233</span>,  <span class="number">0.5347</span>],</span><br><span class="line">        [ <span class="number">1.0698</span>,  <span class="number">1.6187</span>]])</span><br></pre></td></tr></table></figure>
<p>我们不能直接对这个进行 sigmoid 运算，因为我们得不到行相加为 1 的结果（我们希望 3 的概率加上 7 的概率等于 1）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">acts.sigmoid()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.6623</span>, <span class="number">0.5641</span>],</span><br><span class="line">        [<span class="number">0.6151</span>, <span class="number">0.6132</span>],</span><br><span class="line">        [<span class="number">0.0957</span>, <span class="number">0.4079</span>],</span><br><span class="line">        [<span class="number">0.9881</span>, <span class="number">0.2182</span>],</span><br><span class="line">        [<span class="number">0.7157</span>, <span class="number">0.6306</span>],</span><br><span class="line">        [<span class="number">0.7446</span>, <span class="number">0.8346</span>]])</span><br></pre></td></tr></table></figure>
<p>在第四章中，我们的神经网络为每个图像创建了一个单一激活，然后通过<code>sigmoid</code>函数传递。这个单一激活代表了模型对输入是 3 的置信度。二进制问题是分类问题的一种特殊情况，因为目标可以被视为单个布尔值，就像我们在<code>mnist_loss</code>中所做的那样。但是二进制问题也可以在任意数量的类别的分类器的更一般上下文中考虑：在这种情况下，我们碰巧有两个类别。正如我们在熊分类器中看到的，我们的神经网络将为每个类别返回一个激活。</p>
<p>那么在二进制情况下，这些激活实际上表示什么？一对激活仅仅表示输入是 3 还是 7 的<em>相对</em>置信度。总体值，无论它们是高还是低，都不重要，重要的是哪个更高，以及高多少。</p>
<p>我们期望，由于这只是表示相同问题的另一种方式，我们应该能够直接在我们的神经网络的两个激活版本上使用<code>sigmoid</code>。事实上我们可以！我们只需取神经网络激活之间的<em>差异</em>，因为这反映了我们对输入是 3 还是 7 更有把握的程度，然后取其 sigmoid：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(acts[:,<span class="number">0</span>]-acts[:,<span class="number">1</span>]).sigmoid()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.6025</span>, <span class="number">0.5021</span>, <span class="number">0.1332</span>, <span class="number">0.9966</span>, <span class="number">0.5959</span>, <span class="number">0.3661</span>])</span><br></pre></td></tr></table></figure>
<p>第二列（它是 7 的概率）将是该值从 1 中减去的值。现在，我们需要一种适用于多于两列的方法。事实证明，这个名为<code>softmax</code>的函数正是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>): <span class="keyword">return</span> exp(x) / exp(x).<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h1 id="术语：指数函数（exp）"><a href="#术语：指数函数（exp）" class="headerlink" title="术语：指数函数（exp）"></a>术语：指数函数（exp）</h1><p>定义为<code>e**x</code>，其中<code>e</code>是一个特殊的数字，约等于 2.718。它是自然对数函数的倒数。请注意，<code>exp</code>始终为正，并且增长<em>非常</em>迅速！</p>
<p>让我们检查<code>softmax</code>是否为第一列返回与<code>sigmoid</code>相同的值，以及这些值从 1 中减去的值为第二列：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sm_acts = torch.softmax(acts, dim=<span class="number">1</span>)</span><br><span class="line">sm_acts</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.6025</span>, <span class="number">0.3975</span>],</span><br><span class="line">        [<span class="number">0.5021</span>, <span class="number">0.4979</span>],</span><br><span class="line">        [<span class="number">0.1332</span>, <span class="number">0.8668</span>],</span><br><span class="line">        [<span class="number">0.9966</span>, <span class="number">0.0034</span>],</span><br><span class="line">        [<span class="number">0.5959</span>, <span class="number">0.4041</span>],</span><br><span class="line">        [<span class="number">0.3661</span>, <span class="number">0.6339</span>]])</span><br></pre></td></tr></table></figure>
<p><code>softmax</code>是<code>sigmoid</code>的多类别等价物——每当我们有超过两个类别且类别的概率必须加起来为 1 时，我们必须使用它，即使只有两个类别，我们通常也会使用它，只是为了使事情更加一致。我们可以创建其他具有所有激活在 0 和 1 之间且总和为 1 的属性的函数；然而，没有其他函数与我们已经看到是平滑且对称的 sigmoid 函数具有相同的关系。此外，我们很快将看到 softmax 函数与我们将在下一节中看到的损失函数密切配合。</p>
<p>如果我们有三个输出激活，就像在我们的熊分类器中一样，为单个熊图像计算 softmax 看起来会像图 5-3 那样。</p>
<p><img src="/image/dlcf_0503.png" alt="熊 softmax 示例"></p>
<h6 id="图-5-3-熊分类器上-softmax-的示例"><a href="#图-5-3-熊分类器上-softmax-的示例" class="headerlink" title="图 5-3. 熊分类器上 softmax 的示例"></a>图 5-3. 熊分类器上 softmax 的示例</h6><p>实际上，这个函数是做什么的呢？取指数确保我们所有的数字都是正数，然后除以总和确保我们将得到一堆加起来等于 1 的数字。指数还有一个很好的特性：如果我们激活中的某个数字略大于其他数字，指数将放大这个差异（因为它呈指数增长），这意味着在 softmax 中，该数字将更接近 1。</p>
<p>直观地，softmax 函数<em>真的</em>想要在其他类别中选择一个类别，因此在我们知道每张图片都有一个明确标签时，训练分类器时是理想的选择。（请注意，在推断过程中可能不太理想，因为有时您可能希望模型告诉您它在训练过程中看到的类别中没有识别出任何一个，并且不选择一个类别，因为它的激活分数略高。在这种情况下，最好使用多个二进制输出列来训练模型，每个列使用 sigmoid 激活。）</p>
<p>Softmax 是交叉熵损失的第一部分，第二部分是对数似然。</p>
<h2 id="对数似然"><a href="#对数似然" class="headerlink" title="对数似然"></a>对数似然</h2><p>在上一章中为我们的 MNIST 示例计算损失时，我们使用了这个：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_loss</span>(<span class="params">inputs, targets</span>):</span><br><span class="line">    inputs = inputs.sigmoid()</span><br><span class="line">    <span class="keyword">return</span> torch.where(targets==<span class="number">1</span>, <span class="number">1</span>-inputs, inputs).mean()</span><br></pre></td></tr></table></figure>
<p>就像我们从 sigmoid 到 softmax 的转变一样，我们需要扩展损失函数，使其能够处理不仅仅是二元分类，还需要能够对任意数量的类别进行分类（在本例中，我们有 37 个类别）。我们的激活，在 softmax 之后，介于 0 和 1 之间，并且对于预测批次中的每一行，总和为 1。我们的目标是介于 0 和 36 之间的整数。</p>
<p>在二元情况下，我们使用<code>torch.where</code>在<code>inputs</code>和<code>1-inputs</code>之间进行选择。当我们将二元分类作为具有两个类别的一般分类问题处理时，它变得更容易，因为（正如我们在前一节中看到的）现在有两列包含等同于<code>inputs</code>和<code>1-inputs</code>的内容。因此，我们只需要从适当的列中进行选择。让我们尝试在 PyTorch 中实现这一点。对于我们合成的 3 和 7 的示例，假设这些是我们的标签：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">targ = tensor([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>这些是 softmax 激活：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sm_acts</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">0.6025</span>, <span class="number">0.3975</span>],</span><br><span class="line">        [<span class="number">0.5021</span>, <span class="number">0.4979</span>],</span><br><span class="line">        [<span class="number">0.1332</span>, <span class="number">0.8668</span>],</span><br><span class="line">        [<span class="number">0.9966</span>, <span class="number">0.0034</span>],</span><br><span class="line">        [<span class="number">0.5959</span>, <span class="number">0.4041</span>],</span><br><span class="line">        [<span class="number">0.3661</span>, <span class="number">0.6339</span>]])</span><br></pre></td></tr></table></figure>
<p>然后对于每个<code>targ</code>项，我们可以使用它来使用张量索引选择<code>sm_acts</code>的适当列，如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">idx = <span class="built_in">range</span>(<span class="number">6</span>)</span><br><span class="line">sm_acts[idx, targ]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.6025</span>, <span class="number">0.4979</span>, <span class="number">0.1332</span>, <span class="number">0.0034</span>, <span class="number">0.4041</span>, <span class="number">0.3661</span>])</span><br></pre></td></tr></table></figure>
<p>为了准确了解这里发生了什么，让我们将所有列放在一起放在一个表中。这里，前两列是我们的激活，然后是目标，行索引，最后是前面代码中显示的结果：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>3</th>
<th>7</th>
<th>targ</th>
<th>idx</th>
<th>loss</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.602469</td>
<td>0.397531</td>
<td>0</td>
<td>0</td>
<td>0.602469</td>
</tr>
<tr>
<td>0.502065</td>
<td>0.497935</td>
<td>1</td>
<td>1</td>
<td>0.497935</td>
</tr>
<tr>
<td>0.133188</td>
<td>0.866811</td>
<td>0</td>
<td>2</td>
<td>0.133188</td>
</tr>
<tr>
<td>0.99664</td>
<td>0.00336017</td>
<td>1</td>
<td>3</td>
<td>0.00336017</td>
</tr>
<tr>
<td>0.595949</td>
<td>0.404051</td>
<td>1</td>
<td>4</td>
<td>0.404051</td>
</tr>
<tr>
<td>0.366118</td>
<td>0.633882</td>
<td>0</td>
<td>5</td>
<td>0.366118</td>
</tr>
</tbody>
</table>
</div>
<p>从这个表中可以看出，最后一列可以通过将<code>targ</code>和<code>idx</code>列作为索引，指向包含<code>3</code>和<code>7</code>列的两列矩阵来计算。这就是<code>sm_acts[idx, targ]</code>的作用。</p>
<p>这里真正有趣的是，这种方法同样适用于超过两列的情况。想象一下，如果我们为每个数字（0 到 9）添加一个激活列，然后<code>targ</code>包含从 0 到 9 的数字。只要激活列总和为 1（如果我们使用 softmax，它们将是这样），我们将有一个损失函数，显示我们预测每个数字的准确程度。</p>
<p>我们只从包含正确标签的列中选择损失。我们不需要考虑其他列，因为根据 softmax 的定义，它们加起来等于 1 减去与正确标签对应的激活。因此，使正确标签的激活尽可能高必须意味着我们也在降低其余列的激活。</p>
<p>PyTorch 提供了一个与<code>sm_acts[range(n), targ]</code>完全相同的函数（除了它取负数，因为之后应用对数时，我们将得到负数），称为<code>nll_loss</code>（<em>NLL</em>代表<em>负对数似然</em>）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-sm_acts[idx, targ]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.6025</span>, -<span class="number">0.4979</span>, -<span class="number">0.1332</span>, -<span class="number">0.0034</span>, -<span class="number">0.4041</span>, -<span class="number">0.3661</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.nll_loss(sm_acts, targ, reduction=<span class="string">&#x27;none&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.6025</span>, -<span class="number">0.4979</span>, -<span class="number">0.1332</span>, -<span class="number">0.0034</span>, -<span class="number">0.4041</span>, -<span class="number">0.3661</span>])</span><br></pre></td></tr></table></figure>
<p>尽管它的名字是这样的，但这个 PyTorch 函数并不取对数。我们将在下一节看到原因，但首先，让我们看看为什么取对数会有用。</p>
<h2 id="取对数"><a href="#取对数" class="headerlink" title="取对数"></a>取对数</h2><p>在前一节中我们看到的函数作为损失函数效果很好，但我们可以让它更好一些。问题在于我们使用的是概率，概率不能小于 0 或大于 1。这意味着我们的模型不会在乎它是预测 0.99 还是 0.999。确实，这些数字非常接近，但从另一个角度来看，0.999 比 0.99 自信程度高 10 倍。因此，我们希望将我们的数字从 0 到 1 转换为从负无穷到无穷。有一个数学函数可以做到这一点：<em>对数</em>（可用<code>torch.log</code>）。它对小于 0 的数字没有定义，并且如下所示：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(torch.log, <span class="built_in">min</span>=<span class="number">0</span>,<span class="built_in">max</span>=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_05in03.png" alt=""></p>
<p>“对数”这个词让你想起了什么吗？对数函数有这个恒等式：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = b**a</span><br><span class="line">a = log(y,b)</span><br></pre></td></tr></table></figure>
<p>在这种情况下，我们假设<code>log(y,b)</code>返回<em>log y 以 b 为底</em>。然而，PyTorch 并没有这样定义<code>log</code>：Python 中的<code>log</code>使用特殊数字<code>e</code>（2.718…）作为底。</p>
<p>也许对数是您在过去 20 年中没有考虑过的东西。但对于深度学习中的许多事情来说，对数是一个非常关键的数学概念，所以现在是一个很好的时机来刷新您的记忆。关于对数的关键事情是这样的关系：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log(a*b) = log(a)+log(b)</span><br></pre></td></tr></table></figure>
<p>当我们以这种格式看到它时，它看起来有点无聊；但想想这实际上意味着什么。这意味着当基础信号呈指数或乘法增长时，对数会线性增加。例如，在地震严重程度的里氏震级和噪音级别的分贝尺中使用。它也经常用于金融图表中，我们希望更清楚地显示复合增长率。计算机科学家喜欢使用对数，因为这意味着可以用加法代替修改，这样可以避免产生计算机难以处理的难以处理的规模。</p>
<h1 id="TIP"><a href="#TIP" class="headerlink" title="TIP"></a>TIP</h1><p>对我们的概率取正对数或负对数的平均值（取决于是否是正确或不正确的类）给出了<em>负对数似然</em>损失。在 PyTorch 中，<code>nll_loss</code>假设您已经对 softmax 取了对数，因此不会为您执行对数运算。</p>
<h1 id="请注意一下函数名称"><a href="#请注意一下函数名称" class="headerlink" title="请注意一下函数名称"></a>请注意一下函数名称</h1><p><code>nll_loss</code>中的“nll”代表“负对数似然”，但实际上它根本不进行对数运算！它假设您已经<em>已经</em>进行了对数运算。PyTorch 有一个名为<code>log_softmax</code>的函数，以快速准确的方式结合了<code>log</code>和<code>softmax</code>。<code>nll_loss</code>设计用于在<code>log_softmax</code>之后使用。</p>
<p>当我们首先进行 softmax，然后对其进行对数似然，这种组合被称为<em>交叉熵损失</em>。在 PyTorch 中，这可以通过<code>nn.CrossEntropyLoss</code>来实现（实际上执行<code>log_softmax</code>然后<code>nll_loss</code>）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_func = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<p>正如您所看到的，这是一个类。实例化它会给您一个像函数一样行为的对象：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss_func(acts, targ)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.8045</span>)</span><br></pre></td></tr></table></figure>
<p>所有 PyTorch 损失函数都以两种形式提供，刚刚显示的类形式以及在<code>F</code>命名空间中提供的普通函数形式：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.cross_entropy(acts, targ)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">1.8045</span>)</span><br></pre></td></tr></table></figure>
<p>两者都可以正常工作，并且可以在任何情况下使用。我们注意到大多数人倾向于使用类版本，并且在 PyTorch 的官方文档和示例中更常见，因此我们也会倾向于使用它。</p>
<p>默认情况下，PyTorch 损失函数取所有项目的损失的平均值。您可以使用<code>reduction=&#39;none&#39;</code>来禁用这一点：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.CrossEntropyLoss(reduction=<span class="string">&#x27;none&#x27;</span>)(acts, targ)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.5067</span>, <span class="number">0.6973</span>, <span class="number">2.0160</span>, <span class="number">5.6958</span>, <span class="number">0.9062</span>, <span class="number">1.0048</span>])</span><br></pre></td></tr></table></figure>
<h1 id="模型解释"><a href="#模型解释" class="headerlink" title="模型解释"></a>模型解释</h1><p>直接解释损失函数非常困难，因为它们被设计为计算机可以区分和优化的东西，而不是人类可以理解的东西。这就是为什么我们有指标。这些指标不用于优化过程，而只是帮助我们这些可怜的人类理解发生了什么。在这种情况下，我们的准确率已经看起来相当不错！那么我们在哪里犯了错误呢？</p>
<p>我们在第一章中看到，我们可以使用混淆矩阵来查看模型表现好和表现不佳的地方：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">interp = ClassificationInterpretation.from_learner(learn)</span><br><span class="line">interp.plot_confusion_matrix(figsize=(<span class="number">12</span>,<span class="number">12</span>), dpi=<span class="number">60</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_05in04.png" alt=""></p>
<p>哦，亲爱的——在这种情况下，混淆矩阵很难阅读。我们有 37 种宠物品种，这意味着在这个巨大矩阵中有 37×37 个条目！相反，我们可以使用<code>most_confused</code>方法，它只显示混淆矩阵中预测错误最多的单元格（这里至少有 5 个或更多）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">interp.most_confused(min_val=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[(<span class="string">&#x27;american_pit_bull_terrier&#x27;</span>, <span class="string">&#x27;staffordshire_bull_terrier&#x27;</span>, <span class="number">10</span>),</span><br><span class="line"> (<span class="string">&#x27;Ragdoll&#x27;</span>, <span class="string">&#x27;Birman&#x27;</span>, <span class="number">6</span>)]</span><br></pre></td></tr></table></figure>
<p>由于我们不是宠物品种专家，很难知道这些类别错误是否反映了识别品种时的实际困难。因此，我们再次求助于谷歌。一点点搜索告诉我们，这里显示的最常见的类别错误是即使是专家育种者有时也会对其存在分歧的品种差异。因此，这让我们有些安慰，我们正在走在正确的道路上。</p>
<p>我们似乎有一个良好的基线。现在我们可以做些什么来使它变得更好呢？</p>
<h1 id="改进模型"><a href="#改进模型" class="headerlink" title="改进模型"></a>改进模型</h1><p>解释迁移学习以及如何尽可能最好地微调我们的预训练模型，而不破坏预训练权重。</p>
<p>重点在于设置学习率（在fastai中有一个高效快捷的工具）</p>
<h1 id="学习率查找器"><a href="#学习率查找器" class="headerlink" title="学习率查找器"></a>学习率查找器</h1><p>学习率过高现象：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet34, metrics=error_rate)</span><br><span class="line">learn.fine_tune(<span class="number">1</span>, base_lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>error_rate</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>8.946717</td>
<td>47.954632</td>
<td>0.893775</td>
<td>00:20</td>
</tr>
<tr>
<td>epoch</td>
<td>train_loss</td>
<td>valid_loss</td>
<td>error_rate</td>
<td>time</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>0</td>
<td>7.231843</td>
<td>4.119265</td>
<td>0.954668</td>
<td>00:24</td>
</tr>
</tbody>
</table>
</div>
<h2 id="现在问题来了怎么找到合适的学习率？"><a href="#现在问题来了怎么找到合适的学习率？" class="headerlink" title="现在问题来了怎么找到合适的学习率？"></a>现在问题来了怎么找到合适的学习率？</h2><p>快使用<em>学习率查找器</em>。他的想法是从一个非常非常小的学习率开始，一个我们永远不会认为它太大而无法处理的学习率。我们用这个学习率进行一个 mini-batch，找到之后的损失，然后按一定百分比增加学习率（例如每次加倍）。然后我们再做另一个 mini-batch，跟踪损失，并再次加倍学习率。我们一直这样做，直到损失变得更糟没有变得更好。然后我们按照要求选择一个比这个点稍低的学习率：</p>
<ul>
<li><p>比最小损失达到的地方少一个数量级（即最小值除以 10）</p>
</li>
<li><p>最后一次损失明显减少的点</p>
</li>
</ul>
<p>学习率查找器计算曲线上的这些点来帮助您。这两个规则通常给出大致相同的值。在第一章中，我们没有指定学习率，而是使用了 fastai 库的默认值（即 1e-3）</p>
<p>关于mini-batch：我们已知在梯度下降中需要对所有样本进行处理过后然后走一步，那么如果我们的样本规模的特别大的话效率就会比较低。假如有500万，甚至5000万个样本(在我们的业务场景中，一般有几千万行，有些大数据有10亿行)的话走一轮迭代就会非常的耗时。这个时候的梯度下降叫做full batch。 所以为了提高效率，我们可以把样本分成等量的子集。 例如我们把100万样本分成1000份， 每份1000个样本， 这些子集就称为mini batch。然后我们分别用一个for循环遍历这1000个子集。 针对每一个子集做一次梯度下降。 然后更新参数w和b的值。接着到下一个子集中继续进行梯度下降。 这样在遍历完所有的mini batch之后我们相当于在梯度下降中做了1000次迭代。 我们将遍历一次所有样本的行为叫做一个 epoch，也就是一个世代。 在mini batch下的梯度下降中做的事情其实跟full batch一样，只不过我们训练的数据不再是所有的样本，而是一个个的子集。 这样在mini batch我们在一个epoch中就能进行1000次的梯度下降，而在full batch中只有一次。 这样就大大的提高了我们算法的运行速度。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet34, metrics=error_rate)</span><br><span class="line">lr_min,lr_steep = learn.lr_find()</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_05in05.png" alt=""></p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Minimum/10: <span class="subst">&#123;lr_min:<span class="number">.2</span>e&#125;</span>, steepest point: <span class="subst">&#123;lr_steep:<span class="number">.2</span>e&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Minimum/<span class="number">10</span>: <span class="number">8.32e-03</span>, steepest point: <span class="number">6.31e-03</span></span><br></pre></td></tr></table></figure>
<p>我们可以看到在 1e-6 到 1e-3 的范围内，没有什么特别的事情发生，模型不会训练。然后损失开始减少，直到达到最小值，然后再次增加。我们不希望学习率大于 1e-1，因为这会导致训练发散（您可以自行尝试），但 1e-1 已经太高了：在这个阶段，我们已经离开了损失稳定下降的阶段。</p>
<p>在这个学习率图中，看起来学习率约为 3e-3 可能是合适的，所以让我们选择这个：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = cnn_learner(dls, resnet34, metrics=error_rate)</span><br><span class="line">learn.fine_tune(<span class="number">2</span>, base_lr=<span class="number">3e-3</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>error_rate</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>1.071820</td>
<td>0.427476</td>
<td>0.133965</td>
<td>00:19</td>
</tr>
<tr>
<td>epoch</td>
<td>train_loss</td>
<td>valid_loss</td>
<td>error_rate</td>
<td>time</td>
</tr>
<tr>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
<td>—-</td>
</tr>
<tr>
<td>0</td>
<td>0.738273</td>
<td>0.541828</td>
<td>0.150880</td>
<td>00:24</td>
</tr>
<tr>
<td>1</td>
<td>0.401544</td>
<td>0.266623</td>
<td>0.081867</td>
<td>00:24</td>
</tr>
</tbody>
</table>
</div>
<h1 id="问卷调查"><a href="#问卷调查" class="headerlink" title="问卷调查"></a>问卷调查</h1><ol>
<li>为什么我们首先在 CPU 上调整大小到较大尺寸，然后在 GPU 上调整到较小尺寸？</li>
</ol>
<ul>
<li>先在CPU上放大以高效处理大尺寸插值，再在GPU上缩小以利用并行加速动态增强并节省显存。</li>
</ul>
<ol>
<li>如果您不熟悉正则表达式，请查找正则表达式教程和一些问题集，并完成它们。查看书籍网站以获取建议。</li>
</ol>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.runoob.com/regexp/regexp-syntax.html">一个可以将文件名字利用起来的程序</a></li>
</ul>
<ol>
<li>对于大多数深度学习数据集，数据通常以哪两种方式提供？</li>
</ol>
<ul>
<li><p>表示数据项的个别文件，例如文本文档或图像，可能组织成文件夹或具有表示有关这些项信息的文件名</p>
</li>
<li><p>数据表（例如，以 CSV 格式）中的数据，其中每行是一个项目，可能包括文件名，提供表中数据与其他格式（如文本文档和图像）中数据之间的连接</p>
</li>
</ul>
<p>有一些例外情况——特别是在基因组学等领域，可能存在二进制数据库格式或甚至网络流——但总体而言，您将处理的绝大多数数据集将使用这两种格式的某种组合。</p>
<ol>
<li>查阅<code>L</code>的文档，并尝试使用它添加的一些新方法。查阅 Python <code>pathlib</code>模块的文档，并尝试使用<code>Path</code>类的几种方法。</li>
</ol>
<ul>
<li>如何找类和模块的文档：问ai最高效<a target="_blank" rel="noopener" href="https://docs.python.org/zh-cn/3.13/library/pathlib.html">pathlib</a>,L文档本地查看：使用 help() 函数help(L)查看文档，print(dir(L))查看所有类和对象。</li>
</ul>
<ol>
<li>给出两个图像转换可能降低数据质量的示例。</li>
</ol>
<ul>
<li>许多旋转和缩放操作将需要插值来创建像素。这些插值像素是从原始图像数据派生的，但质量较低。</li>
</ul>
<ol>
<li>fastai 提供了哪种方法来查看<code>DataLoaders</code>中的数据？</li>
</ol>
<ul>
<li>‘one_batch’的方法</li>
</ul>
<ol>
<li>fastai 提供了哪种方法来帮助您调试<code>DataBlock</code>？</li>
</ol>
<ul>
<li><p>现在我们确认了正则表达式对示例的有效性，让我们用它来标记整个数据集。fastai 提供了许多类来帮助标记。对于使用正则表达式进行标记，我们可以使用<code>RegexLabeller</code>类。在这个例子中，我们使用了数据块 API，我们在第二章中看到过（实际上，我们几乎总是使用数据块 API——它比我们在第一章中看到的简单工厂方法更灵活）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">pets = DataBlock(blocks = (ImageBlock, CategoryBlock),</span><br><span class="line">                get_items=get_image_files,</span><br><span class="line">                splitter=RandomSplitter(seed=<span class="number">42</span>),</span><br><span class="line">                get_y=using_attr(RegexLabeller(<span class="string">r&#x27;(.+)_\d+.jpg$&#x27;</span>), <span class="string">&#x27;name&#x27;</span>),</span><br><span class="line">                item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">                batch_tfms=aug_transforms(size=<span class="number">224</span>, min_scale=<span class="number">0.75</span>))</span><br><span class="line">dls = pets.dataloaders(path/<span class="string">&quot;images&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>这个<code>DataBlock</code>调用中一个重要的部分是我们以前没有见过的这两行：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">item_tfms=Resize(<span class="number">460</span>),</span><br><span class="line">batch_tfms=aug_transforms(size=<span class="number">224</span>, min_scale=<span class="number">0.75</span>)</span><br></pre></td></tr></table></figure>
<p>这些行实现了一个我们称之为<em>预调整</em>的 fastai 数据增强策略。预调整是一种特殊的图像增强方法，旨在最大限度地减少数据破坏，同时保持良好的性能。</p>
</li>
</ul>
<ol>
<li>在彻底清理数据之前，是否应该暂停训练模型？</li>
</ol>
<ul>
<li>必要的</li>
</ul>
<ol>
<li>在 PyTorch 中，交叉熵损失是由哪两个部分组合而成的？</li>
</ol>
<ul>
<li>Softmax 是交叉熵损失的第一部分，第二部分是对数似然。</li>
</ul>
<ol>
<li>softmax 确保的激活函数的两个属性是什么？为什么这很重要？</li>
</ol>
<ul>
<li>以确保激活值都在 0 到 1 之间，并且它们总和为 1。</li>
</ul>
<ol>
<li>何时可能希望激活函数不具有这两个属性？</li>
</ol>
<ul>
<li>非二元情况下</li>
</ul>
<ol>
<li>自己计算图 5-3 中的<code>exp</code>和<code>softmax</code>列（即在电子表格、计算器或笔记本中）。</li>
</ol>
<ul>
<li>exp = exp(output),softmax = exp(output)/count(exp)</li>
</ul>
<ol>
<li>为什么我们不能使用<code>torch.where</code>为标签可能有多于两个类别的数据集创建损失函数？</li>
</ol>
<ul>
<li>我们使用<code>torch.where</code>在<code>inputs</code>和<code>1-inputs</code>之间进行选择</li>
</ul>
<ol>
<li>log（-2）的值是多少？为什么？</li>
</ol>
<ul>
<li>不存在吧 这玩意 哪有log负数这玩意的</li>
</ul>
<ol>
<li>选择学习率时有哪两个好的经验法则来自学习率查找器？</li>
</ol>
<ul>
<li>从一个非常非常小的学习率开始，一个我们永远不会认为它太大而无法处理的学习率。我们用这个学习率进行一个 mini-batch，找到之后的损失，然后按一定百分比增加学习率（例如每次加倍）。然后我们再做另一个 mini-batch，跟踪损失，并再次加倍学习率。</li>
</ul>
<ol>
<li><code>fine_tune</code>方法执行了哪两个步骤？</li>
</ol>
<ul>
<li><p>训练随机添加的层一个周期，同时冻结所有其他层</p>
</li>
<li><p>解冻所有层，并根据请求的周期数进行训练</p>
</li>
</ul>
<ol>
<li>在 Jupyter Notebook 中，如何获取方法或函数的源代码？</li>
</ol>
<ul>
<li>在 Jupyter Notebook 中，使用 ?? 符号（如 函数名??）或 inspect.getsource(函数名) 可直接查看方法或函数的源代码。</li>
</ul>
<ol>
<li>什么是区分性学习率？</li>
</ol>
<ul>
<li>对神经网络的早期层使用较低的学习率，对后期层（尤其是随机添加的层）使用较高的学习率。</li>
</ul>
<ol>
<li>当将 Python <code>slice</code>对象作为学习率传递给 fastai 时，它是如何解释的？</li>
</ol>
<ul>
<li>传递的第一个值将是神经网络最早层的学习率，第二个值将是最后一层的学习率。中间的层将在该范围内等距地乘法地具有学习率。让我们使用这种方法复制先前的训练，但这次我们只将我们网络的<em>最低</em>层的学习率设置为 1e-6；其他层将增加到 1e-4。让我们训练一段时间。</li>
</ul>
<ol>
<li>为什么在使用 1cycle 训练时，提前停止是一个不好的选择？</li>
</ol>
<ul>
<li>因为那些中间的 epochs 出现在学习率还没有机会达到小值的情况下，这时它才能真正找到最佳结果。因此，如果你发现你过拟合了，你应该重新从头开始训练模型，并根据之前找到最佳结果的地方选择一个总的 epochs 数量。</li>
</ul>
<ol>
<li><code>resnet50</code>和<code>resnet101</code>之间有什么区别？</li>
</ol>
<ul>
<li>架构往往只有少数几种变体。例如，在本章中使用的 ResNet 架构有 18、34、50、101 和 152 层的变体，都是在 ImageNet 上预训练的。一个更大的（更多层和参数；有时被描述为模型的<em>容量</em>）ResNet 版本总是能够给我们更好的训练损失，但它可能更容易过拟合，因为它有更多参数可以过拟合。</li>
</ul>
<ol>
<li><code>to_fp16</code>是做什么的？</li>
</ol>
<ul>
<li>将训练的数据精度降低提高训练速度。（半精度浮点数，也称为 fp16）</li>
</ul>
<h2 id="进一步研究"><a href="#进一步研究" class="headerlink" title="进一步研究"></a>进一步研究</h2><ol>
<li><p>找到 Leslie Smith 撰写的介绍学习率查找器的论文，并阅读。</p>
</li>
<li><p>看看是否可以提高本章分类器的准确性。您能达到的最佳准确性是多少？查看论坛和书籍网站，看看其他学生在这个数据集上取得了什么成就以及他们是如何做到的。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/05/12/AboutConda/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/05/12/AboutConda/" class="post-title-link" itemprop="url">AboutConda</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-05-12 14:47:11 / 修改时间：15:22:00" itemprop="dateCreated datePublished" datetime="2025-05-12T14:47:11+08:00">2025-05-12</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="这里会记录一些常用的conda指令"><a href="#这里会记录一些常用的conda指令" class="headerlink" title="这里会记录一些常用的conda指令"></a>这里会记录一些常用的conda指令</h1><h4 id="首先拿到一台陌生的电脑需要判断电脑上有没有安装conda，conda版本又是多少"><a href="#首先拿到一台陌生的电脑需要判断电脑上有没有安装conda，conda版本又是多少" class="headerlink" title="首先拿到一台陌生的电脑需要判断电脑上有没有安装conda，conda版本又是多少"></a>首先拿到一台陌生的电脑需要判断电脑上有没有安装conda，conda版本又是多少</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda --version</span><br></pre></td></tr></table></figure>
<h4 id="如果遇到下载慢的问题就需要设置镜像解决"><a href="#如果遇到下载慢的问题就需要设置镜像解决" class="headerlink" title="如果遇到下载慢的问题就需要设置镜像解决"></a>如果遇到下载慢的问题就需要设置镜像解决</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置清华镜像</span></span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span><br><span class="line">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/</span><br><span class="line"><span class="comment">#设置bioconda</span></span><br><span class="line">conda config --add channels bioconda</span><br><span class="line">conda config --add channels conda-forge</span><br><span class="line"><span class="comment">#设置搜索时显示通道地址</span></span><br><span class="line">conda config --<span class="built_in">set</span> show_channel_urls <span class="built_in">yes</span></span><br></pre></td></tr></table></figure>
<p>tips:事实上之前我查询清华源已经把研究院的ip禁掉了因为有人频繁下载QT</p>
<h4 id="准备好就可以创建虚拟环境了"><a href="#准备好就可以创建虚拟环境了" class="headerlink" title="准备好就可以创建虚拟环境了"></a>准备好就可以创建虚拟环境了</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n env_name python=3.8</span><br></pre></td></tr></table></figure>
<h4 id="激活虚拟环境"><a href="#激活虚拟环境" class="headerlink" title="激活虚拟环境"></a>激活虚拟环境</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate env_name</span><br></pre></td></tr></table></figure>
<h4 id="退出虚拟环境"><a href="#退出虚拟环境" class="headerlink" title="退出虚拟环境"></a>退出虚拟环境</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>
<h5 id="删除虚拟环境"><a href="#删除虚拟环境" class="headerlink" title="删除虚拟环境"></a>删除虚拟环境</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove --name env_name --all</span><br></pre></td></tr></table></figure>
<h4 id="删除环境中的某个包"><a href="#删除环境中的某个包" class="headerlink" title="删除环境中的某个包"></a>删除环境中的某个包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove --name env_name  package_name</span><br></pre></td></tr></table></figure>
<h4 id="查看当前拥有的包"><a href="#查看当前拥有的包" class="headerlink" title="查看当前拥有的包"></a>查看当前拥有的包</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br></pre></td></tr></table></figure>
<h4 id="安装包的方式"><a href="#安装包的方式" class="headerlink" title="安装包的方式"></a>安装包的方式</h4><p>根据我的使用经验可以使用conda安装也可以安装pip进行安装所以总结一下安装的命令</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install package_name</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install package_name</span><br></pre></td></tr></table></figure>
<p>卸载的话就是相应的uninstall</p>
<p><font color = orange>特别说明:</font> pip和conda在安装软件包时，在依赖关系方面的处理机制不同。pip在递归的串行循环中安装依赖项，不会确保同时满足所有软件包的依赖关系，如果按顺序较早安装的软件包相对于按顺序较晚安装的软件包具有不兼容的依赖项版本，则可能导致环境以微妙的方式被破坏掉；conda使用SAT（satisfiability）solver来验证是否满足环境中安装的所有软件包的所有要求，只要有关依赖项的软件包元数据正确，conda就会按预期产生可用的环境。</p>
<p><font color = red >关于混用问题:</font>不建议混用，不然容易出现错误。</p>
<p><font color = geern>关于包的位置:</font></p>
<ul>
<li>conda install xxx：这种方式安装的库都会放在anaconda3/pkgs目录下，这样的好处就是，当在某个环境下已经下载好了某个库，再在另一个环境中还需要这个库时，就可以直接从pkgs目录下将该库复制至新环境而不用重复下载。</li>
<li>pip install xxx：分两种情况，一种情况就是当前conda环境的python是conda安装的，和系统的不一样，那么xxx会被安装到anaconda3/envs/current_env/lib/python3.x/site-packages文件夹中，如果当前conda环境用的是系统的python，那么xxx会通常会被安装到~/.local/lib/python3.x/site-packages文件夹中 </li>
</ul>
<h4 id="清理-conda-缓存"><a href="#清理-conda-缓存" class="headerlink" title="清理 conda 缓存"></a>清理 conda 缓存</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda clean -h</span><br></pre></td></tr></table></figure>
<p>查看清理的具体细节</p>
<h4 id="变更python版本"><a href="#变更python版本" class="headerlink" title="变更python版本"></a>变更python版本</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install python=3.5</span><br></pre></td></tr></table></figure>
<h4 id="更新python版本到最新"><a href="#更新python版本到最新" class="headerlink" title="更新python版本到最新"></a>更新python版本到最新</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda update python</span><br></pre></td></tr></table></figure>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/04/28/fastaichapter4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Wang Song">
      <meta itemprop="description" content="a graduate student working at Huzhou institute of Zhejiang University">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="WangSong's blog">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2025/04/28/fastaichapter4/" class="post-title-link" itemprop="url">Fastai Chapter 4</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2025-04-28 09:30:20" itemprop="dateCreated datePublished" datetime="2025-04-28T09:30:20+08:00">2025-04-28</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2025-05-12 16:31:16" itemprop="dateModified" datetime="2025-05-12T16:31:16+08:00">2025-05-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="第四章：底层：训练数字分类器"><a href="#第四章：底层：训练数字分类器" class="headerlink" title="第四章：底层：训练数字分类器"></a>第四章：底层：训练数字分类器</h1><p>在第二章中看到训练各种模型的样子后，现在让我们深入了解并看看究竟发生了什么。我们将使用计算机视觉来介绍深度学习的基本工具和概念。</p>
<p>确切地说，我们将讨论数组和张量的作用以及广播的作用，这是一种使用它们表达性地的强大技术。我们将解释随机梯度下降（SGD），这是通过自动更新权重学习的机制。我们将讨论基本分类任务的损失函数的选择，以及小批量的作用。我们还将描述基本神经网络正在执行的数学。最后，我们将把所有这些部分组合起来。</p>
<p>在未来的章节中，我们还将深入研究其他应用，并看看这些概念和工具如何泛化。但本章是关于奠定基础的。坦率地说，这也使得这是最困难的章节之一，因为这些概念彼此相互依赖。就像一个拱门，所有的石头都需要放在正确的位置才能支撑结构。也像一个拱门，一旦发生这种情况，它就是一个强大的结构，可以支撑其他事物。但是需要一些耐心来组装。</p>
<p>让我们开始吧。第一步是考虑图像在计算机中是如何表示的。</p>
<h1 id="像素：计算机视觉的基础"><a href="#像素：计算机视觉的基础" class="headerlink" title="像素：计算机视觉的基础"></a>像素：计算机视觉的基础</h1><p>要理解计算机视觉模型中发生的事情，我们首先必须了解计算机如何处理图像。我们将使用计算机视觉中最著名的数据集之一 MNIST 进行实验。MNIST 包含由国家标准与技术研究所收集的手写数字图像，并由 Yann Lecun 及其同事整理成一个机器学习数据集。Lecun 在 1998 年使用 MNIST 在 LeNet-5 中，这是第一个演示实用手写数字序列识别的计算机系统。这是人工智能历史上最重要的突破之一。</p>
<p>对于这个初始教程，我们只是尝试创建一个模型，可以将任何图像分类为 3 或 7。所以让我们下载一个包含这些数字图像的 MNIST 样本：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path = untar_data(URLs.MNIST_SAMPLE)</span><br></pre></td></tr></table></figure>
<p>我们可以使用<code>ls</code>来查看此目录中的内容，这是 fastai 添加的一个方法。这个方法返回一个特殊的 fastai 类<code>L</code>的对象，它具有 Python 内置<code>list</code>的所有功能，还有更多功能。其中一个方便的功能是，在打印时，它会显示项目的计数，然后列出项目本身（如果项目超过 10 个，它只显示前几个）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">path.ls()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#9) [Path(&#x27;cleaned.csv&#x27;),Path(&#x27;item_list.txt&#x27;),Path(&#x27;trained_model.pkl&#x27;),Path(&#x27;</span></span><br><span class="line"> &gt; models<span class="string">&#x27;),Path(&#x27;</span>valid<span class="string">&#x27;),Path(&#x27;</span>labels.csv<span class="string">&#x27;),Path(&#x27;</span>export.pkl<span class="string">&#x27;),Path(&#x27;</span>history.cs</span><br><span class="line"> &gt; v<span class="string">&#x27;),Path(&#x27;</span>train<span class="string">&#x27;)]</span></span><br></pre></td></tr></table></figure>
<p>MNIST 数据集遵循机器学习数据集的常见布局：训练集和验证（和/或测试）集分开存放。让我们看看训练集中的内容：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(path/<span class="string">&#x27;train&#x27;</span>).ls()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#2) [Path(&#x27;train/7&#x27;),Path(&#x27;train/3&#x27;)]</span></span><br></pre></td></tr></table></figure>
<p>有一个包含 3 的文件夹，和一个包含 7 的文件夹。在机器学习术语中，我们说“3”和“7”是这个数据集中的<em>标签</em>（或目标）。让我们看看其中一个文件夹中的内容（使用<code>sorted</code>确保我们都得到相同的文件顺序）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">threes = (path/<span class="string">&#x27;train&#x27;</span>/<span class="string">&#x27;3&#x27;</span>).ls().<span class="built_in">sorted</span>()</span><br><span class="line">sevens = (path/<span class="string">&#x27;train&#x27;</span>/<span class="string">&#x27;7&#x27;</span>).ls().<span class="built_in">sorted</span>()</span><br><span class="line">threes</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#6131) [Path(&#x27;train/3/10.png&#x27;),Path(&#x27;train/3/10000.png&#x27;),Path(&#x27;train/3/10011.pn</span></span><br><span class="line"> &gt; g<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10031.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10034.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10042.</span>p</span><br><span class="line"> &gt; ng<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10052.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">1007.</span>png<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10074.</span>p</span><br><span class="line"> &gt; ng<span class="string">&#x27;),Path(&#x27;</span>train/<span class="number">3</span>/<span class="number">10091.</span>png<span class="string">&#x27;)...]</span></span><br></pre></td></tr></table></figure>
<p>正如我们所预期的那样，它充满了图像文件。让我们现在看一个。这是一个手写数字 3 的图像，来自著名的手写数字 MNIST 数据集：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im3_path = threes[<span class="number">1</span>]</span><br><span class="line">im3 = Image.<span class="built_in">open</span>(im3_path)</span><br><span class="line">im3</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in01.png" alt=""></p>
<p>在这里，我们使用<em>Python Imaging Library</em>（PIL）中的<code>Image</code>类，这是最广泛使用的 Python 包，用于打开、操作和查看图像。Jupyter 知道 PIL 图像，所以它会自动为我们显示图像。</p>
<p>在计算机中，一切都以数字表示。要查看构成这幅图像的数字，我们必须将其转换为<em>NumPy 数组</em>或<em>PyTorch 张量</em>。例如，这是转换为 NumPy 数组后图像的一部分的样子：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array(im3)[<span class="number">4</span>:<span class="number">10</span>,<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">array([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">       [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">29</span>],</span><br><span class="line">       [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">48</span>, <span class="number">166</span>, <span class="number">224</span>],</span><br><span class="line">       [  <span class="number">0</span>,  <span class="number">93</span>, <span class="number">244</span>, <span class="number">249</span>, <span class="number">253</span>, <span class="number">187</span>],</span><br><span class="line">       [  <span class="number">0</span>, <span class="number">107</span>, <span class="number">253</span>, <span class="number">253</span>, <span class="number">230</span>,  <span class="number">48</span>],</span><br><span class="line">       [  <span class="number">0</span>,   <span class="number">3</span>,  <span class="number">20</span>,  <span class="number">20</span>,  <span class="number">15</span>,   <span class="number">0</span>]], dtype=uint8)</span><br></pre></td></tr></table></figure>
<p><code>4:10</code>表示我们请求从索引 4（包括）到 10（不包括）的行，列也是一样。NumPy 从上到下，从左到右索引，因此此部分位于图像的左上角附近。这里是一个 PyTorch 张量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(im3)[<span class="number">4</span>:<span class="number">10</span>,<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">29</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">48</span>, <span class="number">166</span>, <span class="number">224</span>],</span><br><span class="line">        [  <span class="number">0</span>,  <span class="number">93</span>, <span class="number">244</span>, <span class="number">249</span>, <span class="number">253</span>, <span class="number">187</span>],</span><br><span class="line">        [  <span class="number">0</span>, <span class="number">107</span>, <span class="number">253</span>, <span class="number">253</span>, <span class="number">230</span>,  <span class="number">48</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">3</span>,  <span class="number">20</span>,  <span class="number">20</span>,  <span class="number">15</span>,   <span class="number">0</span>]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p><code>4:10</code>表示我们请求从索引 4（包括）到 10（不包括）的行，列也是一样。NumPy 从上到下，从左到右索引，因此此部分位于图像的左上角附近。这里是一个 PyTorch 张量：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(im3)[<span class="number">4</span>:<span class="number">10</span>,<span class="number">4</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">29</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>,  <span class="number">48</span>, <span class="number">166</span>, <span class="number">224</span>],</span><br><span class="line">        [  <span class="number">0</span>,  <span class="number">93</span>, <span class="number">244</span>, <span class="number">249</span>, <span class="number">253</span>, <span class="number">187</span>],</span><br><span class="line">        [  <span class="number">0</span>, <span class="number">107</span>, <span class="number">253</span>, <span class="number">253</span>, <span class="number">230</span>,  <span class="number">48</span>],</span><br><span class="line">        [  <span class="number">0</span>,   <span class="number">3</span>,  <span class="number">20</span>,  <span class="number">20</span>,  <span class="number">15</span>,   <span class="number">0</span>]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<p>我们可以切片数组，只选择包含数字顶部部分的部分，然后使用 Pandas DataFrame 使用渐变对值进行着色，这清楚地显示了图像是如何由像素值创建的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">im3_t = tensor(im3)</span><br><span class="line">df = pd.DataFrame(im3_t[<span class="number">4</span>:<span class="number">15</span>,<span class="number">4</span>:<span class="number">22</span>])</span><br><span class="line">df.style.set_properties(**&#123;<span class="string">&#x27;font-size&#x27;</span>:<span class="string">&#x27;6pt&#x27;</span>&#125;).background_gradient(<span class="string">&#x27;Greys&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in02.png" alt=""></p>
<p>你可以看到，背景白色像素存储为数字 0，黑色为数字 255，灰色在两者之间。整个图像横向包含 28 个像素，纵向包含 28 个像素，总共 768 个像素。（这比你从手机相机得到的图像要小得多，手机相机有数百万像素，但对于我们的初始学习和实验来说，这是一个方便的大小。我们将很快构建更大的全彩图像。）</p>
<p>所以，现在你已经看到了计算机对图像的看法，让我们回顾一下我们的目标：创建一个能够识别 3 和 7 的模型。你会如何让计算机做到这一点呢？</p>
<h1 id="停下来思考！"><a href="#停下来思考！" class="headerlink" title="停下来思考！"></a>停下来思考！</h1><p>在继续阅读之前，花点时间考虑一下计算机可能如何识别这两个数字。它可能能够看到什么样的特征？它可能如何识别这些特征？它如何将它们结合起来？学习最好的方式是尝试自己解决问题，而不仅仅是阅读别人的答案；所以离开这本书几分钟，拿一张纸和笔，写下一些想法。</p>
<font color = red> 我认为计算机可能会用识别到的手写数字图像的矩阵和标准的数字图像的矩阵进行运算像是点乘或是什么得出一个能给表现图片数字和标准数字相似度的一个数值进行比较取得最大的为识别数字。</font>

<h1 id="第一次尝试：像素相似度"><a href="#第一次尝试：像素相似度" class="headerlink" title="第一次尝试：像素相似度"></a>第一次尝试：像素相似度</h1><p>所以，这是一个第一个想法：我们可以找到每个 3 的像素的平均值，然后对 7 做同样的操作。这将给我们两组平均值，定义了我们可能称之为“理想”3 和 7。然后，为了将图像分类为一个数字或另一个数字，我们看看这两个理想数字中图像与哪个更相似。这肯定似乎比没有好，所以这将成为一个很好的基线。</p>
<h1 id="术语：基线"><a href="#术语：基线" class="headerlink" title="术语：基线"></a>术语：基线</h1><p>一个简单的模型，你有信心应该表现得相当不错。它应该简单实现和易于测试，这样你就可以测试每个改进的想法，并确保它们始终优于基线。如果没有以合理的基线开始，很难知道你的超级花哨的模型是否好用。创建基线的一个好方法是做我们在这里做的事情：考虑一个简单、易于实现的模型。另一个好方法是四处寻找解决类似问题的其他人，并在你的数据集上下载并运行他们的代码。最好两者都尝试一下！</p>
<p>我们简单模型的第一步是获取我们两组像素值的平均值。在这个过程中，我们将学习很多有趣的 Python 数值编程技巧！</p>
<p>让我们创建一个包含所有 3 的张量堆叠在一起。我们已经知道如何创建包含单个图像的张量。要创建一个包含目录中所有图像的张量，我们将首先使用 Python 列表推导来创建一个单个图像张量的普通列表。</p>
<p>我们将使用 Jupyter 在途中做一些小的检查——在这种情况下，确保返回的项目数量看起来合理：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">seven_tensors = [tensor(Image.<span class="built_in">open</span>(o)) <span class="keyword">for</span> o <span class="keyword">in</span> sevens]</span><br><span class="line">three_tensors = [tensor(Image.<span class="built_in">open</span>(o)) <span class="keyword">for</span> o <span class="keyword">in</span> threes]</span><br><span class="line"><span class="built_in">len</span>(three_tensors),<span class="built_in">len</span>(seven_tensors)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(<span class="number">6131</span>, <span class="number">6265</span>)</span><br></pre></td></tr></table></figure>
<h1 id="列表推导"><a href="#列表推导" class="headerlink" title="列表推导"></a>列表推导</h1><p>列表和字典推导是 Python 的一个很棒的特性。许多 Python 程序员每天都在使用它们，包括本书的作者们——它们是“Python 的成语”。但是来自其他语言的程序员可能以前从未见过它们。许多很棒的教程只需一次网络搜索，所以我们现在不会花很长时间讨论它们。</p>
<h5 id="我来补充一下"><a href="#我来补充一下" class="headerlink" title="我来补充一下:"></a>我来补充一下:</h5><h6 id="Python-字典-Dictionary"><a href="#Python-字典-Dictionary" class="headerlink" title="Python 字典(Dictionary)"></a>Python 字典(Dictionary)</h6><font color = blue>Python 字典(Dictionary)
字典是另一种可变容器模型，且可存储任意类型对象。

字典的每个键值 key:value 对用冒号 : 分割，每个键值对之间用逗号 , 分割，整个字典包括在花括号 {} 中 ,格式如下所示：

<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;key1 : value1, key2 : value2 &#125;</span><br></pre></td></tr></table></figure>
注意：dict 作为 Python 的关键字和内置函数，变量名不建议命名为 dict。
键一般是唯一的，如果重复最后的一个键值对会替换前面的，值不需要唯一。
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tinydict = &#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;b&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tinydict[<span class="string">&#x27;b&#x27;</span>]</span><br><span class="line"><span class="string">&#x27;3&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tinydict</span><br><span class="line">&#123;<span class="string">&#x27;a&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;b&#x27;</span>: <span class="string">&#x27;3&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
值可以取任何数据类型，但键必须是不可变的，如字符串，数字或元组。
一个简单的字典实例：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tinydict = &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="string">&#x27;2341&#x27;</span>, <span class="string">&#x27;Beth&#x27;</span>: <span class="string">&#x27;9102&#x27;</span>, <span class="string">&#x27;Cecil&#x27;</span>: <span class="string">&#x27;3258&#x27;</span>&#125;</span><br></pre></td></tr></table></figure>
也可如此创建字典：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tinydict1 = &#123; <span class="string">&#x27;abc&#x27;</span>: <span class="number">456</span> &#125;</span><br><span class="line">tinydict2 = &#123; <span class="string">&#x27;abc&#x27;</span>: <span class="number">123</span>, <span class="number">98.6</span>: <span class="number">37</span> &#125;</span><br></pre></td></tr></table></figure>
这里简单介绍一下字典在网页上还有更加详细的介绍和修改方法：[字典](https://www.runoob.com/python/python-dictionary.html)</font>

<h6 id="Python-列表-List"><a href="#Python-列表-List" class="headerlink" title="Python 列表(List)"></a>Python 列表(List)</h6><font color = orange>序列是Python中最基本的数据结构。序列中的每个元素都分配一个数字 - 它的位置，或索引，第一个索引是0，第二个索引是1，依此类推。

Python有6个序列的内置类型，但最常见的是列表和元组。

序列都可以进行的操作包括索引，切片，加，乘，检查成员。

此外，Python已经内置确定序列的长度以及确定最大和最小的元素的方法。

列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现。

列表的数据项不需要具有相同的类型

创建一个列表，只要把逗号分隔的不同的数据项使用方括号括起来即可。如下所示：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="string">&#x27;physics&#x27;</span>, <span class="string">&#x27;chemistry&#x27;</span>, <span class="number">1997</span>, <span class="number">2000</span>]</span><br><span class="line">list2 = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span> ]</span><br><span class="line">list3 = [<span class="string">&quot;a&quot;</span>, <span class="string">&quot;b&quot;</span>, <span class="string">&quot;c&quot;</span>, <span class="string">&quot;d&quot;</span>]</span><br></pre></td></tr></table></figure>
与字符串的索引一样，列表索引从0开始。列表可以进行截取、组合等。更多对列表的操作:[列表](https://www.runoob.com/python/python-lists.html)</font>

<p>这里有一个快速的解释和示例，让您开始。列表推导看起来像这样：<code>new_list = [f(o) for o in a_list if o&gt;0]</code>。这将返回<code>a_list</code>中大于 0 的每个元素，在将其传递给函数<code>f</code>之后。这里有三个部分：您正在迭代的集合（<code>a_list</code>），一个可选的过滤器（<code>if o&gt;0</code>），以及对每个元素执行的操作（<code>f(o)</code>）。不仅写起来更短，而且比用循环创建相同列表的替代方法更快。</p>
<p>我们还将检查其中一张图像是否正常。由于我们现在有张量（Jupyter 默认会将其打印为值），而不是 PIL 图像（Jupyter 默认会显示图像），我们需要使用 fastai 的<code>show_image</code>函数来显示它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_image(three_tensors[<span class="number">1</span>]);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in03.png" alt=""></p>
<p>对于每个像素位置，我们想要计算该像素的强度在所有图像上的平均值。为了做到这一点，我们首先将此列表中的所有图像组合成一个三维张量。描述这样的张量最常见的方式是称之为<em>rank-3 张量</em>。我们经常需要将集合中的单个张量堆叠成一个张量。不出所料，PyTorch 带有一个名为<code>stack</code>的函数，我们可以用它来实现这个目的。</p>
<p>PyTorch 中的一些操作，如取平均值，需要我们将整数类型转换为浮点类型。由于我们稍后会需要这个，我们现在也将我们的堆叠张量转换为<code>float</code>。在 PyTorch 中进行转换就像写下您希望转换为的类型名称，并将其视为方法一样简单。</p>
<p>通常，当图像是浮点数时，像素值应该在 0 到 1 之间，所以我们也会在这里除以 255：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">stacked_sevens = torch.stack(seven_tensors).<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">stacked_threes = torch.stack(three_tensors).<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">stacked_threes.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">6131</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure>
<p>张量最重要的属性也许是其<em>形状</em>。这告诉您每个轴的长度。在这种情况下，我们可以看到我们有 6,131 张图像，每张图像大小为 28×28 像素。关于这个张量没有特别的地方表明第一个轴是图像的数量，第二个是高度，第三个是宽度——张量的语义完全取决于我们以及我们如何构建它。就 PyTorch 而言，它只是内存中的一堆数字。</p>
<p>张量形状的<em>长度</em>是其秩：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">len</span>(stacked_threes.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>对于您来说，将张量术语的这些部分记忆并加以实践非常重要：<em>秩</em>是张量中轴或维度的数量；<em>形状</em>是张量每个轴的大小。</p>
<h1 id="关于维度"><a href="#关于维度" class="headerlink" title="关于维度"></a>关于维度</h1><p>要小心，因为术语“维度”有时以两种方式使用。考虑我们生活在“三维空间”中，其中物理位置可以用长度为 3 的向量<code>v</code>描述。但根据 PyTorch，属性<code>v.ndim</code>（看起来确实像<code>v</code>的“维度数量”）等于一，而不是三！为什么？因为<code>v</code>是一个向量，它是一个秩为一的张量，这意味着它只有一个<em>轴</em>（即使该轴的长度为三）。换句话说，有时维度用于描述轴的大小（“空间是三维的”），而其他时候用于描述秩或轴的数量（“矩阵有两个维度”）。当感到困惑时，我发现将所有陈述转换为秩、轴和长度这些明确的术语是有帮助的。</p>
<p>我们也可以直接使用<code>ndim</code>来获取张量的秩：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stacked_threes.ndim</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure>
<p>最后，我们可以计算理想的 3 是什么样子的。我们通过沿着我们堆叠的 rank-3 张量的维度 0 取平均值来计算所有图像张量的平均值。这是索引所有图像的维度。</p>
<p>换句话说，对于每个像素位置，这将计算所有图像中该像素的平均值。结果将是每个像素位置的一个值，或者一个单独的图像。这就是它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean3 = stacked_threes.mean(<span class="number">0</span>)</span><br><span class="line">show_image(mean3);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in04.png" alt=""></p>
<p>根据这个数据集，这是理想的数字 3！（您可能不喜欢，但这就是顶级数字 3 表现的样子。）您可以看到在所有图像都认为应该是暗的地方非常暗，但在图像不一致的地方变得模糊。</p>
<p>让我们对 7 做同样的事情，但一次将所有步骤放在一起以节省时间：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mean7 = stacked_sevens.mean(<span class="number">0</span>)</span><br><span class="line">show_image(mean7);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in05.png" alt=""></p>
<p>现在让我们选择一个任意的 3，并测量它与我们的“理想数字”的<em>距离</em>。</p>
<h1 id="停下来思考一下！"><a href="#停下来思考一下！" class="headerlink" title="停下来思考一下！"></a>停下来思考一下！</h1><p>您如何计算特定图像与我们的每个理想数字之间的相似程度？在继续前进之前，请记得远离这本书，记录一些想法！研究表明，通过解决问题、实验和尝试新想法，您参与学习过程时，召回和理解会显著提高。</p>
<p>这是一个示例 3：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a_3 = stacked_threes[<span class="number">1</span>]</span><br><span class="line">show_image(a_3);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in06.png" alt=""></p>
<p>我们如何确定它与我们理想的 3 之间的距离？我们不能简单地将此图像的像素之间的差异相加，并与理想数字进行比较。一些差异将是正的，而另一些将是负的，这些差异将相互抵消，导致一种情况，即在某些地方太暗而在其他地方太亮的图像可能被显示为与理想的总差异为零。那将是误导性的！</p>
<p>为了避免这种情况，数据科学家在这种情况下使用两种主要方法来测量距离：</p>
<ul>
<li><p>取差值的<em>绝对值</em>的平均值（绝对值是将负值替换为正值的函数）。这被称为<em>平均绝对差</em>或<em>L1 范数</em>。</p>
</li>
<li><p>取差值的<em>平方</em>的平均值（使所有值变为正数），然后取<em>平方根</em>（撤销平方）。这被称为<em>均方根误差</em>（RMSE）或<em>L2 范数</em>。</p>
</li>
</ul>
<h1 id="在pytorch中-取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1-范数。"><a href="#在pytorch中-取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1-范数。" class="headerlink" title="在pytorch中 取差值的绝对值的平均值（绝对值是将负值替换为正值的函数）。这被称为平均绝对差或L1 范数。"></a>在pytorch中 取差值的<em>绝对值</em>的平均值（绝对值是将负值替换为正值的函数）。这被称为<em>平均绝对差</em>或<em>L1 范数</em>。</h1><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dist_7_abs = (a_3 - mean7).<span class="built_in">abs</span>().mean()</span><br><span class="line">dist_7_sqr = ((a_3 - mean7)**<span class="number">2</span>).mean().sqrt()</span><br><span class="line">dist_7_abs,dist_7_sqr</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.1586</span>), tensor(<span class="number">0.3021</span>))</span><br></pre></td></tr></table></figure>
<p>等同于：</p>
<p>PyTorch 已经提供了这两种作为<em>损失函数</em>。您会在<code>torch.nn.functional</code>中找到这些，PyTorch 团队建议将其导入为<code>F</code>（并且默认情况下以这个名称在 fastai 中可用）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">F.l1_loss(a_3.<span class="built_in">float</span>(),mean7), F.mse_loss(a_3,mean7).sqrt()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.1586</span>), tensor(<span class="number">0.3021</span>))</span><br></pre></td></tr></table></figure>
<p>在这里，<code>MSE</code>代表<em>均方误差</em>，<code>l1</code>是标准数学术语<em>平均绝对值</em>的缩写（在数学中称为<em>L1 范数</em>）。</p>
<h1 id="L1-范数和-均方误差（MSE）之间的区别"><a href="#L1-范数和-均方误差（MSE）之间的区别" class="headerlink" title="L1 范数和 均方误差（MSE）之间的区别"></a>L1 范数和 均方误差（MSE）之间的区别</h1><p>直观地，L1 范数和均方误差（MSE）之间的区别在于，后者会比前者更严厉地惩罚更大的错误（并对小错误更宽容）。</p>
<h1 id="杰里米说"><a href="#杰里米说" class="headerlink" title="杰里米说"></a>杰里米说</h1><p>当我第一次遇到这个 L1 的东西时，我查了一下看它到底是什么意思。我在谷歌上发现它是使用“绝对值”作为“向量范数”，所以我查了“向量范数”并开始阅读：“给定一个实数或复数域 F 上的向量空间 V，V 上的范数是一个非负值的任意函数 p: V → [0,+∞)，具有以下属性：对于所有的 a ∈ F 和所有的 u, v ∈ V，p(u + v) ≤ p(u) + p(v)…”然后我停止阅读。“唉，我永远也理解不了数学！”我想，这已经是第一千次了。从那时起，我学到了每当实践中出现这些复杂的数学术语时，我可以用一点点代码来替换它们！比如，<em>L1 损失</em> 只等于 <code>(a-b).abs().mean()</code>，其中 <code>a</code> 和 <code>b</code> 是张量。我猜数学家们只是和我想法不同…我会确保在本书中，每当出现一些数学术语时，我会给你相应的代码片段，并用通俗的语言解释发生了什么。</p>
<p>我们刚刚在 PyTorch 张量上完成了各种数学运算。如果你之前在 PyTorch 中进行过数值编程，你可能会发现这些与 NumPy 数组相似。让我们来看看这两个重要的数据结构。</p>
<p>（请注意，fastai 在 NumPy 和 PyTorch 中添加了一些功能，使它们更加相似。如果本书中的任何代码在您的计算机上无法运行，可能是因为您忘记在笔记本的开头包含类似这样的一行代码：<code>from fastai.vision.all import *</code>。）</p>
<p>但是数组和张量是什么，为什么你应该关心呢？</p>
<p>Python 相对于许多语言来说速度较慢。在 Python、NumPy 或 PyTorch 中快速的任何东西，很可能是另一种语言（特别是 C）编写（并优化）的编译对象的包装器。事实上，<em>NumPy 数组和 PyTorch 张量可以比纯 Python 快几千倍完成计算</em>。</p>
<p>NumPy 数组是一个多维数据表，所有项都是相同类型的。由于可以是任何类型，它们甚至可以是数组的数组，内部数组可能是不同大小的 - 这被称为 <em>不规则数组</em>。通过“多维数据表”，我们指的是，例如，一个列表（一维）、一个表或矩阵（二维）、一个表的表或立方体（三维），等等。如果所有项都是简单类型，如整数或浮点数，NumPy 将它们存储为紧凑的 C 数据结构在内存中。这就是 NumPy 的优势所在。NumPy 有各种运算符和方法，可以在这些紧凑结构上以优化的 C 速度运行计算，因为它们是用优化的 C 编写的。</p>
<p>PyTorch 张量几乎与 NumPy 数组相同，但有一个额外的限制，可以解锁额外的功能。它与 NumPy 数组相同，也是一个多维数据表，所有项都是相同类型的。然而，限制是张量不能使用任何旧类型 - 它必须对所有组件使用单一基本数值类型。因此，张量不像真正的数组数组那样灵活。例如，PyTorch 张量不能是不规则的。它始终是一个形状规则的多维矩形结构。</p>
<p>NumPy 在这些结构上支持的绝大多数方法和运算符在 PyTorch 上也支持，但 PyTorch 张量具有额外的功能。一个主要功能是这些结构可以存在于 GPU 上，这样它们的计算将被优化为 GPU，并且可以运行得更快（给定大量值进行处理）。此外，PyTorch 可以自动计算这些操作的导数，包括操作的组合。正如你将看到的，没有这种能力，实际上是不可能进行深度学习的。</p>
<h1 id="如何有效地使用数组-张量-API-是最重要的新编码技能。"><a href="#如何有效地使用数组-张量-API-是最重要的新编码技能。" class="headerlink" title="如何有效地使用数组/张量 API 是最重要的新编码技能。"></a>如何有效地使用数组/张量 API 是最重要的新编码技能。</h1><p>要创建一个数组或张量，将列表（或列表的列表，或列表的列表的列表等）传递给<code>array</code>或<code>tensor</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">arr = array (data)</span><br><span class="line">tns = tensor(data)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">arr  <span class="comment"># numpy</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns  <span class="comment"># pytorch</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br></pre></td></tr></table></figure>
<p>以下所有操作都是在张量上展示的，但 NumPy 数组的语法和结果是相同的。</p>
<p>你可以选择一行（请注意，与 Python 中的列表一样，张量是从 0 开始索引的，所以 1 指的是第二行/列）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>或者通过使用<code>:</code>来指示<em>所有第一个轴</em>（我们有时将张量/数组的维度称为<em>轴</em>）选择一列。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns[:,<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>你可以结合 Python 切片语法（<code>[*start*:*end*]</code>，其中<em><code>end</code></em>被排除）来选择一行或一列的一部分：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns[<span class="number">1</span>,<span class="number">1</span>:<span class="number">3</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
<p>你可以使用标准运算符，如<code>+</code>、<code>-</code>、<code>*</code>和<code>/</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns+<span class="number">1</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p>张量有一个类型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns.<span class="built_in">type</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;torch.LongTensor&#x27;</span></span><br></pre></td></tr></table></figure>
<p>并且会根据需要自动更改该类型；例如，从<code>int</code>到<code>float</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tns*<span class="number">1.5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">1.5000</span>, <span class="number">3.0000</span>, <span class="number">4.5000</span>],</span><br><span class="line">        [<span class="number">6.0000</span>, <span class="number">7.5000</span>, <span class="number">9.0000</span>]])</span><br></pre></td></tr></table></figure>
<p>那么，我们的基准模型好吗？为了量化这一点，我们必须定义一个度量。</p>
<h1 id="使用广播计算度量"><a href="#使用广播计算度量" class="headerlink" title="使用广播计算度量"></a>使用广播计算度量</h1><p>回想一下<em>度量</em>是基于我们模型的预测和数据集中正确标签计算出来的一个数字，以告诉我们我们的模型有多好。例如，我们可以使用我们在上一节中看到的两个函数之一，均方误差或平均绝对误差，并计算整个数据集上它们的平均值。然而，这两个数字对大多数人来说并不是很容易理解；实际上，我们通常使用<em>准确度</em>作为分类模型的度量。</p>
<p>正如我们讨论过的，我们想要在<em>验证集</em>上计算我们的度量。这样我们就不会无意中过拟合——也就是说，训练一个模型只在我们的训练数据上表现良好。这对于我们在这里作为第一次尝试使用的像素相似度模型来说并不是真正的风险，因为它没有经过训练的组件，但我们仍然会使用一个验证集来遵循正常的实践，并为我们稍后的第二次尝试做好准备。</p>
<p>为了获得一个验证集，我们需要完全从训练数据中删除一些数据，这样模型根本就看不到它。事实证明，MNIST 数据集的创建者已经为我们做了这个。你还记得<em>valid</em>这个整个独立的目录吗？这个目录就是为此而设立的！</p>
<p>所以，让我们从那个目录中为我们的 3 和 7 创建张量。这些是我们将用来计算度量的张量，用来衡量我们第一次尝试模型的质量，这个度量衡量了与理想图像的距离：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">valid_3_tens = torch.stack([tensor(Image.<span class="built_in">open</span>(o))</span><br><span class="line">                            <span class="keyword">for</span> o <span class="keyword">in</span> (path/<span class="string">&#x27;valid&#x27;</span>/<span class="string">&#x27;3&#x27;</span>).ls()])</span><br><span class="line">valid_3_tens = valid_3_tens.<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">valid_7_tens = torch.stack([tensor(Image.<span class="built_in">open</span>(o))</span><br><span class="line">                            <span class="keyword">for</span> o <span class="keyword">in</span> (path/<span class="string">&#x27;valid&#x27;</span>/<span class="string">&#x27;7&#x27;</span>).ls()])</span><br><span class="line">valid_7_tens = valid_7_tens.<span class="built_in">float</span>()/<span class="number">255</span></span><br><span class="line">valid_3_tens.shape,valid_7_tens.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">1010</span>, <span class="number">28</span>, <span class="number">28</span>]), torch.Size([<span class="number">1028</span>, <span class="number">28</span>, <span class="number">28</span>]))</span><br></pre></td></tr></table></figure>
<p>在进行操作时检查形状是一个好习惯。在这里我们看到两个张量，一个代表了 1,010 张大小为 28×28 的 3 的验证集，另一个代表了 1,028 张大小为 28×28 的 7 的验证集。</p>
<p>我们最终想要编写一个函数<code>is_3</code>，它将决定任意图像是 3 还是 7。它将通过确定任意图像更接近我们的两个“理想数字”中的哪一个来实现这一点。为此，我们需要定义<em>距离</em>的概念——即，计算两个图像之间距离的函数。</p>
<p>我们可以编写一个简单的函数，使用与我们在上一节中编写的表达式非常相似的表达式来计算平均绝对误差：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_distance</span>(<span class="params">a,b</span>): <span class="keyword">return</span> (a-b).<span class="built_in">abs</span>().mean((-<span class="number">1</span>,-<span class="number">2</span>))</span><br><span class="line">mnist_distance(a_3, mean3)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.1114</span>)</span><br></pre></td></tr></table></figure>
<p>这是我们先前为这两个图像之间的距离计算的相同值，理想数字 3 <code>mean_3</code>和任意样本 3 <code>a_3</code>，它们都是形状为<code>[28,28]</code>的单个图像张量。</p>
<p>但是要计算整体准确度的指标，我们需要计算验证集中<em>每张</em>图像到理想数字 3 的距离。我们如何进行这种计算？我们可以编写一个循环，遍历验证集张量<code>valid_3_tens</code>中堆叠的所有单图像张量，其形状为<code>[1010,28,28]</code>，表示 1,010 张图像。但是有一种更好的方法。</p>
<p>当我们使用相同的距离函数，设计用于比较两个单个图像，但将表示 3 的验证集张量<code>valid_3_tens</code>作为参数传入时，会发生一些有趣的事情：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">valid_3_dist = mnist_distance(valid_3_tens, mean3)</span><br><span class="line">valid_3_dist, valid_3_dist.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(tensor([<span class="number">0.1050</span>, <span class="number">0.1526</span>, <span class="number">0.1186</span>,  ..., <span class="number">0.1122</span>, <span class="number">0.1170</span>, <span class="number">0.1086</span>]),</span><br><span class="line"> torch.Size([<span class="number">1010</span>]))</span><br></pre></td></tr></table></figure>
<p>它没有抱怨形状不匹配，而是为每个单个图像返回了一个距离（即，长度为 1,010 的秩-1 张量）。这是如何发生的？</p>
<p>再看看我们的函数<code>mnist_distance</code>，您会看到我们在那里有减法<code>(a-b)</code>。魔术技巧在于 PyTorch 在尝试在不同秩的两个张量之间执行简单的减法操作时，将使用<em>广播</em>：它将自动扩展秩较小的张量，使其大小与秩较大的张量相同。广播是一种重要的功能，使张量代码更容易编写。</p>
<p>在广播后，使两个参数张量具有相同的秩后，PyTorch 对于秩相同的两个张量应用其通常的逻辑：它对两个张量的每个对应元素执行操作，并返回张量结果。例如：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]) + tensor([<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<p>因此，在这种情况下，PyTorch 将<code>mean3</code>视为一个表示单个图像的秩-2 张量，就好像它是 1,010 个相同图像的副本，然后从我们的验证集中的每个 3 中减去每个副本。您期望这个张量的形状是什么？在查看这里的答案之前，请尝试自己想出来：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(valid_3_tens-mean3).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">1010</span>, <span class="number">28</span>, <span class="number">28</span>])</span><br></pre></td></tr></table></figure>
<p>我们正在计算我们的理想数字 3 与验证集中的每个 1,010 个 3 之间的差异，对于每个 28×28 图像，结果形状为<code>[1010,28,28]</code>。</p>
<p>有关广播实现的一些重要要点，使其不仅对于表达性有价值，而且对于性能也有价值：</p>
<ul>
<li><p>PyTorch 实际上并没有将<code>mean3</code>复制 1,010 次。它<em>假装</em>它是一个具有该形状的张量，但不分配任何额外内存。</p>
</li>
<li><p>它在 C 中完成整个计算（或者，如果您使用 GPU，则在 CUDA 中，相当于 GPU 上的 C），比纯 Python 快数万倍（在 GPU 上甚至快数百万倍！）。</p>
</li>
</ul>
<p>这适用于 PyTorch 中所有广播和逐元素操作和函数。<em>这是您要了解的最重要的技术，以创建高效的 PyTorch 代码。</em></p>
<p>接下来在<code>mnist_distance</code>中我们看到<code>abs</code>。现在您可能能猜到将其应用于张量时会发生什么。它将方法应用于张量中的每个单独元素，并返回结果的张量（即，它逐元素应用方法）。因此，在这种情况下，我们将得到 1,010 个绝对值。</p>
<p>最后，我们的函数调用<code>mean((-1,-2))</code>。元组<code>(-1,-2)</code>表示一系列轴。在 Python 中，<code>-1</code>指的是最后一个元素，<code>-2</code>指的是倒数第二个元素。因此，在这种情况下，这告诉 PyTorch 我们要对张量的最后两个轴的值进行平均。最后两个轴是图像的水平和垂直维度。在对最后两个轴进行平均后，我们只剩下第一个张量轴，它索引我们的图像，这就是为什么我们的最终大小是<code>(1010)</code>。换句话说，对于每个图像，我们对该图像中所有像素的强度进行了平均。</p>
<p>在本书中，我们将学习更多关于广播的知识，特别是在第十七章中，并且也会经常进行实践。</p>
<p>我们可以使用<code>mnist_distance</code>来确定一幅图像是否为 3，方法是使用以下逻辑：如果问题中的数字与理想的 3 之间的距离小于到理想的 7 的距离，则它是一个 3。这个函数将自动进行广播，并逐个应用，就像所有 PyTorch 函数和运算符一样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">is_3</span>(<span class="params">x</span>): <span class="keyword">return</span> mnist_distance(x,mean3) &lt; mnist_distance(x,mean7)</span><br></pre></td></tr></table></figure>
<p>让我们在我们的示例案例上测试一下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">is_3(a_3), is_3(a_3).<span class="built_in">float</span>()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="literal">True</span>), tensor(<span class="number">1.</span>))</span><br></pre></td></tr></table></figure>
<p>请注意，当我们将布尔响应转换为浮点数时，<code>True</code>会得到<code>1.0</code>，<code>False</code>会得到<code>0.0</code>。</p>
<p>由于广播，我们还可以在所有 3 的完整验证集上进行测试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">is_3(valid_3_tens)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>,  ..., <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>
<p>现在我们可以计算每个 3 和 7 的准确率，方法是对所有 3 的函数取平均值，对所有 7 的函数取其倒数的平均值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">accuracy_3s =      is_3(valid_3_tens).<span class="built_in">float</span>() .mean()</span><br><span class="line">accuracy_7s = (<span class="number">1</span> - is_3(valid_7_tens).<span class="built_in">float</span>()).mean()</span><br><span class="line"></span><br><span class="line">accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/<span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(<span class="number">0.9168</span>), tensor(<span class="number">0.9854</span>), tensor(<span class="number">0.9511</span>))</span><br></pre></td></tr></table></figure>
<p>这看起来是一个相当不错的开始！我们在 3 和 7 上都获得了超过 90%的准确率，我们已经看到了如何使用广播方便地定义度量。但让我们诚实一点：3 和 7 是非常不同的数字。到目前为止，我们只对 10 个可能的数字中的 2 个进行分类。所以我们需要做得更好！</p>
<p>为了做得更好，也许现在是时候尝试一个真正学习的系统了，一个可以自动修改自身以提高性能的系统。换句话说，现在是时候谈论训练过程和 SGD 了。</p>
<h1 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h1><p>你还记得 Arthur Samuel 在第一章中描述机器学习的方式吗？</p>
<blockquote>
<p>假设我们安排一些自动手段来测试任何当前权重分配的有效性，以实际性能为基础，并提供一种机制来改变权重分配以最大化性能。我们不需要详细了解这种程序的细节，就可以看到它可以完全自动化，并且可以看到一个这样编程的机器会从中学习。</p>
</blockquote>
<p>正如我们讨论过的，这是让我们拥有一个可以变得越来越好的模型的关键，可以学习。但我们的像素相似性方法实际上并没有做到这一点。我们没有任何权重分配，也没有任何根据测试权重分配的有效性来改进的方法。换句话说，我们无法通过修改一组参数来改进我们的像素相似性方法。为了充分利用深度学习的力量，我们首先必须按照 Samuel 描述的方式来表示我们的任务。</p>
<p>与其尝试找到图像与“理想图像”之间的相似性，我们可以查看每个单独的像素，并为每个像素提出一组权重，使得最高的权重与最有可能为特定类别的黑色像素相关联。例如，向右下方的像素不太可能被激活为 7，因此它们对于 7 的权重应该很低，但它们很可能被激活为 8，因此它们对于 8 的权重应该很高。这可以表示为一个函数和每个可能类别的一组权重值，例如，成为数字 8 的概率：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pr_eight</span>(<span class="params">x,w</span>) = (x*w).<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure>
<p>在这里，我们假设<code>X</code>是图像，表示为一个向量—换句话说，所有行都堆叠在一起形成一个长长的单行。我们假设权重是一个向量<code>W</code>。如果我们有了这个函数，我们只需要一种方法来更新权重，使它们变得更好一点。通过这种方法，我们可以重复这个步骤多次，使权重变得越来越好，直到我们能够使它们尽可能好。</p>
<p>我们希望找到导致我们的函数对于那些是 8 的图像结果高，对于那些不是的图像结果低的向量<code>W</code>的特定值。搜索最佳向量<code>W</code>是搜索最佳函数以识别 8 的一种方式。（因为我们还没有使用深度神经网络，我们受到我们的函数能力的限制，我们将在本章后面解决这个约束。）</p>
<p>更具体地说，<font color=orange>以下是将这个函数转化为机器学习分类器所需的步骤：</font></p>
<ol>
<li><p><em>初始化</em>权重。</p>
</li>
<li><p>对于每个图像，使用这些权重来<em>预测</em>它是 3 还是 7。</p>
</li>
<li><p>基于这些预测，计算模型有多好（它的<em>损失</em>）。</p>
</li>
<li><p>计算<em>梯度</em>，它衡量了每个权重的变化如何改变损失。</p>
</li>
<li><p>根据这个计算，<em>改变</em>（即，改变）所有权重。</p>
</li>
<li><p>回到步骤 2 并<em>重复</em>这个过程。</p>
</li>
<li><p>迭代直到你决定<em>停止</em>训练过程（例如，因为模型已经足够好或者你不想再等待了）。</p>
<!-- htmlmin:ignore -->
<p><pre class="mermaid">graph TD<br>A[初始化权重] —&gt; B[预测图像是3或7]<br>B —&gt; C[计算模型损失]<br>C —&gt; D[计算梯度]<br>D —&gt; E[更新权重]<br>E —&gt; F{停止条件满足？}<br>F — 否 —&gt; B<br>F — 是 —&gt; G[结束训练]</p>
<p>style A fill:#f9f,stroke:#333<br>style B fill:#bbf,stroke:#333<br>style C fill:#bfb,stroke:#333<br>style D fill:#ffb,stroke:#333<br>style E fill:#fbb,stroke:#333<br>style F fill:#fbf,stroke:#333,shape:hexagon<br>style G fill:#9f9,stroke:#333&lt;/pre&gt;</p>
<!-- htmlmin:ignore -->
<p>这七个步骤，如图 4-1 所示，是所有深度学习模型训练的关键。深度学习完全依赖于这些步骤，这是非常令人惊讶和反直觉的。令人惊奇的是，这个过程可以解决如此复杂的问题。但是，正如你将看到的，它确实可以！</p>
</li>
</ol>
<p><img src="/image/dlcf_0401.png" alt="显示梯度下降步骤的图表"></p>
<h6 id="图-4-1-梯度下降过程"><a href="#图-4-1-梯度下降过程" class="headerlink" title="图 4-1. 梯度下降过程"></a>图 4-1. 梯度下降过程</h6><p>每个步骤都有许多方法，我们将在本书的其余部分学习它们。这些细节对于深度学习从业者来说非常重要，但事实证明，对于每个步骤的一般方法都遵循一些基本原则。以下是一些建议：</p>
<font color = cred>初始化</font>

<p>我们将参数初始化为随机值。这可能听起来令人惊讶。我们当然可以做其他选择，比如将它们初始化为该类别激活该像素的百分比—但由于我们已经知道我们有一种方法来改进这些权重，结果证明只是从随机权重开始就可以完全正常运行。</p>
<font color = cred>损失</font>

<p>这就是 Samuel 所说的<em>根据实际表现测试任何当前权重分配的有效性</em>。我们需要一个函数，如果模型的表现好，它将返回一个小的数字（标准方法是将小的损失视为好的，大的损失视为坏的，尽管这只是一种约定）。</p>
<font color = cred>步骤</font>

<p>一个简单的方法来判断一个权重是否应该增加一点或减少一点就是尝试一下：增加一点权重，看看损失是增加还是减少。一旦找到正确的方向，你可以再多改变一点或少改变一点，直到找到一个效果好的量。然而，这很慢！正如我们将看到的，微积分的魔力使我们能够直接找出每个权重应该朝哪个方向改变，大概改变多少，而不必尝试所有这些小的改变。这样做的方法是通过计算<em>梯度</em>。这只是一种性能优化；我们也可以通过使用更慢的手动过程得到完全相同的结果。</p>
<font color = cred>停止</font>

<p>一旦我们决定要为模型训练多少个周期（之前的列表中给出了一些建议），我们就会应用这个决定。对于我们的数字分类器，我们会继续训练，直到模型的准确率开始变差，或者我们用完时间为止。</p>
<p>在将这些步骤应用于我们的图像分类问题之前，让我们在一个更简单的情况下看看它们是什么样子。首先我们将定义一个非常简单的函数，二次函数—假设这是我们的损失函数，<code>x</code>是函数的权重参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="keyword">return</span> x**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>这是该函数的图表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(f, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x**2&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in07.png" alt=""></p>
<p>我们之前描述的步骤序列从选择参数的随机值开始，并计算损失的值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_function(f, <span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;x**2&#x27;</span>)</span><br><span class="line">plt.scatter(-<span class="number">1.5</span>, f(-<span class="number">1.5</span>), color=<span class="string">&#x27;red&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in08.png" alt=""></p>
<p>现在我们来看看如果我们稍微增加或减少参数会发生什么—<em>调整</em>。这只是特定点的斜率：</p>
<p><img src="/image/dlcf_04in09.png" alt="显示在某一点的斜率的平方函数的图表"></p>
<p>我们可以稍微改变我们的权重朝着斜坡的方向，计算我们的损失和调整，然后再重复几次。最终，我们将到达曲线上的最低点：</p>
<p><img src="/image/dlcf_04in10.png" alt="梯度下降的示意图"></p>
<p>这个基本思想最早可以追溯到艾萨克·牛顿，他指出我们可以以这种方式优化任意函数。无论我们的函数变得多么复杂，梯度下降的这种基本方法不会有太大变化。我们在本书后面看到的唯一微小变化是一些方便的方法，可以让我们更快地找到更好的步骤。</p>
<h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>唯一的魔法步骤是计算梯度的部分。正如我们提到的，我们使用微积分作为性能优化；它让我们更快地计算当我们调整参数时我们的损失会上升还是下降。换句话说，梯度将告诉我们我们需要改变每个权重多少才能使我们的模型更好。</p>
<p>您可能还记得高中微积分课上的<em>导数</em>告诉您函数参数的变化会如何改变其结果。如果不记得，不用担心；我们很多人高中毕业后就忘了微积分！但在继续之前，您需要对导数有一些直观的理解，所以如果您对此一头雾水，可以前往 Khan Academy 完成<a target="_blank" rel="noopener" href="https://oreil.ly/nyd0R">基本导数课程</a>。您不必自己计算导数；您只需要知道导数是什么。</p>
<p>导数的关键点在于：对于任何函数，比如我们在前一节中看到的二次函数，我们可以计算它的导数。导数是另一个函数。它计算的是变化，而不是值。例如，在值为 3 时，二次函数的导数告诉我们函数在值为 3 时的变化速度。更具体地说，您可能还记得梯度被定义为<em>上升/水平移动</em>；也就是说，函数值的变化除以参数值的变化。当我们知道我们的函数将如何变化时，我们就知道我们需要做什么来使它变小。这是机器学习的关键：有一种方法来改变函数的参数使其变小。微积分为我们提供了一个计算的捷径，即导数，它让我们直接计算我们函数的梯度。</p>
<p>一个重要的事情要注意的是我们的函数有很多需要调整的权重，所以当我们计算导数时，我们不会得到一个数字，而是很多个—每个权重都有一个梯度。但在这里没有数学上的技巧；您可以计算相对于一个权重的导数，将其他所有权重视为常数，然后对每个其他权重重复这个过程。这就是计算所有梯度的方法，对于每个权重。</p>
<p>刚才我们提到您不必自己计算任何梯度。这怎么可能？令人惊讶的是，PyTorch 能够自动计算几乎任何函数的导数！而且，它计算得非常快。大多数情况下，它至少与您手动创建的任何导数函数一样快。让我们看一个例子。</p>
<p>首先，让我们选择一个张量数值，我们想要梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xt = tensor(<span class="number">3.</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<p>注意特殊方法<code>requires_grad_</code>？这是我们告诉 PyTorch 我们想要计算梯度的神奇咒语。这实质上是给变量打上标记，这样 PyTorch 就会记住如何计算您要求的其他直接计算的梯度。</p>
<font color=orange> 好像仅仅是求导 </font>

<h1 id="Alexis-说"><a href="#Alexis-说" class="headerlink" title="Alexis 说"></a>Alexis 说</h1><p>如果您来自数学或物理学，这个 API 可能会让您困惑。在这些背景下，函数的“梯度”只是另一个函数（即，它的导数），因此您可能期望与梯度相关的 API 提供给您一个新函数。但在深度学习中，“梯度”通常意味着函数的导数在特定参数值处的<em>值</em>。PyTorch API 也将重点放在参数上，而不是您实际计算梯度的函数。起初可能感觉有些反常，但这只是一个不同的视角。</p>
<p>现在我们用这个值计算我们的函数。注意 PyTorch 打印的不仅是计算的值，还有一个提示，它有一个梯度函数将在需要时用来计算我们的梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yt = f(xt)</span><br><span class="line">yt</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">9.</span>, grad_fn=&lt;PowBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>最后，我们告诉 PyTorch 为我们计算梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yt.backward()</span><br></pre></td></tr></table></figure>
<p>这里的<code>backward</code>指的是<em>反向传播</em>，这是计算每一层导数的过程的名称。我们将在第十七章中看到这是如何精确完成的，当我们从头开始计算深度神经网络的梯度时。这被称为网络的<em>反向传播</em>，与<em>前向传播</em>相对，前者是计算激活的地方。如果<code>backward</code>只是被称为<code>calculate_grad</code>，生活可能会更容易，但深度学习的人确实喜欢在任何地方添加行话！</p>
<p>我们现在可以通过检查我们张量的<code>grad</code>属性来查看梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xt.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">6.</span>)</span><br></pre></td></tr></table></figure>
<p>如果您记得高中微积分规则，<code>x**2</code>的导数是<code>2*x</code>，我们有<code>x=3</code>，所以梯度应该是<code>2*3=6</code>，这就是 PyTorch 为我们计算的结果！</p>
<p>现在我们将重复前面的步骤，但使用一个向量参数来计算我们的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xt = tensor([<span class="number">3.</span>,<span class="number">4.</span>,<span class="number">10.</span>]).requires_grad_()</span><br><span class="line">xt</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">3.</span>,  <span class="number">4.</span>, <span class="number">10.</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>并且我们将<code>sum</code>添加到我们的函数中，以便它可以接受一个向量（即，一个秩为 1 的张量）并返回一个标量（即，一个秩为 0 的张量）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>): <span class="keyword">return</span> (x**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line"></span><br><span class="line">yt = f(xt)</span><br><span class="line">yt</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">125.</span>, grad_fn=&lt;SumBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>我们的梯度是<code>2*xt</code>，正如我们所期望的！</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yt.backward()</span><br><span class="line">xt.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">6.</span>,  <span class="number">8.</span>, <span class="number">20.</span>])</span><br></pre></td></tr></table></figure>
<p>梯度告诉我们函数的斜率；它们并不告诉我们要调整参数多远。但它们确实给了我们一些想法：如果斜率非常大，那可能意味着我们需要更多的调整，而如果斜率非常小，那可能意味着我们接近最优值。</p>
<h2 id="使用学习率进行步进"><a href="#使用学习率进行步进" class="headerlink" title="使用学习率进行步进"></a>使用学习率进行步进</h2><p>根据梯度值来决定如何改变我们的参数是深度学习过程中的一个重要部分。几乎所有方法都从一个基本思想开始，即将梯度乘以一些小数字，称为<em>学习率</em>（LR）。学习率通常是 0.001 到 0.1 之间的数字，尽管它可以是任何值。通常人们通过尝试几个学习率来选择一个，并找出哪个在训练后产生最佳模型的结果（我们将在本书后面展示一个更好的方法，称为<em>学习率查找器</em>）。一旦选择了学习率，您可以使用这个简单函数调整参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w -= w.grad * lr</span><br></pre></td></tr></table></figure>
<p>这被称为<em>调整</em>您的参数，使用<em>优化步骤</em>。</p>
<p>如果您选择的学习率太低，可能意味着需要执行很多步骤。图 4-2 说明了这一点。</p>
<p><img src="/image/dlcf_0402.png" alt="梯度下降示例，学习率过低"></p>
<h6 id="图-4-2。学习率过低的梯度下降"><a href="#图-4-2。学习率过低的梯度下降" class="headerlink" title="图 4-2。学习率过低的梯度下降"></a>图 4-2。学习率过低的梯度下降</h6><p>但选择一个学习率太高的学习率更糟糕——它可能导致损失变得<em>更糟</em>，正如我们在图 4-3 中看到的！</p>
<p><img src="/image/dlcf_0403.png" alt="学习率过高的梯度下降示例"></p>
<h6 id="图-4-3-学习率过高的梯度下降"><a href="#图-4-3-学习率过高的梯度下降" class="headerlink" title="图 4-3. 学习率过高的梯度下降"></a>图 4-3. 学习率过高的梯度下降</h6><p>如果学习率太高，它也可能会“弹跳”而不是发散；图 4-4 显示了这样做需要许多步骤才能成功训练。</p>
<p><img src="/image/dlcf_0404.png" alt="带有弹跳学习率的梯度下降示例"></p>
<h6 id="图-4-4-带有弹跳学习率的梯度下降"><a href="#图-4-4-带有弹跳学习率的梯度下降" class="headerlink" title="图 4-4. 带有弹跳学习率的梯度下降"></a>图 4-4. 带有弹跳学习率的梯度下降</h6><p>现在让我们在一个端到端的示例中应用所有这些。</p>
<h2 id="一个端到端的-SGD-示例"><a href="#一个端到端的-SGD-示例" class="headerlink" title="一个端到端的 SGD 示例"></a>一个端到端的 SGD 示例</h2><p>我们已经看到如何使用梯度来最小化我们的损失。现在是时候看一个 SGD 示例，并看看如何找到最小值来训练模型以更好地拟合数据。</p>
<p>让我们从一个简单的合成示例模型开始。想象一下，您正在测量过山车通过顶峰时的速度。它会开始快速，然后随着上坡而变慢；在顶部最慢，然后在下坡时再次加速。您想建立一个关于速度随时间变化的模型。如果您每秒手动测量速度 20 秒，它可能看起来像这样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time = torch.arange(<span class="number">0</span>,<span class="number">20</span>).<span class="built_in">float</span>(); time</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>,  <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>, <span class="number">12.</span>, <span class="number">13.</span>,</span><br><span class="line"> &gt; <span class="number">14.</span>, <span class="number">15.</span>, <span class="number">16.</span>, <span class="number">17.</span>, <span class="number">18.</span>, <span class="number">19.</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">speed = torch.randn(<span class="number">20</span>)*<span class="number">3</span> + <span class="number">0.75</span>*(time-<span class="number">9.5</span>)**<span class="number">2</span> + <span class="number">1</span></span><br><span class="line">plt.scatter(time,speed);</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in11.png" alt=""></p>
<p>我们添加了一些随机噪声，因为手动测量不够精确。这意味着很难回答问题：过山车的速度是多少？使用 SGD，我们可以尝试找到一个与我们的观察相匹配的函数。我们无法考虑每种可能的函数，所以让我们猜测它将是二次的；即，一个形式为<code>a*(time**2)+(b*time)+c</code>的函数。</p>
<p>我们希望清楚地区分函数的输入（我们测量过山车速度的时间）和其参数（定义<em>我们正在尝试的</em>二次函数的值）。因此，让我们将参数收集在一个参数中，从而在函数的签名中分离输入<code>t</code>和参数<code>params</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">t, params</span>):</span><br><span class="line">    a,b,c = params</span><br><span class="line">    <span class="keyword">return</span> a*(t**<span class="number">2</span>) + (b*t) + c</span><br></pre></td></tr></table></figure>
<p>换句话说，我们已经将找到最佳拟合数据的最佳函数的问题限制为找到最佳<em>二次</em>函数。这极大地简化了问题，因为每个二次函数都由三个参数<code>a</code>、<code>b</code>和<code>c</code>完全定义。因此，要找到最佳二次函数，我们只需要找到最佳的<code>a</code>、<code>b</code>和<code>c</code>的值。</p>
<p>如果我们可以解决二次函数的三个参数的问题，我们就能够对其他具有更多参数的更复杂函数应用相同的方法——比如神经网络。让我们先找到<code>f</code>的参数，然后我们将回来对 MNIST 数据集使用神经网络做同样的事情。</p>
<p>首先，我们需要定义“最佳”是什么意思。我们通过选择一个<em>损失函数</em>来精确定义这一点，该函数将根据预测和目标返回一个值，其中函数的较低值对应于“更好”的预测。对于连续数据，通常使用<em>均方误差</em>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mse</span>(<span class="params">preds, targets</span>): <span class="keyword">return</span> ((preds-targets)**<span class="number">2</span>).mean()</span><br></pre></td></tr></table></figure>
<p>现在，让我们按照我们的七步流程进行工作。</p>
<h3 id="第一步：初始化参数"><a href="#第一步：初始化参数" class="headerlink" title="第一步：初始化参数"></a>第一步：初始化参数</h3><p>首先，我们将参数初始化为随机值，并告诉 PyTorch 我们要使用<code>requires_grad_</code>跟踪它们的梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params = torch.randn(<span class="number">3</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<h3 id="第二步：计算预测"><a href="#第二步：计算预测" class="headerlink" title="第二步：计算预测"></a>第二步：计算预测</h3><p>接下来，我们计算预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preds = f(time, params)</span><br></pre></td></tr></table></figure>
<p>让我们创建一个小函数来查看我们的预测与目标的接近程度，并看一看：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_preds</span>(<span class="params">preds, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>: ax=plt.subplots()[<span class="number">1</span>]</span><br><span class="line">    ax.scatter(time, speed)</span><br><span class="line">    ax.scatter(time, to_np(preds), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    ax.set_ylim(-<span class="number">300</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_preds(preds)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in12.png" alt=""></p>
<p>这看起来并不接近——我们的随机参数表明过山车最终会倒退，因为我们有负速度！</p>
<h3 id="第一步：初始化参数-1"><a href="#第一步：初始化参数-1" class="headerlink" title="第一步：初始化参数"></a>第一步：初始化参数</h3><p>首先，我们将参数初始化为随机值，并告诉 PyTorch 我们要使用<code>requires_grad_</code>跟踪它们的梯度：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params = torch.randn(<span class="number">3</span>).requires_grad_()</span><br></pre></td></tr></table></figure>
<h3 id="第二步：计算预测-1"><a href="#第二步：计算预测-1" class="headerlink" title="第二步：计算预测"></a>第二步：计算预测</h3><p>接下来，我们计算预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">preds = f(time, params)</span><br></pre></td></tr></table></figure>
<p>让我们创建一个小函数来查看我们的预测与目标的接近程度，并看一看：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">show_preds</span>(<span class="params">preds, ax=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> ax <span class="keyword">is</span> <span class="literal">None</span>: ax=plt.subplots()[<span class="number">1</span>]</span><br><span class="line">    ax.scatter(time, speed)</span><br><span class="line">    ax.scatter(time, to_np(preds), color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    ax.set_ylim(-<span class="number">300</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_preds(preds)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in12.png" alt=""></p>
<p>这看起来并不接近——我们的随机参数表明过山车最终会倒退，因为我们有负速度！</p>
<h3 id="第三步：计算损失"><a href="#第三步：计算损失" class="headerlink" title="第三步：计算损失"></a>第三步：计算损失</h3><p>我们计算损失如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = mse(preds, speed)</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">25823.8086</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>我们的目标现在是改进这一点。为了做到这一点，我们需要知道梯度。</p>
<h3 id="第四步：计算梯度"><a href="#第四步：计算梯度" class="headerlink" title="第四步：计算梯度"></a>第四步：计算梯度</h3><p>下一步是计算梯度，或者近似参数需要如何改变：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">params.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">53195.8594</span>,  -<span class="number">3419.7146</span>,   -<span class="number">253.8908</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params.grad * <span class="number">1e-5</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.5320</span>, -<span class="number">0.0342</span>, -<span class="number">0.0025</span>])</span><br></pre></td></tr></table></figure>
<p>我们可以利用这些梯度来改进我们的参数。我们需要选择一个学习率（我们将在下一章中讨论如何在实践中做到这一点；现在，我们将使用 1e-5 或 0.00001）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">params</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([-<span class="number">0.7658</span>, -<span class="number">0.7506</span>,  <span class="number">1.3525</span>], requires_grad=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h3 id="第-5-步：调整权重"><a href="#第-5-步：调整权重" class="headerlink" title="第 5 步：调整权重"></a>第 5 步：调整权重</h3><p>现在我们需要根据刚刚计算的梯度更新参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1e-5</span></span><br><span class="line">params.data -= lr * params.grad.data</span><br><span class="line">params.grad = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<h1 id="Alexis-说-1"><a href="#Alexis-说-1" class="headerlink" title="Alexis 说"></a>Alexis 说</h1><p>理解这一点取决于记住最近的历史。为了计算梯度，我们在<code>loss</code>上调用<code>backward</code>。但是这个<code>loss</code>本身是通过<code>mse</code>计算的，而<code>mse</code>又以<code>preds</code>作为输入，<code>preds</code>是使用<code>f</code>计算的，<code>f</code>以<code>params</code>作为输入，<code>params</code>是我们最初调用<code>required_grads_</code>的对象，这是最初的调用，现在允许我们在<code>loss</code>上调用<code>backward</code>。这一系列函数调用代表了函数的数学组合，使得 PyTorch 能够在幕后使用微积分的链式法则来计算这些梯度。</p>
<p>让我们看看损失是否有所改善：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = f(time,params)</span><br><span class="line">mse(preds, speed)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">5435.5366</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>再看一下图表：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_preds(preds)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in13.png" alt=""></p>
<p>我们需要重复这个过程几次，所以我们将创建一个应用一步的函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">apply_step</span>(<span class="params">params, prn=<span class="literal">True</span></span>):</span><br><span class="line">    preds = f(time, params)</span><br><span class="line">    loss = mse(preds, speed)</span><br><span class="line">    loss.backward()</span><br><span class="line">    params.data -= lr * params.grad.data</span><br><span class="line">    params.grad = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> prn: <span class="built_in">print</span>(loss.item())</span><br><span class="line">    <span class="keyword">return</span> preds</span><br></pre></td></tr></table></figure>
<h3 id="第-6-步：重复这个过程"><a href="#第-6-步：重复这个过程" class="headerlink" title="第 6 步：重复这个过程"></a>第 6 步：重复这个过程</h3><p>现在我们进行迭代。通过循环和进行许多改进，我们希望达到一个好的结果：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>): apply_step(params)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5435.53662109375</span></span><br><span class="line"><span class="number">1577.4495849609375</span></span><br><span class="line"><span class="number">847.3780517578125</span></span><br><span class="line"><span class="number">709.22265625</span></span><br><span class="line"><span class="number">683.0757446289062</span></span><br><span class="line"><span class="number">678.12451171875</span></span><br><span class="line"><span class="number">677.1839599609375</span></span><br><span class="line"><span class="number">677.0025024414062</span></span><br><span class="line"><span class="number">676.96435546875</span></span><br><span class="line"><span class="number">676.9537353515625</span></span><br></pre></td></tr></table></figure>
<p>损失正在下降，正如我们所希望的！但仅仅看这些损失数字掩盖了一个事实，即每次迭代代表尝试一个完全不同的二次函数，以找到最佳可能的二次函数。如果我们不打印出损失函数，而是在每一步绘制函数，我们可以看到形状是如何接近我们的数据的最佳可能的二次函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">_,axs = plt.subplots(<span class="number">1</span>,<span class="number">4</span>,figsize=(<span class="number">12</span>,<span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> ax <span class="keyword">in</span> axs: show_preds(apply_step(params, <span class="literal">False</span>), ax)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in14.png" alt=""></p>
<h3 id="第-7-步：停止"><a href="#第-7-步：停止" class="headerlink" title="第 7 步：停止"></a>第 7 步：停止</h3><p>我们刚刚决定在任意选择的 10 个 epochs 后停止。在实践中，我们会观察训练和验证损失以及我们的指标，以决定何时停止，正如我们所讨论的那样。</p>
<h2 id="总结梯度下降"><a href="#总结梯度下降" class="headerlink" title="总结梯度下降"></a>总结梯度下降</h2><p>现在您已经看到每个步骤中发生的事情，让我们再次看一下我们的梯度下降过程的图形表示（图 4-5）并进行一个快速回顾。</p>
<p><img src="/image/dlcf_0405.png" alt="显示梯度下降步骤的图表"></p>
<h6 id="图-4-5-梯度下降过程"><a href="#图-4-5-梯度下降过程" class="headerlink" title="图 4-5. 梯度下降过程"></a>图 4-5. 梯度下降过程</h6><p>在开始时，我们模型的权重可以是随机的（从头开始训练）或来自预训练模型（迁移学习）。在第一种情况下，我们从输入得到的输出与我们想要的完全无关，即使在第二种情况下，预训练模型也可能不太擅长我们所针对的特定任务。因此，模型需要学习更好的权重。</p>
<p>我们首先将模型给出的输出与我们的目标进行比较（我们有标记数据，所以我们知道模型应该给出什么结果），使用一个<em>损失函数</em>，它返回一个数字，我们希望通过改进我们的权重使其尽可能低。为了做到这一点，我们从训练集中取出一些数据项（如图像）并将它们馈送给我们的模型。我们使用我们的损失函数比较相应的目标，我们得到的分数告诉我们我们的预测有多么错误。然后我们稍微改变权重使其稍微更好。</p>
<p>为了找出如何改变权重使损失稍微变好，我们使用微积分来计算<em>梯度</em>。（实际上，我们让 PyTorch 为我们做这个！）让我们考虑一个类比。想象一下你在山上迷路了，你的车停在最低点。为了找到回去的路，你可能会朝着随机方向走，但那可能不会有太大帮助。由于你知道你的车在最低点，你最好是往下走。通过始终朝着最陡峭的下坡方向迈出一步，你最终应该到达目的地。我们使用梯度的大小（即坡度的陡峭程度）来告诉我们应该迈多大一步；具体来说，我们将梯度乘以我们选择的一个称为<em>学习率</em>的数字来决定步长。然后我们<em>迭代</em>直到达到最低点，那将是我们的停车场；然后我们可以<em>停止</em>。</p>
<p>我们刚刚看到的所有内容都可以直接转换到 MNIST 数据集，除了损失函数。现在让我们看看如何定义一个好的训练目标。</p>
<h1 id="MNIST-损失函数"><a href="#MNIST-损失函数" class="headerlink" title="MNIST 损失函数"></a>MNIST 损失函数</h1><p>我们已经有了我们的<code>x</code>—也就是我们的自变量，图像本身。我们将它们全部连接成一个单一的张量，并且还将它们从矩阵列表（一个秩为 3 的张量）转换为向量列表（一个秩为 2 的张量）。我们可以使用<code>view</code>来做到这一点，<code>view</code>是一个 PyTorch 方法，可以改变张量的形状而不改变其内容。<code>-1</code>是<code>view</code>的一个特殊参数，意思是“使这个轴尽可能大以适应所有数据”：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_x = torch.cat([stacked_threes, stacked_sevens]).view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br></pre></td></tr></table></figure>
<p>我们需要为每张图片标记。我们将使用<code>1</code>表示 3，<code>0</code>表示 7：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_y = tensor([<span class="number">1</span>]*<span class="built_in">len</span>(threes) + [<span class="number">0</span>]*<span class="built_in">len</span>(sevens)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">train_x.shape,train_y.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">12396</span>, <span class="number">784</span>]), torch.Size([<span class="number">12396</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>在 PyTorch 中，当索引时，<code>Dataset</code>需要返回一个<code>(x,y)</code>元组。Python 提供了一个<code>zip</code>函数，当与<code>list</code>结合使用时，可以简单地实现这个功能：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dset = <span class="built_in">list</span>(<span class="built_in">zip</span>(train_x,train_y))</span><br><span class="line">x,y = dset[<span class="number">0</span>]</span><br><span class="line">x.shape,y</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">784</span>]), tensor([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-<span class="number">1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">valid_y = tensor([<span class="number">1</span>]*<span class="built_in">len</span>(valid_3_tens) + [<span class="number">0</span>]*<span class="built_in">len</span>(valid_7_tens)).unsqueeze(<span class="number">1</span>)</span><br><span class="line">valid_dset = <span class="built_in">list</span>(<span class="built_in">zip</span>(valid_x,valid_y))</span><br></pre></td></tr></table></figure>
<p>现在我们需要为每个像素（最初是随机的）分配一个权重（这是我们七步过程中的<em>初始化</em>步骤）：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_params</span>(<span class="params">size, std=<span class="number">1.0</span></span>): <span class="keyword">return</span> (torch.randn(size)*std).requires_grad_()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights = init_params((<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>函数<code>weights*pixels</code>不够灵活—当像素等于 0 时，它总是等于 0（即其<em>截距</em>为 0）。你可能还记得高中数学中线的公式是<code>y=w*x+b</code>；我们仍然需要<code>b</code>。我们也会将其初始化为一个随机数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bias = init_params(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>在神经网络中，方程<code>y=w*x+b</code>中的<code>w</code>被称为<em>权重</em>，<code>b</code>被称为<em>偏置</em>。权重和偏置一起构成<em>参数</em>。</p>
<h1 id="术语：参数"><a href="#术语：参数" class="headerlink" title="术语：参数"></a>术语：参数</h1><p>模型的<em>权重</em>和<em>偏置</em>。权重是方程<code>w*x+b</code>中的<code>w</code>，偏置是该方程中的<code>b</code>。</p>
<p>现在我们可以为一张图片计算一个预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(train_x[<span class="number">0</span>]*weights.T).<span class="built_in">sum</span>() + bias</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">20.2336</span>], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>虽然我们可以使用 Python 的<code>for</code>循环来计算每张图片的预测，但那将非常慢。因为 Python 循环不在 GPU 上运行，而且因为 Python 在一般情况下循环速度较慢，我们需要尽可能多地使用高级函数来表示模型中的计算。</p>
<p>在这种情况下，有一个非常方便的数学运算可以为矩阵的每一行计算<code>w*x</code>—它被称为<em>矩阵乘法</em>。图 4-6 展示了矩阵乘法的样子。</p>
<p><img src="/image/dlcf_0406.png" alt="矩阵乘法"></p>
<h6 id="图-4-6-矩阵乘法"><a href="#图-4-6-矩阵乘法" class="headerlink" title="图 4-6. 矩阵乘法"></a>图 4-6. 矩阵乘法</h6><p>这幅图展示了两个矩阵<code>A</code>和<code>B</code>相乘。结果的每个项目，我们称之为<code>AB</code>，包含了<code>A</code>的对应行的每个项目与<code>B</code>的对应列的每个项目相乘后相加。例如，第 1 行第 2 列（带有红色边框的黄色点）计算为<math alttext="a 下标 1，1 乘以 b 下标 1，2 加上 a 下标 1，2 乘以 b 下标 2，2">。如果您需要复习矩阵乘法，我们建议您查看 Khan Academy 的“矩阵乘法简介”，因为这是深度学习中最重要的数学运算。</p>
<p>在 Python 中，矩阵乘法用<code>@</code>运算符表示。让我们试一试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear1</span>(<span class="params">xb</span>): <span class="keyword">return</span> xb@weights + bias</span><br><span class="line">preds = linear1(train_x)</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="number">20.2336</span>],</span><br><span class="line">        [<span class="number">17.0644</span>],</span><br><span class="line">        [<span class="number">15.2384</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="number">18.3804</span>],</span><br><span class="line">        [<span class="number">23.8567</span>],</span><br><span class="line">        [<span class="number">28.6816</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>第一个元素与我们之前计算的相同，正如我们所期望的。这个方程<code>batch @ weights + bias</code>是任何神经网络的两个基本方程之一（另一个是<em>激活函数</em>，我们马上会看到）。</p>
<p>让我们检查我们的准确性。为了确定输出代表 3 还是 7，我们只需检查它是否大于 0，因此我们可以计算每个项目的准确性（使用广播，因此没有循环！）如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">corrects = (preds&gt;<span class="number">0.0</span>).<span class="built_in">float</span>() == train_y</span><br><span class="line">corrects</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">tensor([[ <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        ...,</span><br><span class="line">        [<span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">corrects.<span class="built_in">float</span>().mean().item()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4912068545818329</span></span><br></pre></td></tr></table></figure>
<p>现在让我们看看一个权重的微小变化对准确性的影响是什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">weights[<span class="number">0</span>] *= <span class="number">1.0001</span></span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = linear1(train_x)</span><br><span class="line">((preds&gt;<span class="number">0.0</span>).<span class="built_in">float</span>() == train_y).<span class="built_in">float</span>().mean().item()</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4912068545818329</span></span><br></pre></td></tr></table></figure>
<p>正如我们所看到的，我们需要梯度来通过 SGD 改进我们的模型，为了计算梯度，我们需要一个<em>损失函数</em>，它代表了我们的模型有多好。这是因为梯度是损失函数如何随着对权重的微小调整而变化的度量。</p>
<p>因此，我们需要选择一个损失函数。显而易见的方法是使用准确性作为我们的度量标准，也作为我们的损失函数。在这种情况下，我们将为每个图像计算我们的预测，收集这些值以计算总体准确性，然后计算每个权重相对于总体准确性的梯度。</p>
<p>不幸的是，我们在这里有一个重要的技术问题。函数的梯度是其<em>斜率</em>，或者是其陡峭程度，可以定义为<em>上升与下降</em>——也就是说，函数值上升或下降的幅度，除以我们改变输入的幅度。我们可以用数学方式写成：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(y_new – y_old) / (x_new – x_old)</span><br></pre></td></tr></table></figure>
<p>当<code>x_new</code>非常类似于<code>x_old</code>时，这给出了梯度的良好近似，这意味着它们的差异非常小。但是，只有当预测从 3 变为 7，或者反之时，准确性才会发生变化。问题在于，从<code>x_old</code>到<code>x_new</code>的权重的微小变化不太可能导致任何预测发生变化，因此<code>(y_new - y_old)</code>几乎总是为 0。换句话说，梯度几乎在任何地方都为 0。</p>
<p>权重值的微小变化通常不会改变准确性。这意味着使用准确性作为损失函数是没有用的——如果我们这样做，大多数时候我们的梯度将为 0，模型将无法从该数字中学习。</p>
<h1 id="Sylvain-说"><a href="#Sylvain-说" class="headerlink" title="Sylvain 说"></a>Sylvain 说</h1><p>在数学术语中，准确性是一个几乎在任何地方都是常数的函数（除了阈值 0.5），因此它的导数几乎在任何地方都是零（在阈值处为无穷大）。这将导致梯度为 0 或无穷大，这对于更新模型是没有用的。</p>
<p>相反，我们需要一个损失函数，当我们的权重导致稍微更好的预测时，给出稍微更好的损失。那么，“稍微更好的预测”具体是什么样呢？在这种情况下，这意味着如果正确答案是 3，则分数稍高，或者如果正确答案是 7，则分数稍低。</p>
<p>现在让我们编写这样一个函数。它是什么形式？</p>
<p>损失函数接收的不是图像本身，而是模型的预测。因此，让我们做一个参数<code>prds</code>，值在 0 和 1 之间，其中每个值是图像是 3 的预测。它是一个矢量（即，一个秩-1 张量），索引在图像上。</p>
<p>损失函数的目的是衡量预测值与真实值之间的差异，即目标（又称标签）。因此，让我们再做一个参数<code>trgts</code>，其值为 0 或 1，告诉图像实际上是 3 还是不是 3。它也是一个矢量（即，另一个秩-1 张量），索引在图像上。</p>
<p>例如，假设我们有三幅图像，我们知道其中一幅是 3，一幅是 7，一幅是 3。假设我们的模型以高置信度（<code>0.9</code>）预测第一幅是 3，以轻微置信度（<code>0.4</code>）预测第二幅是 7，以公平置信度（<code>0.2</code>），但是错误地预测最后一幅是 7。这意味着我们的损失函数将接收这些值作为其输入：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trgts  = tensor([<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">prds   = tensor([<span class="number">0.9</span>, <span class="number">0.4</span>, <span class="number">0.2</span>])</span><br></pre></td></tr></table></figure>
<p>这是一个测量<code>predictions</code>和<code>targets</code>之间距离的损失函数的第一次尝试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    <span class="keyword">return</span> torch.where(targets==<span class="number">1</span>, <span class="number">1</span>-predictions, predictions).mean()</span><br></pre></td></tr></table></figure>
<p>我们正在使用一个新函数，<code>torch.where(a,b,c)</code>。这与运行列表推导<code>[b[i] if a[i] else c[i] for i in range(len(a))]</code>相同，只是它在张量上运行，以 C/CUDA 速度运行。简单来说，这个函数将衡量每个预测离 1 有多远，如果应该是 1 的话，以及它离 0 有多远，如果应该是 0 的话，然后它将取所有这些距离的平均值。</p>
<h1 id="阅读文档"><a href="#阅读文档" class="headerlink" title="阅读文档"></a>阅读文档</h1><p>学习 PyTorch 这样的函数很重要，因为在 Python 中循环张量的速度是 Python 速度，而不是 C/CUDA 速度！现在尝试运行<code>help(torch.where)</code>来阅读此函数的文档，或者更好的是，在 PyTorch 文档站点上查找。</p>
<p>让我们在我们的<code>prds</code>和<code>trgts</code>上尝试一下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.where(trgts==<span class="number">1</span>, <span class="number">1</span>-prds, prds)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([<span class="number">0.1000</span>, <span class="number">0.4000</span>, <span class="number">0.8000</span>])</span><br></pre></td></tr></table></figure>
<p>您可以看到，当预测更准确时，当准确预测更自信时（绝对值更高），以及当不准确预测更不自信时，此函数返回较低的数字。在 PyTorch 中，我们始终假设损失函数的较低值更好。由于我们需要一个标量作为最终损失，<code>mnist_loss</code>取前一个张量的平均值：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist_loss(prds,trgts)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.4333</span>)</span><br></pre></td></tr></table></figure>
<p>例如，如果我们将对一个“错误”目标的预测从<code>0.2</code>更改为<code>0.8</code>，损失将减少，表明这是一个更好的预测：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mnist_loss(tensor([<span class="number">0.9</span>, <span class="number">0.4</span>, <span class="number">0.8</span>]),trgts)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.2333</span>)</span><br></pre></td></tr></table></figure>
<p><code>mnist_loss</code>当前定义的一个问题是它假设预测总是在 0 和 1 之间。因此，我们需要确保这实际上是这种情况！恰好有一个函数可以做到这一点，让我们来看看。</p>
<h2 id="Sigmoid"><a href="#Sigmoid" class="headerlink" title="Sigmoid"></a>Sigmoid</h2><p><code>sigmoid</code>函数总是输出一个介于 0 和 1 之间的数字。它的定义如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>): <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+torch.exp(-x))</span><br></pre></td></tr></table></figure>
<p>PyTorch 为我们定义了一个加速版本，所以我们不需要自己的。这是深度学习中一个重要的函数，因为我们经常希望确保数值在 0 和 1 之间。它看起来是这样的：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(torch.sigmoid, title=<span class="string">&#x27;Sigmoid&#x27;</span>, <span class="built_in">min</span>=-<span class="number">4</span>, <span class="built_in">max</span>=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in15.png" alt=""></p>
<p>正如您所看到的，它接受任何输入值，正数或负数，并将其压缩为 0 和 1 之间的输出值。它还是一个只上升的平滑曲线，这使得 SGD 更容易找到有意义的梯度。</p>
<p>让我们更新<code>mnist_loss</code>，首先对输入应用<code>sigmoid</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mnist_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    predictions = predictions.sigmoid()</span><br><span class="line">    <span class="keyword">return</span> torch.where(targets==<span class="number">1</span>, <span class="number">1</span>-predictions, predictions).mean()</span><br></pre></td></tr></table></figure>
<p>现在我们可以确信我们的损失函数将起作用，即使预测不在 0 和 1 之间。唯一需要的是更高的预测对应更高的置信度。</p>
<p>定义了一个损失函数，现在是一个好时机回顾为什么这样做。毕竟，我们已经有了一个度量标准，即整体准确率。那么为什么我们定义了一个损失？</p>
<p>关键区别在于指标用于驱动人类理解，而损失用于驱动自动学习。为了驱动自动学习，损失必须是一个具有有意义导数的函数。它不能有大的平坦部分和大的跳跃，而必须是相当平滑的。这就是为什么我们设计了一个损失函数，可以对置信水平的小变化做出响应。这个要求意味着有时它实际上并不完全反映我们试图实现的目标，而是我们真正目标和一个可以使用其梯度进行优化的函数之间的妥协。损失函数是针对数据集中的每个项目计算的，然后在时代结束时，所有损失值都被平均，整体均值被报告为时代。</p>
<p>另一方面，指标是我们关心的数字。这些是在每个时代结束时打印的值，告诉我们我们的模型表现如何。重要的是，我们学会关注这些指标，而不是损失，来评估模型的性能。</p>
<h2 id="SGD-和小批次"><a href="#SGD-和小批次" class="headerlink" title="SGD 和小批次"></a>SGD 和小批次</h2><p>现在我们有了一个适合驱动 SGD 的损失函数，我们可以考虑学习过程的下一阶段涉及的一些细节，即根据梯度改变或更新权重。这被称为<em>优化步骤</em>。</p>
<p>要进行优化步骤，我们需要计算一个或多个数据项的损失。我们应该使用多少？我们可以为整个数据集计算并取平均值，或者可以为单个数据项计算。但这两种方法都不理想。为整个数据集计算将需要很长时间。为单个数据项计算将不会使用太多信息，因此会导致不精确和不稳定的梯度。您将费力更新权重，但只考虑这将如何改善模型在该单个数据项上的性能。</p>
<p>因此，我们做出妥协：我们一次计算几个数据项的平均损失。这被称为<em>小批次</em>。小批次中的数据项数量称为<em>批次大小</em>。较大的批次大小意味着您将从损失函数中获得更准确和稳定的数据集梯度估计，但这将需要更长时间，并且您将在每个时代处理较少的小批次。选择一个好的批次大小是您作为深度学习从业者需要做出的决定之一，以便快速准确地训练您的模型。我们将在本书中讨论如何做出这个选择。</p>
<p>使用小批次而不是在单个数据项上计算梯度的另一个很好的理由是，实际上，我们几乎总是在加速器上进行训练，例如 GPU。这些加速器只有在一次有很多工作要做时才能表现良好，因此如果我们可以给它们很多数据项来处理，这将是有帮助的。使用小批次是实现这一目标的最佳方法之一。但是，如果您一次给它们太多数据来处理，它们会耗尽内存——让 GPU 保持愉快也是棘手的！</p>
<p>正如您在第二章中关于数据增强的讨论中所看到的，如果我们在训练过程中可以改变一些东西，我们会获得更好的泛化能力。我们可以改变的一个简单而有效的事情是将哪些数据项放入每个小批次。我们通常不是简单地按顺序枚举我们的数据集，而是在每个时代之前随机洗牌，然后创建小批次。PyTorch 和 fastai 提供了一个类，可以为您执行洗牌和小批次整理，称为<code>DataLoader</code>。</p>
<p><code>DataLoader</code>可以将任何 Python 集合转换为一个迭代器，用于生成多个批次，就像这样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">coll = <span class="built_in">range</span>(<span class="number">15</span>)</span><br><span class="line">dl = DataLoader(coll, batch_size=<span class="number">5</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(dl)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[tensor([ <span class="number">3</span>, <span class="number">12</span>,  <span class="number">8</span>, <span class="number">10</span>,  <span class="number">2</span>]),</span><br><span class="line"> tensor([ <span class="number">9</span>,  <span class="number">4</span>,  <span class="number">7</span>, <span class="number">14</span>,  <span class="number">5</span>]),</span><br><span class="line"> tensor([ <span class="number">1</span>, <span class="number">13</span>,  <span class="number">0</span>,  <span class="number">6</span>, <span class="number">11</span>])]</span><br></pre></td></tr></table></figure>
<p>对于训练模型，我们不只是想要任何 Python 集合，而是一个包含独立和相关变量（模型的输入和目标）的集合。包含独立和相关变量元组的集合在 PyTorch 中被称为<code>Dataset</code>。这是一个极其简单的<code>Dataset</code>的示例：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ds = L(<span class="built_in">enumerate</span>(string.ascii_lowercase))</span><br><span class="line">ds</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(<span class="comment">#26) [(0, &#x27;a&#x27;),(1, &#x27;b&#x27;),(2, &#x27;c&#x27;),(3, &#x27;d&#x27;),(4, &#x27;e&#x27;),(5, &#x27;f&#x27;),(6, &#x27;g&#x27;),(7,</span></span><br><span class="line"> &gt; <span class="string">&#x27;h&#x27;</span>),(<span class="number">8</span>, <span class="string">&#x27;i&#x27;</span>),(<span class="number">9</span>, <span class="string">&#x27;j&#x27;</span>)...]</span><br></pre></td></tr></table></figure>
<p>当我们将<code>Dataset</code>传递给<code>DataLoader</code>时，我们将得到许多批次，它们本身是表示独立和相关变量批次的张量元组：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dl = DataLoader(ds, batch_size=<span class="number">6</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">list</span>(dl)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[(tensor([<span class="number">17</span>, <span class="number">18</span>, <span class="number">10</span>, <span class="number">22</span>,  <span class="number">8</span>, <span class="number">14</span>]), (<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;i&#x27;</span>, <span class="string">&#x27;o&#x27;</span>)),</span><br><span class="line"> (tensor([<span class="number">20</span>, <span class="number">15</span>,  <span class="number">9</span>, <span class="number">13</span>, <span class="number">21</span>, <span class="number">12</span>]), (<span class="string">&#x27;u&#x27;</span>, <span class="string">&#x27;p&#x27;</span>, <span class="string">&#x27;j&#x27;</span>, <span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;m&#x27;</span>)),</span><br><span class="line"> (tensor([ <span class="number">7</span>, <span class="number">25</span>,  <span class="number">6</span>,  <span class="number">5</span>, <span class="number">11</span>, <span class="number">23</span>]), (<span class="string">&#x27;h&#x27;</span>, <span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;l&#x27;</span>, <span class="string">&#x27;x&#x27;</span>)),</span><br><span class="line"> (tensor([ <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">0</span>, <span class="number">24</span>, <span class="number">19</span>, <span class="number">16</span>]), (<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;y&#x27;</span>, <span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;q&#x27;</span>)),</span><br><span class="line"> (tensor([<span class="number">2</span>, <span class="number">4</span>]), (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;e&#x27;</span>))]</span><br></pre></td></tr></table></figure>
<p>我们现在准备为使用 SGD 的模型编写我们的第一个训练循环！</p>
<h1 id="把所有东西放在一起"><a href="#把所有东西放在一起" class="headerlink" title="把所有东西放在一起"></a>把所有东西放在一起</h1><p>是时候实现我们在图 4-1 中看到的过程了。在代码中，我们的过程将为每个时期实现类似于这样的东西：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> x,y <span class="keyword">in</span> dl:</span><br><span class="line">    pred = model(x)</span><br><span class="line">    loss = loss_func(pred, y)</span><br><span class="line">    loss.backward()</span><br><span class="line">    parameters -= parameters.grad * lr</span><br></pre></td></tr></table></figure>
<p>首先，让我们重新初始化我们的参数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights = init_params((<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>))</span><br><span class="line">bias = init_params(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p><code>DataLoader</code>可以从<code>Dataset</code>创建：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dl = DataLoader(dset, batch_size=<span class="number">256</span>)</span><br><span class="line">xb,yb = first(dl)</span><br><span class="line">xb.shape,yb.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">256</span>, <span class="number">784</span>]), torch.Size([<span class="number">256</span>, <span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>我们将对验证集执行相同的操作：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">valid_dl = DataLoader(valid_dset, batch_size=<span class="number">256</span>)</span><br></pre></td></tr></table></figure>
<p>让我们创建一个大小为 4 的小批量进行测试：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">batch = train_x[:<span class="number">4</span>]</span><br><span class="line">batch.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([<span class="number">4</span>, <span class="number">784</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">preds = linear1(batch)</span><br><span class="line">preds</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[-<span class="number">11.1002</span>],</span><br><span class="line">        [  <span class="number">5.9263</span>],</span><br><span class="line">        [  <span class="number">9.9627</span>],</span><br><span class="line">        [ -<span class="number">8.1484</span>]], grad_fn=&lt;AddBackward0&gt;)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = mnist_loss(preds, train_y[:<span class="number">4</span>])</span><br><span class="line">loss</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.5006</span>, grad_fn=&lt;MeanBackward0&gt;)</span><br></pre></td></tr></table></figure>
<p>现在我们可以计算梯度了：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br><span class="line">weights.grad.shape,weights.grad.mean(),bias.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">784</span>, <span class="number">1</span>]), tensor(-<span class="number">0.0001</span>), tensor([-<span class="number">0.0008</span>]))</span><br></pre></td></tr></table></figure>
<p>让我们把所有这些放在一个函数中：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calc_grad</span>(<span class="params">xb, yb, model</span>):</span><br><span class="line">    preds = model(xb)</span><br><span class="line">    loss = mnist_loss(preds, yb)</span><br><span class="line">    loss.backward()</span><br></pre></td></tr></table></figure>
<p>并测试它：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_grad(batch, train_y[:<span class="number">4</span>], linear1)</span><br><span class="line">weights.grad.mean(),bias.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(-<span class="number">0.0002</span>), tensor([-<span class="number">0.0015</span>]))</span><br></pre></td></tr></table></figure>
<p>但是看看如果我们调用两次会发生什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calc_grad(batch, train_y[:<span class="number">4</span>], linear1)</span><br><span class="line">weights.grad.mean(),bias.grad</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(tensor(-<span class="number">0.0003</span>), tensor([-<span class="number">0.0023</span>]))</span><br></pre></td></tr></table></figure>
<p>梯度已经改变了！这是因为<code>loss.backward</code> <em>添加</em>了<code>loss</code>的梯度到当前存储的任何梯度中。因此，我们首先必须将当前梯度设置为 0：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">weights.grad.zero_()</span><br><span class="line">bias.grad.zero_();</span><br></pre></td></tr></table></figure>
<h1 id="原地操作"><a href="#原地操作" class="headerlink" title="原地操作"></a>原地操作</h1><p>PyTorch 中以下划线结尾的方法会<em>原地</em>修改它们的对象。例如，<code>bias.zero_</code>会将张量<code>bias</code>的所有元素设置为 0。</p>
<p>我们唯一剩下的步骤是根据梯度和学习率更新权重和偏差。当我们这样做时，我们必须告诉 PyTorch 不要对这一步骤进行梯度计算，否则当我们尝试在下一个批次计算导数时会变得混乱！如果我们将张量的<code>data</code>属性赋值，PyTorch 将不会对该步骤进行梯度计算。这是我们用于一个时期的基本训练循环：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">model, lr, params</span>):</span><br><span class="line">    <span class="keyword">for</span> xb,yb <span class="keyword">in</span> dl:</span><br><span class="line">        calc_grad(xb, yb, model)</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> params:</span><br><span class="line">            p.data -= p.grad*lr</span><br><span class="line">            p.grad.zero_()</span><br></pre></td></tr></table></figure>
<p>我们还想通过查看验证集的准确性来检查我们的表现。要决定输出是否代表 3 或 7，我们只需检查它是否大于 0。因此，我们可以计算每个项目的准确性（使用广播，所以没有循环！）如下：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(preds&gt;<span class="number">0.0</span>).<span class="built_in">float</span>() == train_y[:<span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tensor([[<span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<p>这给了我们计算验证准确性的这个函数：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">batch_accuracy</span>(<span class="params">xb, yb</span>):</span><br><span class="line">    preds = xb.sigmoid()</span><br><span class="line">    correct = (preds&gt;<span class="number">0.5</span>) == yb</span><br><span class="line">    <span class="keyword">return</span> correct.<span class="built_in">float</span>().mean()</span><br></pre></td></tr></table></figure>
<p>我们可以检查它是否有效：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch_accuracy(linear1(batch), train_y[:<span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor(<span class="number">0.5000</span>)</span><br></pre></td></tr></table></figure>
<p>然后把批次放在一起：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">validate_epoch</span>(<span class="params">model</span>):</span><br><span class="line">    accs = [batch_accuracy(model(xb), yb) <span class="keyword">for</span> xb,yb <span class="keyword">in</span> valid_dl]</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">round</span>(torch.stack(accs).mean().item(), <span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">validate_epoch(linear1)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.5219</span></span><br></pre></td></tr></table></figure>
<p>这是我们的起点。让我们训练一个时期，看看准确性是否提高：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = <span class="number">1.</span></span><br><span class="line">params = weights,bias</span><br><span class="line">train_epoch(linear1, lr, params)</span><br><span class="line">validate_epoch(linear1)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.6883</span></span><br></pre></td></tr></table></figure>
<p>然后再做几次：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    train_epoch(linear1, lr, params)</span><br><span class="line">    <span class="built_in">print</span>(validate_epoch(linear1), end=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.8314</span> <span class="number">0.9017</span> <span class="number">0.9227</span> <span class="number">0.9349</span> <span class="number">0.9438</span> <span class="number">0.9501</span> <span class="number">0.9535</span> <span class="number">0.9564</span> <span class="number">0.9594</span> <span class="number">0.9618</span> <span class="number">0.9613</span></span><br><span class="line"> &gt; <span class="number">0.9638</span> <span class="number">0.9643</span> <span class="number">0.9652</span> <span class="number">0.9662</span> <span class="number">0.9677</span> <span class="number">0.9687</span> <span class="number">0.9691</span> <span class="number">0.9691</span> <span class="number">0.9696</span></span><br></pre></td></tr></table></figure>
<p>看起来不错！我们的准确性已经接近“像素相似性”方法的准确性，我们已经创建了一个通用的基础可以构建。我们的下一步将是创建一个将处理 SGD 步骤的对象。在 PyTorch 中，它被称为<em>优化器</em>。</p>
<h2 id="创建一个优化器"><a href="#创建一个优化器" class="headerlink" title="创建一个优化器"></a>创建一个优化器</h2><p>因为这是一个如此通用的基础，PyTorch 提供了一些有用的类来使实现更容易。我们可以做的第一件事是用 PyTorch 的<code>nn.Linear</code>模块替换我们的<code>linear</code>函数。<em>模块</em>是从 PyTorch <code>nn.Module</code>类继承的类的对象。这个类的对象的行为与标准 Python 函数完全相同，您可以使用括号调用它们，它们将返回模型的激活。</p>
<p><code>nn.Linear</code>做的事情与我们的<code>init_params</code>和<code>linear</code>一样。它包含了<em>权重</em>和<em>偏差</em>在一个单独的类中。这是我们如何复制上一节中的模型：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">linear_model = nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>每个 PyTorch 模块都知道它有哪些可以训练的参数；它们可以通过<code>parameters</code>方法获得：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w,b = linear_model.parameters()</span><br><span class="line">w.shape,b.shape</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(torch.Size([<span class="number">1</span>, <span class="number">784</span>]), torch.Size([<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>我们可以使用这些信息创建一个优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicOptim</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,params,lr</span>): <span class="variable language_">self</span>.params,<span class="variable language_">self</span>.lr = <span class="built_in">list</span>(params),lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">step</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.params: p.data -= p.grad.data * <span class="variable language_">self</span>.lr</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">zero_grad</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="variable language_">self</span>.params: p.grad = <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>我们可以通过传入模型的参数来创建优化器：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opt = BasicOptim(linear_model.parameters(), lr)</span><br></pre></td></tr></table></figure>
<p>我们的训练循环现在可以简化：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_epoch</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="keyword">for</span> xb,yb <span class="keyword">in</span> dl:</span><br><span class="line">        calc_grad(xb, yb, model)</span><br><span class="line">        opt.step()</span><br><span class="line">        opt.zero_grad()</span><br></pre></td></tr></table></figure>
<p>我们的验证函数不需要任何更改：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">validate_epoch(linear_model)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4157</span></span><br></pre></td></tr></table></figure>
<p>让我们把我们的小训练循环放在一个函数中，让事情变得更简单：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_model</span>(<span class="params">model, epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">        train_epoch(model)</span><br><span class="line">        <span class="built_in">print</span>(validate_epoch(model), end=<span class="string">&#x27; &#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>结果与上一节相同：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_model(linear_model, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4932</span> <span class="number">0.8618</span> <span class="number">0.8203</span> <span class="number">0.9102</span> <span class="number">0.9331</span> <span class="number">0.9468</span> <span class="number">0.9555</span> <span class="number">0.9629</span> <span class="number">0.9658</span> <span class="number">0.9673</span> <span class="number">0.9687</span></span><br><span class="line"> &gt; <span class="number">0.9707</span> <span class="number">0.9726</span> <span class="number">0.9751</span> <span class="number">0.9761</span> <span class="number">0.9761</span> <span class="number">0.9775</span> <span class="number">0.978</span> <span class="number">0.9785</span> <span class="number">0.9785</span></span><br></pre></td></tr></table></figure>
<p>fastai 提供了<code>SGD</code>类，默认情况下与我们的<code>BasicOptim</code>做相同的事情：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">linear_model = nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>)</span><br><span class="line">opt = SGD(linear_model.parameters(), lr)</span><br><span class="line">train_model(linear_model, <span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.4932</span> <span class="number">0.852</span> <span class="number">0.8335</span> <span class="number">0.9116</span> <span class="number">0.9326</span> <span class="number">0.9473</span> <span class="number">0.9555</span> <span class="number">0.9624</span> <span class="number">0.9648</span> <span class="number">0.9668</span> <span class="number">0.9692</span></span><br><span class="line"> &gt; <span class="number">0.9712</span> <span class="number">0.9731</span> <span class="number">0.9746</span> <span class="number">0.9761</span> <span class="number">0.9765</span> <span class="number">0.9775</span> <span class="number">0.978</span> <span class="number">0.9785</span> <span class="number">0.9785</span></span><br></pre></td></tr></table></figure>
<p>fastai 还提供了<code>Learner.fit</code>，我们可以使用它来代替<code>train_model</code>。要创建一个<code>Learner</code>，我们首先需要创建一个<code>DataLoaders</code>，通过传入我们的训练和验证<code>DataLoader</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dls = DataLoaders(dl, valid_dl)</span><br></pre></td></tr></table></figure>
<p>要创建一个<code>Learner</code>而不使用应用程序（如<code>cnn_learner</code>），我们需要传入本章中创建的所有元素：<code>DataLoaders</code>，模型，优化函数（将传递参数），损失函数，以及可选的任何要打印的指标：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = Learner(dls, nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">1</span>), opt_func=SGD,</span><br><span class="line">                loss_func=mnist_loss, metrics=batch_accuracy)</span><br></pre></td></tr></table></figure>
<p>现在我们可以调用<code>fit</code>：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit(<span class="number">10</span>, lr=lr)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>epoch</th>
<th>train_loss</th>
<th>valid_loss</th>
<th>batch_accuracy</th>
<th>time</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.636857</td>
<td>0.503549</td>
<td>0.495584</td>
<td>00:00</td>
</tr>
<tr>
<td>1</td>
<td>0.545725</td>
<td>0.170281</td>
<td>0.866045</td>
<td>00:00</td>
</tr>
<tr>
<td>2</td>
<td>0.199223</td>
<td>0.184893</td>
<td>0.831207</td>
<td>00:00</td>
</tr>
<tr>
<td>3</td>
<td>0.086580</td>
<td>0.107836</td>
<td>0.911187</td>
<td>00:00</td>
</tr>
<tr>
<td>4</td>
<td>0.045185</td>
<td>0.078481</td>
<td>0.932777</td>
<td>00:00</td>
</tr>
<tr>
<td>5</td>
<td>0.029108</td>
<td>0.062792</td>
<td>0.946516</td>
<td>00:00</td>
</tr>
<tr>
<td>6</td>
<td>0.022560</td>
<td>0.053017</td>
<td>0.955348</td>
<td>00:00</td>
</tr>
<tr>
<td>7</td>
<td>0.019687</td>
<td>0.046500</td>
<td>0.962218</td>
<td>00:00</td>
</tr>
<tr>
<td>8</td>
<td>0.018252</td>
<td>0.041929</td>
<td>0.965162</td>
<td>00:00</td>
</tr>
<tr>
<td>9</td>
<td>0.017402</td>
<td>0.038573</td>
<td>0.967615</td>
<td>00:00</td>
</tr>
</tbody>
</table>
</div>
<p>正如您所看到的，PyTorch 和 fastai 类并没有什么神奇之处。它们只是方便的预打包部件，使您的生活变得更轻松！（它们还提供了许多我们将在未来章节中使用的额外功能。）</p>
<p>有了这些类，我们现在可以用神经网络替换我们的线性模型。</p>
<h1 id="添加非线性"><a href="#添加非线性" class="headerlink" title="添加非线性"></a>添加非线性</h1><p>到目前为止，我们已经有了一个优化函数的一般过程，并且我们已经在一个无聊的函数上尝试了它：一个简单的线性分类器。线性分类器在能做什么方面受到限制。为了使其更复杂一些（并且能够处理更多任务），我们需要在两个线性分类器之间添加一些非线性（即与 ax+b 不同的东西）——这就是给我们神经网络的东西。</p>
<p>这是一个基本神经网络的完整定义：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">simple_net</span>(<span class="params">xb</span>):</span><br><span class="line">    res = xb@w1 + b1</span><br><span class="line">    res = res.<span class="built_in">max</span>(tensor(<span class="number">0.0</span>))</span><br><span class="line">    res = res@w2 + b2</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>
<p>就是这样！在<code>simple_net</code>中，我们只有两个线性分类器，它们之间有一个<code>max</code>函数。</p>
<p>在这里，<code>w1</code>和<code>w2</code>是权重张量，<code>b1</code>和<code>b2</code>是偏置张量；也就是说，这些参数最初是随机初始化的，就像我们在上一节中所做的一样：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w1 = init_params((<span class="number">28</span>*<span class="number">28</span>,<span class="number">30</span>))</span><br><span class="line">b1 = init_params(<span class="number">30</span>)</span><br><span class="line">w2 = init_params((<span class="number">30</span>,<span class="number">1</span>))</span><br><span class="line">b2 = init_params(<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>关键点是<code>w1</code>有 30 个输出激活（这意味着<code>w2</code>必须有 30 个输入激活，以便匹配）。这意味着第一层可以构建 30 个不同的特征，每个特征代表不同的像素混合。您可以将<code>30</code>更改为任何您喜欢的数字，以使模型更复杂或更简单。</p>
<p>那个小函数<code>res.max(tensor(0.0))</code>被称为<em>修正线性单元</em>，也被称为<em>ReLU</em>。我们认为我们都可以同意<em>修正线性单元</em>听起来相当花哨和复杂…但实际上，它不过是<code>res.max(tensor(0.0))</code>——换句话说，用零替换每个负数。这个微小的函数在 PyTorch 中也可以作为<code>F.relu</code>使用：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot_function(F.relu)</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in16.png" alt=""></p>
<h1 id="Jeremy-说"><a href="#Jeremy-说" class="headerlink" title="Jeremy 说"></a>Jeremy 说</h1><p>深度学习中有大量行话，包括<em>修正线性单元</em>等术语。绝大多数这些行话并不比我们在这个例子中看到的一行代码更复杂。事实是，学术界为了发表论文，他们需要让论文听起来尽可能令人印象深刻和复杂。他们通过引入行话来实现这一点。不幸的是，这导致该领域变得比应该更加令人生畏和难以进入。您确实需要学习这些行话，因为否则论文和教程对您来说将毫无意义。但这并不意味着您必须觉得这些行话令人生畏。只需记住，当您遇到以前未见过的单词或短语时，它几乎肯定是指一个非常简单的概念。</p>
<p>基本思想是通过使用更多的线性层，我们的模型可以进行更多的计算，从而模拟更复杂的函数。但是，直接将一个线性布局放在另一个线性布局之后是没有意义的，因为当我们将事物相乘然后多次相加时，可以用不同的事物相乘然后只相加一次来替代！也就是说，一系列任意数量的线性层可以被替换为具有不同参数集的单个线性层。</p>
<p>但是，如果我们在它们之间放置一个非线性函数，比如<code>max</code>，这就不再成立了。现在每个线性层都有点解耦，可以做自己有用的工作。<code>max</code>函数特别有趣，因为它作为一个简单的<code>if</code>语句运行。</p>
<h1 id="Sylvain-说-1"><a href="#Sylvain-说-1" class="headerlink" title="Sylvain 说"></a>Sylvain 说</h1><p>数学上，我们说两个线性函数的组合是另一个线性函数。因此，我们可以堆叠任意多个线性分类器在一起，而它们之间没有非线性函数，这将与一个线性分类器相同。</p>
<p>令人惊讶的是，可以数学证明这个小函数可以解决任何可计算问题，只要你能找到<code>w1</code>和<code>w2</code>的正确参数，并且使这些矩阵足够大。对于任何任意波动的函数，我们可以将其近似为一堆连接在一起的线条；为了使其更接近波动函数，我们只需使用更短的线条。这被称为<em>通用逼近定理</em>。我们这里的三行代码被称为<em>层</em>。第一和第三行被称为<em>线性层</em>，第二行代码被称为<em>非线性</em>或<em>激活函数</em>。</p>
<p>就像在前一节中一样，我们可以利用 PyTorch 简化这段代码：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">simple_net = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">28</span>*<span class="number">28</span>,<span class="number">30</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">30</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p><code>nn.Sequential</code>创建一个模块，依次调用列出的每个层或函数。</p>
<p><code>nn.ReLU</code>是一个 PyTorch 模块，与<code>F.relu</code>函数完全相同。大多数可以出现在模型中的函数也有相同的模块形式。通常，只需将<code>F</code>替换为<code>nn</code>并更改大小写。在使用<code>nn.Sequential</code>时，PyTorch 要求我们使用模块版本。由于模块是类，我们必须实例化它们，这就是为什么在这个例子中看到<code>nn.ReLU</code>。</p>
<p>因为<code>nn.Sequential</code>是一个模块，我们可以获取它的参数，它将返回它包含的所有模块的所有参数的列表。让我们试一试！由于这是一个更深层的模型，我们将使用更低的学习率和更多的周期：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">learn = Learner(dls, simple_net, opt_func=SGD,</span><br><span class="line">                loss_func=mnist_loss, metrics=batch_accuracy)</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.fit(<span class="number">40</span>, <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>我们这里不展示 40 行输出，以节省空间；训练过程记录在<code>learn.recorder</code>中，输出表存储在<code>values</code>属性中，因此我们可以绘制训练过程中的准确性：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(L(learn.recorder.values).itemgot(<span class="number">2</span>));</span><br></pre></td></tr></table></figure>
<p><img src="/image/dlcf_04in17.png" alt=""></p>
<p>我们可以查看最终的准确性：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">learn.recorder.values[-<span class="number">1</span>][<span class="number">2</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.982826292514801</span></span><br></pre></td></tr></table></figure>
<p>在这一点上，我们有一些非常神奇的东西：</p>
<ul>
<li><p>给定正确的参数集，可以解决任何问题到任何精度的函数（神经网络）</p>
</li>
<li><p>找到任何函数的最佳参数集的方法（随机梯度下降）</p>
</li>
</ul>
<p>这就是为什么深度学习可以做出如此奇妙的事情。相信这些简单技术的组合确实可以解决任何问题是我们发现许多学生必须迈出的最大步骤之一。这似乎太好了，以至于难以置信——事情肯定应该比这更困难和复杂吧？我们的建议是：试一试！我们刚刚在 MNIST 数据集上尝试了一下，你已经看到了结果。由于我们自己从头开始做所有事情（除了计算梯度），所以你知道背后没有隐藏任何特殊的魔法。</p>
<h2 id="更深入地探讨"><a href="#更深入地探讨" class="headerlink" title="更深入地探讨"></a>更深入地探讨</h2><p>我们不必止步于只有两个线性层。我们可以添加任意数量的线性层，只要在每对线性层之间添加一个非线性。然而，正如您将了解的那样，模型变得越深，实际中优化参数就越困难。在本书的后面，您将学习一些简单但非常有效的训练更深层模型的技巧。</p>
<p>我们已经知道，一个带有两个线性层的单个非线性足以逼近任何函数。那么为什么要使用更深的模型呢？原因是性能。通过更深的模型（具有更多层），我们不需要使用太多参数；事实证明，我们可以使用更小的矩阵，更多的层，获得比使用更大的矩阵和少量层获得更好的结果。</p>
<p>这意味着我们可以更快地训练模型，并且它将占用更少的内存。在 1990 年代，研究人员如此专注于通用逼近定理，以至于很少有人尝试超过一个非线性。这种理论但不实际的基础阻碍了该领域多年。然而，一些研究人员确实尝试了深度模型，并最终能够证明这些模型在实践中表现得更好。最终，出现了理论结果，解释了为什么会发生这种情况。今天，几乎不可能找到任何人只使用一个非线性的神经网络。</p>
<p>当我们使用与我们在第一章中看到的相同方法训练一个 18 层模型时会发生什么：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dls = ImageDataLoaders.from_folder(path)</span><br><span class="line">learn = cnn_learner(dls, resnet18, pretrained=<span class="literal">False</span>,</span><br><span class="line">                    loss_func=F.cross_entropy, metrics=accuracy)</span><br><span class="line">learn.fit_one_cycle(<span class="number">1</span>, <span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th>时代</th>
<th>训练损失</th>
<th>验证损失</th>
<th>准确性</th>
<th>时间</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0.082089</td>
<td>0.009578</td>
<td>0.997056</td>
<td>00:11</td>
</tr>
</tbody>
</table>
</div>
<p>近乎 100%的准确性！这与我们简单的神经网络相比有很大的差异。但是在本书的剩余部分中，您将学习到一些小技巧，可以让您自己从头开始获得如此出色的结果。您已经了解了关键的基础知识。 （当然，即使您知道所有技巧，您几乎总是希望使用 PyTorch 和 fastai 提供的预构建类，因为它们可以帮助您省去自己考虑所有细节的麻烦。）</p>
<h1 id="术语回顾"><a href="#术语回顾" class="headerlink" title="术语回顾"></a>术语回顾</h1><p>恭喜：您现在知道如何从头开始创建和训练深度神经网络了！我们经历了很多步骤才达到这一点，但您可能会惊讶于它实际上是多么简单。</p>
<p>既然我们已经到了这一点，现在是一个很好的机会来定义和回顾一些术语和关键概念。</p>
<p>神经网络包含很多数字，但它们只有两种类型：计算的数字和这些数字计算出的参数。这给我们学习最重要的两个术语：</p>
<p>激活</p>
<p>计算的数字（线性和非线性层）</p>
<p>参数</p>
<p>随机初始化并优化的数字（即定义模型的数字）</p>
<p>在本书中，我们经常谈论激活和参数。请记住它们具有特定的含义。它们是数字。它们不是抽象概念，而是实际存在于您的模型中的具体数字。成为一名优秀的深度学习从业者的一部分是习惯于查看您的激活和参数，并绘制它们以及测试它们是否正确运行的想法。</p>
<p>我们的激活和参数都包含在 <em>张量</em> 中。这些只是正规形状的数组—例如，一个矩阵。矩阵有行和列；我们称这些为 <em>轴</em> 或 <em>维度</em>。张量的维度数是它的 <em>等级</em>。有一些特殊的张量：</p>
<ul>
<li><p>等级-0：标量</p>
</li>
<li><p>等级-1：向量</p>
</li>
<li><p>等级-2：矩阵</p>
</li>
</ul>
<p>神经网络包含多个层。每一层都是<em>线性</em>或<em>非线性</em>的。我们通常在神经网络中交替使用这两种类型的层。有时人们将线性层及其后续的非线性一起称为一个单独的层。是的，这很令人困惑。有时非线性被称为<em>激活函数</em>。</p>
<p>表 4-1 总结了与 SGD 相关的关键概念。</p>
<p>表 4-1. 深度学习词汇表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>术语</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReLU</td>
<td>对负数返回 0 且不改变正数的函数。</td>
</tr>
<tr>
<td>小批量</td>
<td>一小组输入和标签，聚集在两个数组中。在这个批次上更新梯度下降步骤（而不是整个 epoch）。</td>
</tr>
<tr>
<td>前向传播</td>
<td>将模型应用于某些输入并计算预测。</td>
</tr>
<tr>
<td>损失</td>
<td>代表我们的模型表现如何（好或坏）的值。</td>
</tr>
<tr>
<td>梯度</td>
<td>损失相对于模型某个参数的导数。</td>
</tr>
<tr>
<td>反向传播</td>
<td>计算损失相对于所有模型参数的梯度。</td>
</tr>
<tr>
<td>梯度下降</td>
<td>沿着梯度相反方向迈出一步，使模型参数稍微变得更好。</td>
</tr>
<tr>
<td>学习率</td>
<td>当应用 SGD 更新模型参数时我们所采取的步骤的大小。</td>
</tr>
</tbody>
</table>
</div>
<h1 id="选择你的冒险-提醒"><a href="#选择你的冒险-提醒" class="headerlink" title="选择你的冒险 提醒"></a><em>选择你的冒险</em> 提醒</h1><p>在你兴奋地想要窥探内部机制时，你选择跳过第 2 和第三章节了吗？好吧，这里提醒你现在回到第二章，因为你很快就会需要了解那些内容！</p>
<h1 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h1><ol>
<li><p>灰度图像在计算机上是如何表示的？彩色图像呢？</p>
<ul>
<li>灰度图像通过单通道的二维矩阵（每个像素为0-255的亮度值）表示，而彩色图像通过三通道（RGB）的三维数组（每个像素包含红、绿、蓝三个0-255的强度值）表示。</li>
</ul>
</li>
<li><p><code>MNIST_SAMPLE</code>数据集中的文件和文件夹是如何结构化的？为什么？</p>
<ul>
<li>MNIST_SAMPLE数据集通常按类别分层组织（如train/3、train/7和valid/3、valid/7），这种结构便于机器学习框架（如PyTorch的ImageFolder）自动识别标签并划分训练集/验证集，简化数据加载流程。</li>
</ul>
</li>
<li><p>解释“像素相似性”方法如何工作以对数字进行分类。</p>
<ul>
<li>与其尝试找到图像与“理想图像”之间的相似性，我们可以查看每个单独的像素，并为每个像素提出一组权重，使得最高的权重与最有可能为特定类别的黑色像素相关联。例如，向右下方的像素不太可能被激活为 7，因此它们对于 7 的权重应该很低，但它们很可能被激活为 8，因此它们对于 8 的权重应该很高。这可以表示为一个函数和每个可能类别的一组权重值，例如，成为数字 8 的概率</li>
</ul>
</li>
<li><p>什么是列表推导？现在创建一个从列表中选择奇数并将其加倍的列表推导。</p>
<ul>
<li>new_list = [x*2 for x in a_list if x%2 != 0]</li>
</ul>
</li>
<li><p>什么是秩-3 张量？</p>
<ul>
<li>秩-3 张量是具有三个独立维度的多维数组（形状如 (a, b, c)），可表示多矩阵堆叠或复杂三维数据（如视频帧序列、批量文本的词向量等）。</li>
</ul>
</li>
<li><p>张量秩和形状之间有什么区别？如何从形状中获取秩？</p>
<ul>
<li>张量秩指维度数量（如秩3是三维数组），形状描述各维度长度（如(2,3,4)）；秩等于形状元组的长度（len(tensor.shape)）。</li>
</ul>
</li>
<li><p>RMSE 和 L1 范数是什么？</p>
<ul>
<li>RMSE（均方根误差）是预测值与真实值误差平方均值的平方根，用于衡量回归模型精度；<script type="math/tex; mode=display">RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}</script></li>
</ul>
<ul>
<li>真实值：$y_i \in \mathbb{R}$  </li>
<li>预测值：$\hat{y}_i \in \mathbb{R}$  </li>
<li>样本数：$n \in \mathbb{N}^*$  </li>
</ul>
<ul>
<li>L1范数（如MAE）是误差绝对值的总和，常用于鲁棒性损失函数或稀疏正则化。<script type="math/tex; mode=display">L1 = \sum_{i=1}^{n} |y_i - \hat{y}_i|</script></li>
</ul>
<ul>
<li><strong>MAE（平均绝对误差）</strong>：<script type="math/tex; mode=display">MAE = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|</script></li>
</ul>
</li>
</ol>
<ol>
<li><p>如何才能比 Python 循环快几千倍地一次性对数千个数字进行计算？</p>
<ul>
<li>将数据转换为 NumPy 数组，利用其底层C语言实现的向量化操作：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 原生Python循环（慢）</span></span><br><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, ..., <span class="number">10000</span>]</span><br><span class="line">result = [x * <span class="number">2</span> + <span class="number">5</span> <span class="keyword">for</span> x <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line"><span class="comment"># NumPy向量化（快几千倍）</span></span><br><span class="line">data_np = np.array(data)</span><br><span class="line">result_np = data_np * <span class="number">2</span> + <span class="number">5</span>  <span class="comment"># 无显式循环，逐元素操作</span></span><br></pre></td></tr></table></figure>
<ul>
<li>超大规模数据时，通过 GPU 并行计算（需NVIDIA显卡）：<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cupy <span class="keyword">as</span> cp</span><br><span class="line"></span><br><span class="line">data_gpu = cp.array(data)  <span class="comment"># 数据传至GPU显存</span></span><br><span class="line">result_gpu = cp.exp(data_gpu) * <span class="number">10</span>  <span class="comment"># GPU并行计算指数和乘法</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li>对复杂逻辑，通过 JIT编译 生成机器码加速</li>
</ul>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numba <span class="keyword">import</span> jit</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="meta">@jit(<span class="params">nopython=<span class="literal">True</span></span>)  </span><span class="comment"># 强制编译为原生机器码</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_array</span>(<span class="params">arr</span>):</span><br><span class="line">    result = np.empty_like(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        result[i] = arr[i] ** <span class="number">2</span> + np.sin(arr[i])</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">data = np.random.rand(<span class="number">1000000</span>)</span><br><span class="line">result = process_array(data)  <span class="comment"># 首次运行编译，后续调用极快</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个包含从 1 到 9 的数字的 3×3 张量或数组。将其加倍。选择右下角的四个数字。</p>
<ul>
<li>PyTorch<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 3x3 张量（数值1-9）</span></span><br><span class="line">tensor = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有元素加倍</span></span><br><span class="line">doubled_tensor = tensor * <span class="number">2</span>  <span class="comment"># tensor([[ 2,  4,  6], [ 8, 10, 12], [14, 16, 18]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择右下角四个数字（最后两行 &amp; 最后两列）</span></span><br><span class="line">selected = doubled_tensor[<span class="number">1</span>:, <span class="number">1</span>:]  <span class="comment"># tensor([[10, 12], [16, 18]])</span></span><br></pre></td></tr></table></figure></li>
<li>numpy<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 3x3 数组（数值1-9）</span></span><br><span class="line">array = np.arange(<span class="number">1</span>, <span class="number">10</span>).reshape(<span class="number">3</span>, <span class="number">3</span>)  <span class="comment"># array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所有元素加倍</span></span><br><span class="line">doubled_array = array * <span class="number">2</span>  <span class="comment"># array([[ 2,  4,  6], [ 8, 10, 12], [14, 16, 18]])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择右下角四个数字</span></span><br><span class="line">selected = doubled_array[<span class="number">1</span>:, <span class="number">1</span>:]  <span class="comment"># array([[10, 12], [16, 18]])</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>广播是什么？</p>
<ul>
<li>广播是一种重要的功能，使张量代码更容易编写。在广播后，使两个参数张量具有相同的秩后，PyTorch 对于秩相同的两个张量应用其通常的逻辑：它对两个张量的每个对应元素执行操作，并返回张量结果。</li>
</ul>
</li>
<li><p>度量通常是使用训练集还是验证集计算的？为什么？</p>
<ul>
<li>验证集，排除过拟合状态。</li>
</ul>
</li>
<li><p>SGD 是什么？</p>
<ul>
<li>SGD（随机梯度下降，Stochastic Gradient Descent） 是一种用于优化模型参数的迭代算法，广泛应用于机器学习和深度学习。其核心思想是通过随机选取小批量数据（mini-batch） 计算梯度并更新参数，而非使用全部数据，从而显著提升训练效率。</li>
</ul>
</li>
<li><p>为什么 SGD 使用小批量？</p>
<ul>
<li><p>利用硬件并行加速，提升计算效率；</p>
</li>
<li><p>平衡梯度噪声与稳定性，避免剧烈震荡；</p>
</li>
<li><p>适应内存限制，灵活处理大规模数据。</p>
</li>
</ul>
</li>
<li><p>SGD 在机器学习中有哪七个步骤？</p>
<ul>
<li><p><em>初始化</em>权重。</p>
</li>
<li><p>对于每个图像，使用这些权重来<em>预测</em>它是 3 还是 7。</p>
</li>
<li><p>基于这些预测，计算模型有多好（它的<em>损失</em>）。</p>
</li>
<li><p>计算<em>梯度</em>，它衡量了每个权重的变化如何改变损失。</p>
</li>
<li><p>根据这个计算，<em>改变</em>（即，改变）所有权重。</p>
</li>
<li><p>回到步骤 2 并<em>重复</em>这个过程。</p>
</li>
<li><p>迭代直到你决定<em>停止</em>训练过程（例如，因为模型已经足够好或者你不想再等待了）。</p>
</li>
</ul>
</li>
<li><p>我们如何初始化模型中的权重？</p>
<ul>
<li>我们将参数初始化为随机值。这可能听起来令人惊讶。我们当然可以做其他选择，比如将它们初始化为该类别激活该像素的百分比—但由于我们已经知道我们有一种方法来改进这些权重，结果证明只是从随机权重开始就可以完全正常运行。</li>
</ul>
</li>
<li><p>什么是损失？</p>
<ul>
<li>这就是 Samuel 所说的<em>根据实际表现测试任何当前权重分配的有效性</em>。我们需要一个函数，如果模型的表现好，它将返回一个小的数字（标准方法是将小的损失视为好的，大的损失视为坏的，尽管这只是一种约定）。</li>
</ul>
</li>
<li><p>为什么我们不能总是使用高学习率？</p>
<ul>
<li>如果学习率太高，它也可能会“弹跳”而不是发散；图 4-4 显示了这样做需要许多步骤才能成功训练。</li>
</ul>
</li>
</ol>
<ol>
<li><p>什么是梯度？</p>
<ul>
<li>SGD中的梯度是损失函数对参数的敏感度，指导参数向损失降低的方向调整。小批量计算兼顾了效率与稳定性，是深度学习的核心驱动力。</li>
</ul>
</li>
<li><p>你需要知道如何自己计算梯度吗？</p>
<ul>
<li>不需要</li>
</ul>
</li>
<li><p>为什么我们不能将准确率作为损失函数使用？</p>
<ul>
<li>关键区别在于指标用于驱动人类理解，而损失用于驱动自动学习。为了驱动自动学习，损失必须是一个具有有意义导数的函数。它不能有大的平坦部分和大的跳跃，而必须是相当平滑的。这就是为什么我们设计了一个损失函数，可以对置信水平的小变化做出响应。这个要求意味着有时它实际上并不完全反映我们试图实现的目标，而是我们真正目标和一个可以使用其梯度进行优化的函数之间的妥协。损失函数是针对数据集中的每个项目计算的，然后在时代结束时，所有损失值都被平均，整体均值被报告为时代。</li>
</ul>
</li>
<li><p>绘制 Sigmoid 函数。它的形状有什么特别之处？</p>
<ul>
<li>正如您所看到的，它接受任何输入值，正数或负数，并将其压缩为 0 和 1 之间的输出值。它还是一个只上升的平滑曲线，这使得 SGD 更容易找到有意义的梯度。</li>
</ul>
<p><img src="/image/dlcf_04in15.png" alt=""></p>
</li>
</ol>
<ol>
<li><p>损失函数和度量之间有什么区别？</p>
<ul>
<li>损失函数：是模型训练的“指南针”，需可导且适合优化。<br>度量：是模型性能的“成绩单”，反映实际任务需求。<br>核心原则：损失函数服务于训练过程，度量服务于业务目标，两者需根据任务特性协同设计。</li>
</ul>
</li>
<li><p>使用学习率计算新权重的函数是什么？</p>
<p>参数-=学习率*梯度</p>
</li>
<li><p><code>DataLoader</code>类是做什么的？</p>
<ul>
<li>如果我们在训练过程中可以改变一些东西，我们会获得更好的泛化能力。我们可以改变的一个简单而有效的事情是将哪些数据项放入每个小批次。我们通常不是简单地按顺序枚举我们的数据集，而是在每个时代之前随机洗牌，然后创建小批次。PyTorch 和 fastai 提供了一个类，可以为您执行洗牌和小批次整理，称为<code>DataLoader</code>。</li>
</ul>
</li>
<li><p>编写伪代码，显示每个 epoch 中 SGD 所采取的基本步骤。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化模型参数 θ（例如权重矩阵、偏置向量）</span></span><br><span class="line">Initialize θ randomly</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">num_epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="number">1</span> to num_epochs:</span><br><span class="line">    <span class="comment"># 将训练数据随机打乱（确保样本独立性）</span></span><br><span class="line">    Shuffle training data</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将数据划分为多个小批量（mini-batch）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="number">0</span> to (num_samples / batch_size - <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 提取当前小批量数据</span></span><br><span class="line">        batch_X = X_train[i*batch_size : (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        batch_Y = Y_train[i*batch_size : (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向传播：计算当前参数下的预测值</span></span><br><span class="line">        predictions = forward_pass(θ, batch_X)  <span class="comment"># 例如 y_pred = θ^T X + b</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失函数值（如均方误差、交叉熵）</span></span><br><span class="line">        loss = compute_loss(predictions, batch_Y)  <span class="comment"># 例如 L = 1/m Σ(y_pred - y_true)^2</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反向传播：计算损失对参数θ的梯度 ∇θ</span></span><br><span class="line">        gradients = compute_gradients(θ, batch_X, batch_Y)  <span class="comment"># ∇θ = dL/dθ</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 更新参数：沿梯度反方向调整θ</span></span><br><span class="line">        θ = θ - learning_rate * gradients</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># （可选）在每轮结束后计算验证集损失/度量</span></span><br><span class="line">    val_predictions = forward_pass(θ, X_val)</span><br><span class="line">    val_loss = compute_loss(val_predictions, Y_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>: Train Loss = <span class="subst">&#123;loss&#125;</span>, Val Loss = <span class="subst">&#123;val_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个函数，如果传递两个参数<code>[1,2,3,4]</code>和<code>&#39;abcd&#39;</code>，则返回<code>[(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]</code>。该输出数据结构有什么特别之处？</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">pair_elements</span>(<span class="params">nums, chars</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span>(<span class="built_in">zip</span>(nums, chars))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="built_in">print</span>(pair_elements([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>], <span class="string">&#x27;abcd&#x27;</span>))  <span class="comment"># 输出 [(1, &#x27;a&#x27;), (2, &#x27;b&#x27;), (3, &#x27;c&#x27;), (4, &#x27;d&#x27;)]</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>PyTorch 中的<code>view</code>是做什么的？</p>
<ul>
<li>我们已经有了我们的<code>x</code>—也就是我们的自变量，图像本身。我们将它们全部连接成一个单一的张量，并且还将它们从矩阵列表（一个秩为 3 的张量）转换为向量列表（一个秩为 2 的张量）。我们可以使用<code>view</code>来做到这一点，<code>view</code>是一个 PyTorch 方法，可以改变张量的形状而不改变其内容。<code>-1</code>是<code>view</code>的一个特殊参数，意思是“使这个轴尽可能大以适应所有数据”</li>
</ul>
</li>
<li><p>神经网络中的偏差参数是什么？我们为什么需要它们？</p>
<ul>
<li><p>神经网络中的偏差参数（Bias）是每个神经元中的一个可学习参数，用于在加权和计算后添加一个常数偏移。</p>
</li>
<li><p>平移激活函数的输入：允许调整加权和的基线位置，使激活函数能适应不同数据分布。增强模型表达能力：没有偏差时，模型只能学习经过原点的超平面；加入偏差后，模型可以表示任意位置的超平面。</p>
</li>
</ul>
</li>
<li><p>Python 中的<code>@</code>运算符是做什么的？</p>
<ul>
<li>矩阵乘法用<code>@</code>运算符表示</li>
</ul>
</li>
<li><p><code>backward</code>方法是做什么的？</p>
<ul>
<li>这里的<code>backward</code>指的是<em>反向传播</em>，这是计算每一层导数的过程的名称。我们将在第十七章中看到这是如何精确完成的，当我们从头开始计算深度神经网络的梯度时。这被称为网络的<em>反向传播</em>，与<em>前向传播</em>相对，前者是计算激活的地方。如果<code>backward</code>只是被称为<code>calculate_grad</code>，生活可能会更容易，但深度学习的人确实喜欢在任何地方添加行话！</li>
</ul>
</li>
<li><p>为什么我们必须将梯度清零？</p>
<ul>
<li>必须将梯度清零是为了防止不同批次（batch）的梯度在反向传播时累积，导致参数更新方向错误，确保每个批次的梯度独立计算并正确更新模型参数。</li>
</ul>
</li>
<li><p>我们需要向<code>Learner</code>传递什么信息？</p>
<ul>
<li>我们需要传入本章中创建的所有元素：<code>DataLoaders</code>，模型，优化函数（将传递参数），损失函数，以及可选的任何要打印的指标：</li>
</ul>
</li>
<li><p>展示训练循环的基本步骤的 Python 或伪代码。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 初始化模型、损失函数、优化器</span></span><br><span class="line">model = 初始化神经网络()</span><br><span class="line">loss_function = 选择损失函数()  <span class="comment"># 如交叉熵、均方误差</span></span><br><span class="line">optimizer = 选择优化器(model.parameters(), 学习率)  <span class="comment"># 如SGD、Adam</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据预处理</span></span><br><span class="line">dataset = 加载数据集()</span><br><span class="line">dataloader = 分批次(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练循环（按epoch迭代）</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="number">1</span>到最大训练轮次:</span><br><span class="line">    model.训练模式()  <span class="comment"># 启用Dropout/BatchNorm等训练特定层</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 遍历所有小批量（mini-batch）</span></span><br><span class="line">    <span class="keyword">for</span> 每个batch的输入数据x, 标签y <span class="keyword">in</span> dataloader:</span><br><span class="line">        <span class="comment"># 3.1 梯度清零（关键！防止梯度累积）</span></span><br><span class="line">        optimizer.清空梯度()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.2 前向传播：计算预测值</span></span><br><span class="line">        预测值 = model.前向计算(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.3 计算损失（预测值与真实值差距）</span></span><br><span class="line">        loss = loss_function(预测值, y)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.4 反向传播：计算梯度</span></span><br><span class="line">        loss.反向传播()  <span class="comment"># 自动计算各参数梯度</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3.5 参数更新：沿梯度反方向调整参数</span></span><br><span class="line">        optimizer.更新参数()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># （可选）4. 验证/测试循环</span></span><br><span class="line">    model.评估模式()  <span class="comment"># 禁用Dropout/BatchNorm等训练特定层</span></span><br><span class="line">    total_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> 每个验证batch的输入x_val, 标签y_val <span class="keyword">in</span> 验证集:</span><br><span class="line">        预测_val = model.前向计算(x_val)</span><br><span class="line">        total_loss += loss_function(预测_val, y_val)</span><br><span class="line">    平均验证损失 = total_loss / 验证batch数量</span><br><span class="line">    打印(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch&#125;</span>, 验证损失: <span class="subst">&#123;平均验证损失&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>ReLU 是什么？为值从<code>-2</code>到<code>+2</code>绘制一个图。</p>
<ul>
<li>那个小函数<code>res.max(tensor(0.0))</code>被称为<em>修正线性单元</em>，也被称为<em>ReLU</em>。我们认为我们都可以同意<em>修正线性单元</em>听起来相当花哨和复杂…但实际上，它不过是<code>res.max(tensor(0.0))</code>——换句话说，用零替换每个负数。这个微小的函数在 PyTorch 中也可以作为<code>F.relu</code>使用：</li>
</ul>
</li>
</ol>
<p><img src="/image/dlcf_04in16.png" alt=""></p>
<ol>
<li><p>什么是激活函数？</p>
<ul>
<li>神经网络包含多个层。每一层都是<em>线性</em>或<em>非线性</em>的。我们通常在神经网络中交替使用这两种类型的层。有时人们将线性层及其后续的非线性一起称为一个单独的层。是的，这很令人困惑。有时非线性被称为<em>激活函数</em>。</li>
</ul>
</li>
<li><p><code>F.relu</code>和<code>nn.ReLU</code>之间有什么区别？</p>
<ul>
<li>nn.ReLU：是 模块化的层，适合定义静态模型结构，参数在初始化时固定。<br>F.relu：是 函数式接口，适合动态或条件性激活场景，参数在调用时指定。<br>当需要将激活函数作为模型的一部分（如保存/加载模型）时，优先用 nn.ReLU。<br>当需要灵活控制激活逻辑时，用 F.relu。</li>
</ul>
</li>
<li><p>通用逼近定理表明，任何函数都可以使用一个非线性逼近得到所需的精度。那么为什么我们通常使用更多的非线性函数？</p>
<ul>
<li>在 1990 年代，研究人员如此专注于通用逼近定理，以至于很少有人尝试超过一个非线性。这种理论但不实际的基础阻碍了该领域多年。然而，一些研究人员确实尝试了深度模型，并最终能够证明这些模型在实践中表现得更好。最终，出现了理论结果，解释了为什么会发生这种情况。今天，几乎不可能找到任何人只使用一个非线性的神经网络。</li>
</ul>
</li>
</ol>
<h2 id="进一步研究"><a href="#进一步研究" class="headerlink" title="进一步研究"></a>进一步研究</h2><ol>
<li><p>从头开始创建自己的<code>Learner</code>实现，基于本章展示的训练循环。</p>
</li>
<li><p>使用完整的 MNIST 数据集完成本章的所有步骤（不仅仅是 3 和 7）。这是一个重要的项目，需要花费相当多的时间来完成！您需要进行一些研究，以找出如何克服在途中遇到的障碍。</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Wang Song"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Wang Song</p>
  <div class="site-description" itemprop="description">a graduate student working at Huzhou institute of Zhejiang University</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/SongSop" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;SongSop" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:3042903197@qq.com" title="E-Mail → mailto:3042903197@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Wang Song</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'default',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
